<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pyhton可视化(2): 中国66家环保股上市公司市值Top20强]]></title>
    <url>%2FPython_visualization02.html</url>
    <content type="text"><![CDATA[Tushare包提取中国环保股上市公司近10年市值排名，并结合D3.js做动态数据可视化表。 摘要： 之前介绍过Tushare包和D3.js可视化动态表格，本文将二者进行结合，制作中国上市公司环境保护股Top20强，近十年的市值变化动态表。 制作近10年期间，环保板块市值最高的20只股票动态变化，需要获得各只股票在不同年份的市值。获取特定股票的市值可以利用pro.daily_basic接口获取到每日的市值，然后利用Resample函数获得年均市值。但获取环保板块所有几十只股票的数据，用手动输入股票代码就不是很方便，此时，可以利用该包另外一个接口ts.get_stock_basics()接口获取所有股票基本数据，该接口能够返回股票代码、行业类别等数据。两个接口合二为一就可以提取出所需的数据，下面开始详细实现步骤。 1. 提取所有股票代码123456789101112import tushare as ts# 获取所有股票列表data = ts.get_stock_basics()print(data.head())# 返回数据如下，所有列值可以参考：http://tushare.org/fundamental.html name industry area pe outstanding totals totalAssets \code 002936 N郑银 银行 河南 8.27 6.00 59.22 44363604.00 600856 中天能源 供气供热 吉林 21.28 13.43 13.67 1712831.63 300021 大禹节水 农业综合 甘肃 35.27 6.48 7.97 359294.91 603111 康尼机电 运输设备 江苏 0.00 7.38 9.93 734670.69 000498 山东路桥 建筑施工 山东 19.52 4.41 11.20 1926262.38 可以看到，index是股票代码，name股票名称，industry是行业分类。我们需要获取环保类（可以获取任意行业类别，也可以全部获取所有股票，为了后期数据提取量耗时短一些，所以选择提取环保类股票）的股票代码和股票名称，代码如下： 123456789101112data = data[data.industry =='环境保护']print(data.head()) #返回的环保股数据print('环保股股票数量为'：len(data.industry)) #计算环保股股票数量结果如下： name industry area pe outstanding totals totalAssets \code 300056 三维丝 环境保护 福建 0.00 2.37 3.85 266673.63 002549 凯美特气 环境保护 湖南 34.44 6.20 6.24 122630.13 300422 博世科 环境保护 广西 19.22 2.64 3.56 509822.44 601330 绿色动力 环境保护 深圳 59.83 1.16 11.61 784969.25 000820 神雾节能 环境保护 辽宁 0.00 2.88 6.37 284674.34 环保股股票数量为： 66 可以看到，环境保护股一共有66只，下面我们将用这66只股票的代码和名称，输入到pro.daily_basic()接口中，获取每只股票的每日数据，其中包括每日市值。时间期限从2009年1月1日至2018年9月10日，共10年的逐日数据。 2. 提股票每日市值每日基本指标的数据接口：https://tushare.pro/document/2?doc_id=32 1234pro = ts.pro_api()pro.daily_basic(ts_code='', trade_date='',start_date = '',end_date = '')# ts_code是股票代码，格式为000002.SZ,可以为一只股票，也可以是列表组成的多支股票# 后面三个是交易日期，可以为固定日期，也可以为一个时期，格式'20180919' 该接口股票代码的格式是000002.SZ，而上面股票代码格式是：000002，没有带后缀.SZ，由此需要添加上，然后就可获取每只股票近10年的逐日市值数据。 12345678910data['code2'] = data.index# apply方法添加.SZ后缀data['code2'] = data['code2'].apply(lambda i:i+'.SZ')data = data.set_index(['code2'])# 将code和name转为dict，因为我们只需要表格中的代码和名称列data = data['name']data = data.to_dict()# print(data) #测试返回的环保股字典数据 ok&#123;'300056.SZ': '三维丝', '002549.SZ': '凯美特气', '300422.SZ': '博世科', '601330.SZ': '绿色动力', '000820.SZ': '神雾节能', '300072.SZ': '三聚环保', '300055.SZ': '万邦达', '002717.SZ': '岭南股份', '300070.SZ': '碧水源', '000504.SZ': '南华生物', '300203.SZ': '聚光科技', '002672.SZ': '东江环保', '000967.SZ': '盈峰环境', '002322.SZ': '理工环科', '300272.SZ': '开能健康', '300495.SZ': '美尚生态', '603717.SZ': '天域生态', '300266.SZ': '兴源环境', '603126.SZ': '中材节能', '002200.SZ': '云投生态', '300385.SZ': '雪浪环境', '603200.SZ': '上海洗霸', '000826.SZ': '启迪桑德', '300262.SZ': '巴安水务', '002887.SZ': '绿茵生态', '603568.SZ': '伟明环保', '300631.SZ': '久吾高科', '002616.SZ': '长青集团', '300156.SZ': '神雾环保', '000920.SZ': '南方汇通', '600008.SZ': '首创股份', '601200.SZ': '上海环境', '603955.SZ': '大千生态', '603177.SZ': '德创环保', '600481.SZ': '双良节能', '300190.SZ': '维尔利', '603588.SZ': '高能环境', '002034.SZ': '旺能环境', '603817.SZ': '海峡环保', '002499.SZ': '科林环保', '603822.SZ': '嘉澳环保', '300664.SZ': '鹏鹞环保', '300332.SZ': '天壕环境', '600526.SZ': '菲达环保', '600874.SZ': '创业环保', '600292.SZ': '远达环保', '603903.SZ': '中持股份', '300172.SZ': '中电环保', '000544.SZ': '中原环保', '300692.SZ': '中环环保', '600388.SZ': '龙净环保', '300425.SZ': '环能科技', '300388.SZ': '国祯环保', '300362.SZ': '天翔环境', '300197.SZ': '铁汉生态', '300187.SZ': '永清环保', '300090.SZ': '盛运环保', '002573.SZ': '清新环境', '000035.SZ': '中国天楹', '603797.SZ': '联泰环保', '603603.SZ': '博天环境', '300137.SZ': '先河环保', '300355.SZ': '蒙草生态', '300152.SZ': '科融环境', '002658.SZ': '雪迪龙', '600217.SZ': '中再资环'&#125; 可以看到，很完整地显示了环保股的股票代码和名称，下面通过for循环即可获取每日数据。为了方便，将上式代码命名为一个函数get_code()，return data 为上面的dict。 3. 提取环保股公司数据123456789101112ts_codes = get_code()start = '20090101'end = '201809010'for key,value in ts_codes.items(): data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) # 获取每只股票时间段数据 # 添加代码列和名称列 # 替换掉末尾的.SZ,regex设置为true才行 data['code'] = data['ts_code'].replace('.SZ','',regex = True) data['name'] = value # 存储结果 data.to_csv('environment.csv',mode='a',encoding = 'utf_8_sig',index = False,header = 0) print('数据提取完毕') 表格结果如下，66只股票10年一共产生了75933行数据。如果提前全部3000多家股票的数据，那么数据量会达到几百万行，量太大，所以这里仅提取了66支。其中，选中的列为每日市值（万元）。下面就可以根据日期、市值得到各只股票每年的市值均值，然后绘制股票动态表。 4. 绘制Top20强动态表首先读取上面的表格，获取DataFrame信息： 1234567891011121314151617181920212223df = pd.read_csv('environment.csv',encoding = 'utf-8',converters = &#123;'code':str&#125;)# converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示print(df.info())Data columns (total 17 columns):ts_code 75932 non-null objecttrade_date 75932 non-null int64close 75932 non-null float64turnover_rate 75932 non-null float64volume_ratio 0 non-null float64pe 70861 non-null float64e_ttm 69254 non-null float64pb 73713 non-null float64ps 75932 non-null float64ps_ttm 75840 non-null float64total_share 75932 non-null float64float_share 75932 non-null float64free_share 75932 non-null float64total_mv 75932 non-null float64circ_mv 75932 non-null float64code 75932 non-null int64name 75932 non-null objectdtypes: float64(13), int64(2), object(2) 可以看到trade_date交易日期是整形，需将交易日期先转换为字符型再转换为datetime日期型。 1234567891011121314151617181920212223242526from datetime import datetime# trade_date是int型，需转为字符型df['trade_date'] = df['trade_date'].apply(str)# 或者# df['trade_date'] = df['trade_date'].astype(str)# 将object转为datatimedf['trade_date'] = pd.to_datetime(df['trade_date'],format = '%Y%m%d',errors = 'ignore') #errors忽略无法转换的数据，不然会报错# 结果如下：Data columns (total 17 columns):ts_code 75932 non-null objecttrade_date 75932 non-null datetime64[ns]close 75932 non-null float64turnover_rate 75932 non-null float64volume_ratio 0 non-null float64pe 70861 non-null float64e_ttm 69254 non-null float64pb 73713 non-null float64ps 75932 non-null float64ps_ttm 75840 non-null float64total_share 75932 non-null float64float_share 75932 non-null float64free_share 75932 non-null float64total_mv 75932 non-null float64circ_mv 75932 non-null float64code 75932 non-null int64name 75932 non-null object 1234567891011121314# 设置总市值数字格式由万元变为亿元df['total_mv'] = (df['total_mv']/10000)# 保留四列,并将交易日期设为indexdf = df[['ts_code','trade_date','total_mv','name']]df = df.set_index('trade_date')print(df.head())# 结果如下: ts_code total_mv nametrade_date 2018-08-30 300090.SZ 36.034715 盛运环保2018-08-29 300090.SZ 38.014644 盛运环保2018-08-28 300090.SZ 39.202602 盛运环保2018-08-27 300090.SZ 40.126569 盛运环保2018-08-24 300090.SZ 38.938611 盛运环保 接下来，求出每只股票每年的市值平均值： 12345678910111213141516171819202122232425求平均市值时需切片同一股票，这里股票名称切片赋值为value变量，也就是dict字典里66只股票名称df = df[df.name == value]# 不能用query方法,会报错 df = df.query('name == value')# resampe按年统计数据df = df.resample('AS').mean() #年平均市值print(df.head())# 结果如下： total_mv codetrade_date 2009-01-01 25.184678 三维丝2010-01-01 50.672849 三维丝2011-01-01 46.488004 三维丝2012-01-01 39.214508 三维丝2013-01-01 59.110332 三维丝# 再用to_period按年显示市值数据df = df.to_period('A')print(df.head())# 结果如下: total_mv codetrade_date 2009 25.184678 三维丝2010 50.672849 三维丝2011 46.488004 三维丝2012 39.214508 三维丝2013 59.110332 三维丝 经过以上处理，基本就获得了想要的数据。为了能够满足D3.js模板表格条件，再做一点修改： 12345678910# 增加code列df['code'] = value# 重置indexdf = df.reset_index()# 重命名为d3.js格式# 增加一列空typedf['type'] = ''df = df[['code','type','total_mv','trade_date']]df.rename(columns = &#123;'code':'name','total_mv':'value','type':'type','trade_date':'date'&#125;)# df.to_csv('parse_environment.csv',mode='a',encoding = 'utf_8_sig',index = False,float_format = '%.1f',header = 0) # float_format = '%.1f' #设置输出浮点数格式为1位小数 最终，生成parse_environment.csv文件如下： 123456789101112name type value date中国天楹 8.8 2009中国天楹 9.8 2010中国天楹 15 2011中国天楹 18.8 2012中国天楹 22.5 2013...东方园林 177.1 2014东方园林 320.5 2015东方园林 288.4 2016东方园林 481 2017东方园林 461.5 2018 可绘制出动态可视化表格，见下面视频。可以看到前几年市值龙头由东方园林、碧桂园轮流坐庄，近两年三聚环保强势崛起，市值增长迅猛，跃居头名。 https://www.bilibili.com/video/av32087716/ 本文仅对比了环保股企业的市值变化，你还可以分析互联网股、金融股等100多种行业的企业市值对比。另外，Tushare包返回的参数还可以做更多其他分析。 文章完整的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import pandas as pdimport tushare as tsfrom datetime import datetimeimport matplotlib.pyplot as pltts.set_token('404ba015bd44c01cf09c8183dcd89bb9b25749057ff72b5f8671b9e6')pro = ts.pro_api()def get_code(): # 所有股票列表 data = ts.get_stock_basics() # data = data.query('industry == "环境保护"') # 或者 data = data[data.industry =='环境保护'] # 提取股票代码code并转化为list data['code2'] = data.index # apply方法添加.SZ后缀 data['code2'] = data['code2'].apply(lambda i:i+'.SZ') data = data.set_index(['code2']) # 将code和name转为dict data = data['name'] data = data.to_dict() # 增加东方园林 data['002310.SZ'] = '东方园林' # print(data) #测试返回的环保股dict ok return datadef stock(key,start,end,value): data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) # 获取每只股票时间段数据 # 替换掉末尾的.SZ,regex设置为true才行 data['code'] = data['ts_code'].replace('.SZ','',regex = True) data['name'] = value # print(data) data.to_csv('environment.csv',mode='a',encoding = 'utf_8_sig',index = False,header = 0)def parse_code(): df = pd.read_csv('environment.csv',encoding = 'utf-8',converters = &#123;'code':str&#125;) # converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示 df.columns = ['ts_code','trade_date','close','turnover_rate','volume_ratio','pe','e_ttm','pb','ps','ps_ttm','total_share','float_share','free_share','total_mv','circ_mv', 'code','name'] # trade_date是int型，需转为字符型 df['trade_date'] = df['trade_date'].apply(str) # 或者df['trade_date'] = df['trade_date'].astype(str) # 将object转为datatime df['trade_date'] = pd.to_datetime(df['trade_date'],format = '%Y%m%d',errors = 'ignore') #errors忽略无法转换的数据，不然会报错 ## 设置总市值数字格式由万元变为亿元 df['total_mv'] = (df['total_mv']/10000) # 保留四列,并将交易日期设为index df = df[['ts_code','trade_date','total_mv','name']] df = df.set_index('trade_date') df = df[df.name == value] # # 不能用query方法 # # df = df.query('name == ') df = df.resample('AS').mean()/10000 #年平均市值 df = df.to_period('A') # # 增加code列 df['code'] = value # # 重置index df = df.reset_index() # 重命名为d3.js格式 # 增加一列空type df['type'] = '' df = df[['code','type','total_mv','trade_date']] df.rename(columns = &#123;'code':'name','total_mv':'value','type':'type','trade_date':'date'&#125;) df.to_csv('parse_environment.csv',mode='a',encoding = 'utf_8_sig',index = False,float_format = '%.1f',header = 0) float_format = '%.1f' #设置输出浮点数格式 # print(df) # print(df.info())def main(): # get_code() #提取环保股dict start = '20090101' end = '201809010' ts_codes = get_code() # dict_values转list keys = list(ts_codes.keys()) values = list(ts_codes.values()) for key,value in ts_codes.items(): stock(key,start,end,value) for value in values: parse_code(value)if __name__ == '__main__': main() 文件素材可以在这里获得： https://github.com/makcyun/web_scraping_with_python/tree/master/Tushare 本文完。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的白血病妻子]]></title>
    <url>%2Flife01.html</url>
    <content type="text"><![CDATA[世界骨髓捐献者日。 当夜晚来临，人们结束一天的忙碌，正赶着回家之际，我的一天才刚刚开始。守夜时间是晚七时至凌晨七时，我称之为”深夜医院”。你问我有没有人，我会说人还挺少的。 1. 前言这是一篇关于一个年轻的家庭和白血病作斗争的生活随笔。人物介绍：我，苏克，27岁。妻子，”23”，25岁。儿子，馍馍，2岁半。时间 ： 2018年5月至今地点 ：东莞—广州—北京 2. 天使与恶魔5月18日23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”我：”老婆，太棒了，我就说你可以的，你都找不到工作，那很多人更找不到了！” 23：”一直说想换工作，这次终于是跨出这一步了，很紧张啊！”我：”不要紧张，你肯定能很快适应新工作的。” 23：”好吧，我答应 HR 6月1号去签合同，再上几天班就把年假休了，放松一下。”我：”对啊，好好休息休息，我要是也有年假，就陪你出去玩了。” 23：”你在深圳，周末能有空回来就不错了…”我：”以后，回东莞来就有时间了…” 一周后 5月25日 周五23：”老公，我住院了。”我：”啊？怎么回事？” 23：” 背痛了两天，痛地不行了，还发烧…”我：”还是没有缓解啊，我刚下班，现在从梅林关坐大巴回来，到了马上去医院…” 一个多小时后…我：”怎么样？医生怎么说?”23：”医生看了血常规，说白细胞很高(29万多，正常是1万以内)，让我转到血液科来了，现在打消炎针退烧…”我：”希望没大碍，早点退烧吧，这几天晚上我陪你。” 半夜…23：”老公，我很热，出了好多汗，衣服全湿了…”我：”我给你换，你额头不烫了，多出点汗烧就退下来了。”23：”我背好疼…”我：”忍受不了的痛么？我刚去找了医生，医生说忍一忍，坚强一点。” 3天后 周一我：” 老婆，我得赶去深圳上班了，希望你这周能退烧。”23：”我也希望，我这周五还要去公司签劳动合同呢。” 6月1日 周五我：”老婆，今天儿童节，我在网上买了好多书给儿子，以后周末回来给他讲故事，争取做个会讲故事的好爸爸。”23：”好，你快回来吧，我这几天反复高烧到40度，医生也找不到原因，让做骨穿还建议转院去广州”我：”啊？太奇怪了，这次发烧怎么这么顽固，骨穿是做什么？我马上坐车回来…” 6月2日 转院到广州一家医院的血液科当地医院的医生找不到病因，让我们转院到广州去。我一直不明白，发烧为什么要住到血液科病区。到了之后，医生给23抽了骨髓，说是查找病因。 一天后医生单独跟我我和家里人说：”确诊病人患了急性淋巴细胞白血病，之前反复高烧和骨骼疼痛是这种病的常见临床表现。这种病得马上开始化疗，不然会有生命危险。” 听完，我一时没有反应过来。 后来了解到，如果一切顺利的话，治疗和康复时长大概要2年，治疗费用在100万左右；如果不顺利，生命可能只剩不到2年。 我突然明白了：原来这不是普通地发烧和背疼啊。不过，”白血病是什么病? 严重么？好治么？” 23那几天一直问：”检测结果有没有出来？到底是怎么回事？” 我说：”医生说需要十几天才能检测出来，别着急，这边的医生肯定会把你治好的。” 我偷偷百度了下，大致了解到白血病俗称”血癌”，是癌症的一种。后面一直在想，为什么23会得上这个病？不管怎样，首先要做好长期住院治疗的准备，那么得尽快辞职了。 6月8日我回到深圳，火速辞了职、退了租房。 晚上，关了灯，望着窗外灯火通明的深圳，看着这个刚刚来了3个月的城市。各种思绪开始涌上心头：23会很顺利地治好病么？未来的生活会是怎样？2年后进入社会我们还有竞争力么？很多刚毕业的人感叹生活艰难，一出来就要变成”房奴”。那我，如果不靠父母亲友，岂不是要变成”病奴”了？ 想了好一会儿却什么没想出来，那么就顺其自然、虔诚祈祷吧。转而，开始回忆起这几年的走过的点点滴滴，想从曾面对过的种种困难中找到信心。 3. 时光倒流3.1. 2014年-浪漫邂逅2014年夏天，我本科毕业，从成都开启了筹划已久的70天毕业旅行。对于一个在新疆长大的内地人来说，22、3岁才第一次见到大海，那是一种难以言表的心情。整个旅途，我都尽可能地享受和体验每一处风土人情。 70天，不知不觉就快要结束，来到了最后一站：南亚岛国斯里兰卡。在一个青旅里我第一次见到了23。那种感觉用一个词形容就够了：”一见钟情”。聊了几次天之后，得知她在广州念大三，来斯里兰卡是到一个孤儿院做义工。 一周后，我到了她所在的城市”加勒”——一座濒临印度洋的小城镇。距离回国倒数的那几天里，我用一顿大盘鸡把她”收买”了。后来，我认为这是我人生中最重要的决定之一。 3.2. 2015年-约定终身回国后，我们开始异地，我在成都读研，她在广州读大四。这一年，我们的感情状况可以用”打飞的”来形容，她常来成都，我也常过去，还参加了她的毕业典礼。 最终，我们决定在8月，认识的一周年的那天领证结婚。婚礼仪式很简单，仅是请亲人朋友吃了一顿饭。 3.3. 2016年-馍馍出生2016年过年期间，感谢伟大的妈妈，把儿子馍馍带到了这个世界上。 坦诚地说，那种感觉是惊喜与压力并存。我们俩都没有什么经济来源，只能靠家里”救济”，我仅能每个月回来几天。 3.4. 2017年-毕业工作就这样，异地状态维持了近3年，我终于毕业，可以回去抱娃和工作。曾经，八卦地转发过一篇文章给23说：”居然有人带娃参加毕业典礼”，没想到，说的也算是自己啊。 家也从西北搬到了东南，走过这一路，仅仅花了多数人二到三分之一的时间。 3.5. 2018年-再次异地在东莞工作了半年，2018年春节后，我决定还是去隔壁更大的深圳看看。4月的某一天，我和23说：”要不你还是从政府出来吧，趁还年轻换个更有活力和挑战的工作环境。”23：”是啊，为了馍馍在里面待了3年，我也想出来了。”我：”那我们就开始找找看吧。” 一个月后的5月18日23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”(回到了文章的开头） 窗外的霓虹灯渐渐地黯淡下来，发现已经半夜，赶紧睡吧，明天得赶去广州的医院，要开始新的人生节奏了。 4. 难熬的化疗4.1. 治疗过程6月2日，23在广州医院住下来，仍然在发烧并伴随强烈的骨痛。前3天体温持续超过38度，半夜最高烧到了40.3度。 6月4日下午，瞒着23马上开始进行了化疗药物注射。说起化疗，我脑子里最先想到的是赵本山和范伟演过的一个小品，里面戏称化疗为”谈话治疗”。可实际中的化疗远比这难受地多。 23对药物反应特别大，不停呕吐、浑身乏力、也没有任何食欲，体重开始暴瘦。连续打了三天化疗针以后，总算压制住了体内的恶性白细胞，23不再发烧和骨痛，整个人才渐渐恢复精神，开始有胃口喝粥和下床活动一下。随着在病区看到和听到医生、护士、病友关于白血病的治疗和讲解，23慢慢接受了自己得病的事实。化疗的第14天，出现了掉头发现象后，她同意剪发，过肩的长发被剪成了齐耳短发。好在，她很看得开，说：”这样很方便，至少出汗不用那么难受了”。 没有料到，头发脱落的速度是那么地快。仅仅2天过后，头发稍微一抓就是一把，整个床上和地下到处都是，打扫卫生的阿姨说”快点剪了吧，打扫卫生不方便”。坚持了几天后，在出院的那一天，给23彻底剪光了所有的头发。她依然很释怀，还在朋友圈让大家推荐戴哪个假发好，我心里感到很意外。 在病房，23每天要经历的三件事就是输液、抽血和用药。 大部分的时间都是在输液，各种各样的药物通过一袋袋的氯化钠和葡萄糖注射进23的血液里，有时是单手臂，有时双手都要打。同时，几乎每天要扎针抽血，各种颜色的抽血管，到后期手背已全部瘀青。至于服药，就像一日三餐一样稀松平常。就这样，每一天都仿佛在重复着昨天。 整个疗程22天下来，一共输入了114袋液体，平均每天5袋，重达23200 ml，大致和一个5岁的小孩体重差不多。一共抽了81管血液，平均每天4管。注射和服用各种各样的药物加起来，有60种之多。化疗两周后，身高168 cm 的23体重降到了79斤，搀扶她上厕所，摸到的都是骨骼。多数女生喜欢越瘦越好，瘦到这种程度可能就不喜欢了吧。 4.2. 23的感受尽管生理和心理都遭受着从未体验过的艰难考验，23的表现大大超出了我的心理预估。不难受的时候，她会发一些朋友圈，每个人看到都会情不自禁地被感动到。 要说，会让23难受的一件事，那就是见不到儿子。从他出生下来至今，从来没有这么多天看不到他。所以，每次和馍馍视频，她都特别开心。 有一天晚上，打电话回家，发现突然找不到馍馍，23急得要命。后来，终于在一个小剧场找到了他，原来和小伙伴在一起。23想哭又有点生气，训了他几句。之后，我妈说原来儿子是想跟着小伙伴的妈妈去进剧场看节目，但他没有票就赖在那里不想走。他在那里哭着说：”我妈妈在就好了，妈妈在就会给我买票。” 听完23就哭了，我也鼻头一酸，2岁半的他竟然会讲出这样的话。 6月23日住了3周院后，23终于能踏出病区，呼吸到外面的空气。回家的路上，她一直看着车窗外，没有说一句话。回到家后第一件事就是抱馍馍，馍馍居然很自然地接受了光头的妈妈。回到房间之后，23突然大哭了一场，吓了我一跳，急忙问怎么了，她说：”活着真好。” 4.3. 陪伴我一直都是一个大老粗，不怎么会照顾人，更不用说病人。好在，家里人给予了非常强力的支持。在23身边，做的最多的仅仅是陪伴。 5. 关于白血病5.1. 什么是白血病在此之前，我的医学常识差到连感冒、发烧该吃什么药都不太清楚。白血病？我甚至有百度过：是不是血液是白色的病？(最好不要去百度) 在医院住久了之后，慢慢地开始对这个病有所了解。 白血病，是一种骨髓中的白细胞大量、异常增生的癌症。这些白细胞会破坏骨髓的造血功能，转而取代正常细胞。初期会导致病人出现发烧、贫血、感染等症状，具体发病原因至今没有完全弄清楚。 虽然是癌症，但受益于科技发展，目前白血病是可以根治的。5年的生存率总体在50%以上，儿童高于成年人，且不同类型的白血病存活率也会不一样。根据细胞类型和发病程度，可将白血病分为四种主要类型： 细胞类型 急性 慢性 淋巴细胞白血病 急性淋巴细胞白血病 慢性淋巴细胞白血病 （ALL） （CLL ） 髓细胞白血病 急性髓性白血病 慢性粒细胞白血病 （AML ） （CML） 儿童中，四分之三患上的是ALL；成人中，AML 和CLL 最为常见，《我不是药神》里面说的是CML。23患上的是ALL，并且属于其中的高危型，存活率为50%。ALL接受造血干细胞移植且5年内没有复发的话，之后终身基本上就不会再复发，也就是会和正常人一样。 普通个体患这种病的概率是十万分之1-1.5，所以很多人对这个病比较陌生，觉得离自己很遥远。其实，不用觉得陌生和遥远。像宋庆龄、肯德基创始人、居里夫人等这些名人就是罹患这种病而逝世的。根据中国红十字会统计，2012年中国内地有400万患者，并且每年新增4万人。 5.2. 治疗方法白血病的治疗通常是采用：化疗、放疗、干细胞移植中的一种或者组合。以23所属的ALL高危类型来说，治疗主要分为三个阶段：化疗缓解、造血干细胞移植和排异控制和恢复。 5.2.1. 化疗刚患病时，骨髓中会产生超出常人数十倍的白细胞，这么多的白细胞需要采用高强度的化疗进行治疗。最终将骨髓中的白血病原始细胞减少至&lt;5％，并从血液中清除肿瘤细胞。这个阶段的治疗通常需要3-6个疗程，然后为下一个阶段做准备。 5.2.2. 造血干细胞捐献移植干细胞具有自我更新、繁殖并能分化成不同种类的成熟细胞的能力。造血干细胞是一群未分化的血液细胞，可以制造运输氧气的红血球、帮助凝血的血小板、抵抗感染的白血球等。所以，造血干细胞可用来治疗许多包括白血病在内的血液疾病、肿瘤等。它存在于骨髓、婴儿脐带血、以及成人周边血液中。 不少人会认为捐献造血干细胞就是抽取骨髓，而抽取骨髓会有很大风险甚至会残疾。所以常出现志愿者反悔的情形，给出的理由常常是”父母不同意”、”儿女不同意”，甚至是”丈母娘不同意”、”领导不同意”。其实，骨髓捐献造血干细胞已经是很久以前的方法了，现在90%以上的捐献者采用的捐献周边血干细胞。其实，这就是一个加强版的抽血过程，血液中左手臂抽出经过一个血液分离机，提取出干细胞，然后再经过右手臂回输到自己身体中。整个过程持续半天，最终抽取出200-300ml的血液就算完成捐献了。 对于病人来说，造血干细胞移植，就是利用供者的干细胞的造血能力全部换掉自身体内的血液，从而彻底清除癌变的白细胞，然后重新造血。因此，病人的血型在移植完后会变成供体的血型。 供体的选择，通常是按照：兄弟姐妹、直系亲属、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序去筛选寻找。 判断供体和不合适，其实和常人输液一样，如果你是A型血，那你需要同样是A型血的人，B型血的人就爱莫能助。只不过移植不是用血型来判断，而是根据HLA（人类白细胞抗原）。 HLA（人类白细胞抗原）是编码人类的主要组织相容性复合体的基因，负责调节人体免疫系统。简单地说，HLA是人体生物学的”身份证”，由父母遗传，能识别”自己”和”非己”，会通过免疫反应排除”非己”。所以，HLA在造血干细胞移植的成败中起着重要的作用。造血干细胞移植就需要供体和受体HLA配型相同，如果不相同就会发生致命的排斥反应。 由于不同人种、不同种族、不同个体的HLA差别很大，所以要采用一种方法来确定HLA。判断两个人的HLA是否相同，只需要抽8ml的血，做HLA高分辨检测化验，然后将HLA上5组基因的数字进行对比就能判定。 简单地举个例子，供体1的10组数字和受体全部对应，称为”全相合”，这是最理想的，移植效果也最好；供体2有5组数字和受体对应，称为”半相合”，如果是直系亲属，那么可以移植，如果是非血缘供提，那么无法移植；供体3一组数字都对不上，称为”零相合”，完全不可用。类似地还会有9组数字对应的情况，称为9个点匹配，以此类推。 尽管判断HLA是否相同很简单，但是供体的选择范围其实很有限。按照兄弟姐妹、父母子女、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序来解释一下。 兄弟姐妹 根据染色体的基本常识，正常情况下，子女的染色体一半来自父亲、一半来自母亲。可以知道，兄弟姐妹有25%的概率是全相合、50%的概率是半相合、剩下25%则是零相合。可以说，只要有兄弟姐妹，供体基本就找到了，但很多人没有兄弟姐妹，尤其90后。 父母子女 没有兄弟姐妹的人，可以将父母子、女身作为潜在的供体对象。绝大多数情况下，父母和子女之间是半相合，这是最常见的。目前，国内半相合移植技术发展地很好，很多患者才得以生存下来。但是也有条件限制，比如在健康的条件下，父母年龄最好不要超过60岁，子女体重要达到一定要求才行。 堂（表）兄弟姐妹 在堂（表）兄弟姐妹中，这个概率其实就比较低了，但如果前两类人群中无法找到，也只能去试一下。 骨髓库志愿者 上面三类人群都无法找到，那么只能寄希望于骨髓库中的志愿者了。没有血缘关系的人，HLA能匹配上的概率非常低，只有数十万到上百万分之一！但依然得抱着希望去尝试，因为这是最后一根救命稻草。 下面，说一下骨髓库和志愿者。 截至2018年9月，全球骨髓库有3250万志愿者，占全世界75亿人的千分之四左右。中国内地排在美国、德国和巴西之后，位列世界第四，志愿者的人数为250万，仅占总人口比例的0.18%，比其他国家（地区）低一个数量级。如果能够达到台湾1.79%的比例，那么我们的志愿者人数将达到2500万！是现在的十倍。 250万这个数字，还只是登记在库内的初查人数，实际上真正愿意做志愿者的人，会比这低很多。而且，从2001年成立到现在，17年只捐献了7600例，意味着最多有7600个患者得到了救治，每年只有平均450人。第一大骨髓库美国，30年来也不过，只有8万人得到了救治。可以说，不仅国内，全世界骨髓库都很需要志愿者的加入。 骨髓库 成立时间（年） 库容志愿者人数（万） 人口（亿） 占人口比例 捐献造血干细胞例数 美国 1988 900 3.3 2.73% 80,000 德国 1993 400 0.83 9.37% - 巴西 - 500 2.12 2.36% - 中国 2001 250 13.9 0.18% 7,600 台湾 1993 43 0.24 1.79% 5,100 参考来源：WMDA global trend report 2017、 http://lnmdp.org.admin.yuntai.net/readnews.aspx?n_id=1817&amp;type=4 5.2.3. 控制排异移植完成后，供体的干细胞进入病人身体，自身细胞会视为外来敌人然后会发生排斥。可以比喻成战争，视战役的惨烈程度分为两种情况：第一种小规模或者和平停战，那么移植算成功，慢慢恢复就能康复起来；第二种是拼到了两败俱伤的境地，就会引起严重的排异反应，这就会致命。控制排异和恢复的过程需要持续数年。 5.3. 治疗费用根据治疗方案就可以大致计算白血病的治疗费用。23的治疗费用主要包括：化疗费用+移植费用+排异控制费用+服药费用。简单做了一下费用预估： 化疗费用15-40万 移植费用30-50万 排异控制费用：可能几万，也可能几百万 口服药（几万到数十万）。 拿口服药举个例子，服用进口药，60片共3g要8500元。目前，千足金（99.99%）的价格是265元/g，这种药的单价是其10倍不止。一瓶够服用一个月，服用时间按年计算。 所以，白血病可能50万能治好，也可能花了500万，仍然留不住病人。 6. 希望之光很多亲朋好友在得知我们的情况后，非常关心并且给予了很多的帮助，让我们不要有压力。 我觉得，压力大不大最好是去对比一下。一对比，很容易就能得出答案。3个多月以来，住了3家医院，接触和听说过不少病友的故事。比如：有怀身孕6个月的妻子照顾患病的丈夫、这辈子几乎无法再当妈妈的未婚女生、撇下2岁多孩子消失的父亲等等。 相比之下，我们的压力不算大，唯一的压力来自于供体的寻找。 9月13日 医生告诉了一个久违的好消息：在台湾骨髓库找到了一个全相合且有意愿的供者。全家听到后都特别的开心，祈祷这位同胞体检能够通过，最终顺利捐出造血干细胞。 2015年，世界骨髓捐献者协会宣布：每年9月的第3个周六，为”世界骨髓捐献者日”。 本文完。 链接： 世界骨髓捐献者协会WMDA：https://www.wmda.info/ 世界骨髓捐献者日网站WMDD：https://worldmarrowdonorday.org/ 中华骨髓库CMDP：http://www.cmdp.org.cn/ 台湾慈濟骨髓幹細胞中心BTCSCC：http://btcscc.tzuchi.com.tw/ 香港骨髓庫：https://www5.ha.org.hk/rcbts/hkarticle.asp?bid=56&amp;MenuID=6#.W52iwfl4nDc 美国骨髓库NMDP：https://bethematch.org/ 欧洲骨髓库EMDIS：www.emdis.net/ 德国骨髓库ZKRD：https://www.zkrd.de/de/ 日本骨髓库JMDP： http://www.jmdp.or.jp/ 白血病介绍：https://en.m.wikipedia.org/wiki/Leukemia 急性淋巴细胞白血病介绍： https://en.m.wikipedia.org/wiki/Acute_lymphoblastic_leukemia HLA介绍：https://en.m.wikipedia.org/wiki/Human_leukocyte_antigen]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>白血病</tag>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据处理分析(1)：日期型数据处理]]></title>
    <url>%2Fpython_data_analysis01.html</url>
    <content type="text"><![CDATA[之前用爬虫实现了上市公司信息的抓取。但还有更简单的方法，通过调用Tushare包，可以很便捷地拿到干净的各种股市数据。 以金融股票分析包Tushare为案例，介绍日期型数据的处理。 摘要： Python数据处理分析中，日期型数据的处理是相对复杂且非常重要的一环。Tushare是一个开源免费、强大的python财经数据接口包。调用该包返回的数据格式基本是Pandas DataFrame类型，非常便于后续处理分析。本文通过调用该包获得股票的各种信息数据，介绍日期数据的处理，然后结合D3.js展示股市多项指标的的动态变化。 先推荐一下这款由国内团队开发的包，Github上目前Star数 6000+。包的数据来源于新浪财经、腾讯财经、上交所和深交所，比较齐全，质量也很可靠。 参考：https://tushare.pro/document/2https://github.com/waditu/Tushare 获取数据接口使用前提：首先在官网注册成功后获得token，然后通过下面命令下载Tushare包，然后在程序中调用就可以使用了。1pip instasll tushare 可以获得的信息接口非常多，包括：行情数据、基础数据、财务数据板块等。 下面就简单使用下部分接口。首先，获取国内股票列表数据。123456import tushare as tsts.set_token('你的token')pro = ts.pro_api()data = pro.stock_basic(exchange_id='', is_hs='', fields='symbol,name,is_hs,list_date,list_status')print(data)# ''表示获取全部 exchange_id表示股票代码，可以获取特定股票的基础信息，为空则获取全部;is_hs表示是否沪深港通，为空表示提取所有股市；fields表示想要提取的信息列表。 结果如下： 1234567891011121314151617181920212223 ts_code symbol name list_status list_date is_hs0 000001.SZ 000001 平安银行 L 19910403 S1 000002.SZ 000002 万科A L 19910129 S2 000004.SZ 000004 国农科技 L 19910114 N3 000005.SZ 000005 世纪星源 L 19901210 N4 000006.SZ 000006 深振业A L 19920427 S5 000007.SZ 000007 全新好 L 19920413 N6 000008.SZ 000008 神州高铁 L 19920507 S7 000009.SZ 000009 中国宝安 L 19910625 S8 000010.SZ 000010 美丽生态 L 19951027 N9 000011.SZ 000011 深物业A L 19920330 S10 000012.SZ 000012 南玻A L 19920228 S···3532 603987.SH 603987 康德莱 L 20161121 N3533 603988.SH 603988 中电电机 L 20141104 N3534 603989.SH 603989 艾华集团 L 20150515 H3535 603990.SH 603990 麦迪科技 L 20161208 N3536 603991.SH 603991 至正股份 L 20170308 N3537 603993.SH 603993 洛阳钼业 L 20121009 H3538 603996.SH 603996 中新科技 L 20151222 N3539 603997.SH 603997 继峰股份 L 20150302 H3540 603998.SH 603998 方盛制药 L 20141205 N3541 603999.SH 603999 读者传媒 L 20151210 N 很轻松地就能获得3542家上市公司的基本情况。下面就将这个数据作为日期型处理的基础数据。 日期型数据处理查看一下数据结构： 123456789RangeIndex: 3542 entries, 0 to 3541Data columns (total 6 columns):ts_code 3542 non-null objectsymbol 3542 non-null objectname 3542 non-null objectlist_status 3542 non-null objectlist_date 3542 non-null objectis_hs 3542 non-null objectdtypes: object(6) 所有列都是object字符型。这里想对日期做数据分析，比如可以统计一下历年上市公司数量。需更改日期型数据字符型为日期型。1data['list_date'] = pd.to_datetime(data['list_date']) pd.to_datetime将’list_date’列格式改为datetime格式，再来看一下： 123456789RangeIndex: 3542 entries, 0 to 3541Data columns (total 6 columns):ts_code 3542 non-null objectsymbol 3542 non-null objectname 3542 non-null objectlist_status 3542 non-null objectlist_date 3542 non-null datetime64[ns]is_hs 3542 non-null objectdtypes: object(6) 按日期切片筛选数据有时候我们需要按年、季度、月、日这样的日期格式来筛选提取相应的数据。 按年度 获取单一年份数据，比如2017年 1234567891011data = data.set_index(data['list_date'])data = data['2017']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2017-12-25 001965.SZ 001965 招商公路 L 2017-12-25 S2017-03-24 002774.SZ 002774 快意电梯 L 2017-03-24 N2017-01-12 002824.SZ 002824 和胜股份 L 2017-01-12 N2017-01-06 002838.SZ 002838 道恩股份 L 2017-01-06 N2017-01-24 002839.SZ 002839 张家港行 L 2017-01-24 S 获取多个年份，比如2015-2017 12345678910data = data['2015':'2017']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2015-01-26 000166.SZ 000166 申万宏源 L 2015-01-26 S2017-12-25 001965.SZ 001965 招商公路 L 2017-12-25 S2015-12-30 001979.SZ 001979 招商蛇口 L 2015-12-30 S2015-01-27 002734.SZ 002734 利民股份 L 2015-01-27 N2015-01-22 002739.SZ 002739 万达电影 L 2015-01-22 S 按月度12345678910data = data['2017-1']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2017-01-12 002824.SZ 002824 和胜股份 L 2017-01-12 N2017-01-06 002838.SZ 002838 道恩股份 L 2017-01-06 N2017-01-24 002839.SZ 002839 张家港行 L 2017-01-24 S2017-01-10 002840.SZ 002840 华统股份 L 2017-01-10 N2017-01-19 002841.SZ 002841 视源股份 L 2017-01-19 S 按具体天123456789data = data['2017-1-12']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2017-01-12 002824.SZ 002824 和胜股份 L 2017-01-12 N2017-01-12 300584.SZ 300584 海辰药业 L 2017-01-12 N2017-01-12 603628.SH 603628 清源股份 L 2017-01-12 H2017-01-12 603639.SH 603639 海利尔 L 2017-01-12 H to_period按日期显示数据dataframe.to_period方法只是用于显示数据，但不会进行统计。 按年度12345678910data = data.to_period('A') # 'A'默认是从'A-DEC'开始算,也可以根据情况设置为'A-JAN'print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 1991 000001.SZ 000001 平安银行 L 1991-04-03 S1991 000002.SZ 000002 万科A L 1991-01-29 S1991 000004.SZ 000004 国农科技 L 1991-01-14 N1990 000005.SZ 000005 世纪星源 L 1990-12-10 N1992 000006.SZ 000006 深振业A L 1992-04-27 S 可以看到，相比上面筛选数据时是按原始的日期，这里利用to_period方法，设置参数为’A’后，可以直接显示为年，这在后期可视化绘图时非常有用。 按季度12345678910data = data.to_period('Q') # 'Q'默认是从'Q-DEC'开始算,也可以根据情况设置为“Q-SEP”，“Q-FEB”等print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 1991Q2 000001.SZ 000001 平安银行 L 1991-04-03 S1991Q1 000002.SZ 000002 万科A L 1991-01-29 S1991Q1 000004.SZ 000004 国农科技 L 1991-01-14 N1990Q4 000005.SZ 000005 世纪星源 L 1990-12-10 N1992Q2 000006.SZ 000006 深振业A L 1992-04-27 S 按月度12345678910data = data.to_period('M')print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 1991-04 000001.SZ 000001 平安银行 L 1991-04-03 S1991-01 000002.SZ 000002 万科A L 1991-01-29 S1991-01 000004.SZ 000004 国农科技 L 1991-01-14 N1990-12 000005.SZ 000005 世纪星源 L 1990-12-10 N1992-04 000006.SZ 000006 深振业A L 1992-04-27 S resample按日期统计数据按日期进行统计数据，可以利用resample方法。 按年度123456789data = data.resample('AS').count()['name'] # count对各年上市公司数量进行计数print(data.head())# 结果list_date1990-01-01 71991-01-01 41992-01-01 371993-01-01 1061994-01-01 99 按季度123456789data = data.resample('Q').count()['name'] print(data.head())# 结果list_date1990-12-31 71991-03-31 21991-06-30 21991-09-30 01991-12-31 0 按月度123456789data = data.resample('M').count()['name'] print(data.head())# 结果list_date1990-12-31 71991-01-31 21991-02-28 01991-03-31 01991-04-30 1 统计和显示结合利用前面的resample和to.period方法，可以按年、季度、月份汇总数据。 123456# 汇总各年上市公司数量data = data.set_index(['list_date'])data = data.resample('AS').count()['ts_code']data = data.to_period('A')print(data.head())print(data.tail()) 结果如下：1234567891011121314list_date1990 71991 41992 371993 1061994 99Freq: A-DEC, Name: name, dtype: int64list_date2014 1242015 2232016 2272017 4382018 78Freq: A-DEC, Name: name, dtype: int64 基于上述数据，可以利用matplotlib绘制出历年上市公司数量的折线图： 折线图的具体绘制方法，见后续文章。 本文完。]]></content>
      <categories>
        <category>Python数据清洗处理</category>
      </categories>
      <tags>
        <tag>Python数据分析</tag>
        <tag>Tushare</tag>
        <tag>Python包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python可视化(1)：折线图]]></title>
    <url>%2Fpython_data_visualization01.html</url>
    <content type="text"><![CDATA[日期型数据的折线图绘制。 摘要： 利用matplotlib绘制横轴为日期格式的折线图时，存在不少技巧。本文借助Tushare包返回的股票数据，介绍日期折线图绘制的方法。最后结合D3.js展示股市多项指标的的动态变化。 折线图绘制的数据源，采用Tushare包获取的上市公司基本数据表，格式如下： 123456789import pandas as pddata = pd.read('get_stock_basics.csv',encoding = 'utf8')print(data.head())ts_code symbol name list_status list_date is_hs000001.SZ 1 平安银行 L 19910403 S000002.SZ 2 万科A L 19910129 S000004.SZ 4 国农科技 L 19910114 N000005.SZ 5 世纪星源 L 19901210 N 然后利用resample和to.period方法汇总各年度的上市公司数量数据，格式为Pandas.Series数组格式。 1234567891011121314151617181920# 汇总各年上市公司数量data = data.set_index(['list_date'])data = data.resample('AS').count()['ts_code']data = data.to_period('A')print(data.head())print(data.tail())# 结果如下：list_date1990 71991 41992 371993 1061994 99...list_date2014 1242015 2232016 2272017 4382018 78 Series直接绘制折线图我们可以直接利用数组绘制折线图： 123456789101112import matplotlib.pyplot as pltplt.style.use('ggplot') # 设置绘图风格fig = plt.figure(figsize = (10,6)) # 设置图框的大小ax1 = fig.add_subplot(1,1,1)data.plot() # 绘制折线图# 设置标题及横纵坐标轴标题colors1 = '#6D6D6D' #设置标题颜色为灰色plt.title('历年中国内地上市公司数量变化',color = colors1,fontsize = 18)plt.xlabel('年份')plt.ylabel('数量(家)')plt.show() 可以发现，图中存在两个问题：一是缺少数值标签，二是横坐标年份被自动分割了，而我们需要是每年的数据都显示。要解决这两个问题，我们需要重新绘制折线图。 折线图完善12345678910111213141516171819# 创建x,y轴标签x = np.arange(0,len(data),1) ax1.plot(x,data.values, #x、y坐标 color = '#C42022', #折线图颜色为红色 marker = 'o',markersize = 4 #标记形状、大小设置 )ax1.set_xticks(x) # 设置x轴标签为自然数序列ax1.set_xticklabels(data.index) # 更改x轴标签值为年份plt.xticks(rotation=90) # 旋转90度，不至太拥挤for x,y in zip(x,data.values): plt.text(x,y + 10,'%.0f' %y,ha = 'center',color = colors1,fontsize = 10 ) # '%.0f' %y 设置标签格式不带小数# 设置标题及横纵坐标轴标题plt.title('历年中国内地上市公司数量变化',color = colors1,fontsize = 18)plt.xlabel('年份')plt.ylabel('数量(家)')# plt.savefig('stock.png',bbox_inches = 'tight',dpi = 300)plt.show() 完善后的折线图如下： 可以看到，x轴逐年的数据都显示并且数值标签也添加上了。]]></content>
      <categories>
        <category>Python可视化</category>
      </categories>
      <tags>
        <tag>Python可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于PDF阅读处理软件，你需要的都在这里了]]></title>
    <url>%2Ffuli01.html</url>
    <content type="text"><![CDATA[分享几款佳软。 摘要： 我们办公中经常要和pdf打交道，包括同各种软件的转换、提取分割、压缩等处理操作，但你可能常常苦于找不到合适的软件。这里和你分享6款最棒的软件，满足你对pdf所有的需求。 先推荐两款pc和手机上比较好用的pdf阅读软件，这两款都是难得的国产良心之作。 1. Foxit Reader一句话概括：这款国产软件足以媲美大名鼎鼎的Adobe Acrobat。 2. WPS Office一句话概括：手机版的和电脑版的WPS可谓”天壤之别”，功能足够强大。 3. 最好用的OCR文字识别软件一句话概括：知乎上PDF话题中最多赞文章中的推荐。官方售价500+RMB。https://www.zhihu.com/topic/19556393/top-answers它最实用最强大的的功能：当你的pdf是扫描或者图片形式的，那么你无法进行复制但你又想复制，此时，这款软件能帮你搞定一切。不仅是pdf，手机拍的照片文字也可以识别然后转化成word。 4. Small PDF一句话概括：网页版很好用，功能齐全，但是有次数限制，专业版费用是每年300+RMB。 5. Solid PDF Converterhttps://www.soliddocuments.com/zh/features.htm?product=SolidConverterPDF一句话概括：当pdf是可复制的前提下，转换成word，excel和ppt的效果最好。官方售价100美元。 6. PDFdo一句话概括：它最实用最强大的的功能是合并、分割、提取等功能。官方售价99RMB。 以上这几款软件足以解决日常遇到的大部分问题。如需，公众号后台回复pdf即可得到。]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>pdf</tag>
        <tag>佳软</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pyhton可视化(1): 中国大学学术排行榜]]></title>
    <url>%2FPython_visualization01.html</url>
    <content type="text"><![CDATA[Python爬虫近十年中国大学Top20强并结合D3.js做动态数据可视化表。 摘要：：最近在朋友圈看到一个很酷炫的动态数据可视化表，介绍了新中国成立后各省GDP的发展历程，非常惊叹竟然还有这种操作，也想试试。于是，照葫芦画瓢虎，在网上爬取了历年中国大学学术排行榜，制作了一个中国大学排名Top20强动态表。 1. 作品介绍这里先放一下这个动态表是什么样的：https://www.bilibili.com/video/av24503002 不知道你看完是什么感觉，至少我是挺震惊的，想看看作者是怎么做出来的，于是追到了作者的B站主页，发现了更多有意思的动态视频： 这些作品的作者是：@Jannchie见齐，他的主页：https://space.bilibili.com/1850091/#/video 这些会动的图表是如何做出来的呢？他用到的是一个动态图形显示数据的JavaScript库：D3.js，一种前端技术。难怪不是一般地酷炫。那么，如果不会D3.js是不是就做不出来了呢？当然不是，Jannchie非常Open地给出了一个手把手简单教程：https://www.bilibili.com/video/av28087807 他同时还开放了程序源码，你只需要做2步就能够实现： 到他的Github主页下载源码到本地电脑：https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js 打开dist文件夹里面的exampe.csv文件，放进你想要展示的数据，再用浏览器打开bargraph.html网页，就可以实现动态效果了。 下面，我们稍微再说详细一点，实现这种效果的关键点。最重要的是要有数据。观察一下上面的作品可以看到，横向柱状图中的数据要满足两个条件：一是要有多个对比的对象，二是要在时间上连续。这样才可以做出动态效果来。 看完后我立马就有了一个想法：想看看近十年中国的各个大学排名是个什么情况。下面我们就通过实实例来操作下。 2. 案例操作：中国大学Top20强2.1. 数据来源世界上最权威的大学排名有4类，分别是： 原上海交通大学的ARWU http://www.shanghairanking.com/ARWU2018.html 英国教育组织的QShttps://www.topuniversities.com/university-rankings/world-university-rankings/2018 泰晤士的THEhttps://www.timeshighereducation.com/world-university-rankings 美国的usnewshttps://www.usnews.com/best-colleges/rankings 关于，这四类排名的更多介绍，可以看这个：https://www.zhihu.com/question/20825030/answer/71336291 这里，我们选取相对比较权威也比较符合国情的第一个ARWU的排名结果。打开官网，可以看到有英文版和中文版排名，这里选取中文版。排名非常齐全，从2003年到最新的2018年都有，非常好。 同时，可以看到这是世界500强的大学排名，而我们需要的是中国（包括港澳台）的大学排名。怎么办呢？ 当然不能一年年地复制然后再从500条数据里一条条筛选出中国的，这里就要用爬虫来实现了。可以参考不久前的一篇爬取表格的文章：https://www.makcyun.top/web_scraping_withpython2.html 2.2. 抓取数据2.2.1. 分析url首先，分析一下URL: 1234http://www.zuihaodaxue.com/ARWU2018.htmlhttp://www.zuihaodaxue.com/ARWU2017.html...http://www.zuihaodaxue.com/ARWU2009.html 可以看到，url非常有规律，只有年份数字在变，很简单就能构造出for循环。格式如下： 1url = 'http://www.zuihaodaxue.com/ARWU%s.html' % (str(year)) 下面就可以开始写爬虫了。 2.2.2. 获取网页内容1234567891011import requeststry: url = 'http://www.zuihaodaxue.com/ARWU%s.html' % (str(year)) response = requests.get(url,headers = headers) # 2009-2015用'gbk'，2016-2018用'utf-8' if response.status_code == 200: # return response.text # text会乱码，content没有问题 return response.content return Noneexcept RequestException:print('爬取失败') 上面需要注意的是，不同年份网页采用的编码不同，返回response.test会乱码，返回response.content则不会。关于编码乱码的问题，以后单独写一篇文章。 2.2.3. 解析表格用read_html函数一行代码来抓取表格，然后输出：12tb = pd.read_html(html)[0]print(tb) 可以看到，很顺利地表格就被抓取了下来：但是表格需要进行处理，比如删除掉不需要的评分列，增加年份列等，代码实现如下： 1234567891011121314tb = pd.read_html(html)[0]# 重命名表格列，不需要的列用数字表示tb.columns = ['world rank','university', 2,3, 'score',5,6,7,8,9,10]tb.drop([2,3,5,6,7,8,9,10],axis = 1,inplace = True)# 删除后面不需要的评分列# rank列100名后是区间，需需唯一化，增加一列index作为排名tb['index_rank'] = tb.indextb['index_rank'] = tb['index_rank'].astype(int) + 1# 增加一列年份列tb['year'] = i# read_html没有爬取country，需定义函数单独爬取tb['country'] = get_country(html)return tb 需要注意的是，国家没有被抓取下来，因为国家是用的图片表示的，定位到国家代码位置： 可以看到美国是用英文的USA表示的，那么我们可以单独提取出src属性，然后用正则提取出国家名称就可以了，代码实现如下： 1234567891011# 提取国家名称def get_country(html): soup = BeautifulSoup(html,'lxml') countries = soup.select('td &gt; a &gt; img') lst = [] for i in countries: src = i['src'] pattern = re.compile('flag.*\/(.*?).png') country = re.findall(pattern,src)[0] lst.append(country) return lst 然后，我们就可以输出一下结果： 12345678910111213141516171819202122 world rank university score index_rank year country0 1 哈佛大学 100.0 1 2018 USA1 2 斯坦福大学 75.6 2 2018 USA2 3 剑桥大学 71.8 3 2018 UK3 4 麻省理工学院 69.9 4 2018 USA4 5 加州大学-伯克利 68.3 5 2018 USA5 6 普林斯顿大学 61.0 6 2018 USA6 7 牛津大学 60.0 7 2018 UK7 8 哥伦比亚大学 58.2 8 2018 USA8 9 加州理工学院 57.4 9 2018 USA9 10 芝加哥大学 55.5 10 2018 USA10 11 加州大学-洛杉矶 51.2 11 2018 USA11 12 康奈尔大学 50.7 12 2018 USA12 12 耶鲁大学 50.7 13 2018 USA13 14 华盛顿大学-西雅图 50.0 14 2018 USA14 15 加州大学-圣地亚哥 47.8 15 2018 USA15 16 宾夕法尼亚大学 46.4 16 2018 USA16 17 伦敦大学学院 46.1 17 2018 UK17 18 约翰霍普金斯大学 45.4 18 2018 USA18 19 苏黎世联邦理工学院 43.9 19 2018 Switzerland19 20 华盛顿大学-圣路易斯 42.1 20 2018 USA20 21 加州大学-旧金山 41.9 21 2018 USA 数据很完美，接下来就可以按照D3.js模板中的example.csv文件的格式作进一步的处理了。 2.3. 数据处理这里先将数据输出为university.csv文件，结果见下表： 10年一共5011行×6列数据。接着，读入该表作进一步数据处理，代码如下： 123456789101112131415161718192021222324252627df = pd.read_csv('university.csv')# 包含港澳台# df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]# 只包括内地df = df.query("(country == 'China')")df['index_rank_score'] = df['index_rank']# 将index_rank列转为整形df['index_rank'] = df['index_rank'].astype(int)# 美国# df = df.query("(country == 'UnitedStates')|(country == 'USA')") #求topn名def topn(df): top = df.sort_values(['year','index_rank'],ascending = True) return top[:20].reset_index()df = df.groupby(by =['year']).apply(topn)# 更改列顺序df = df[['university','index_rank_score','index_rank','year']]# 重命名列df.rename (columns = &#123;'university':'name','index_rank_score':'type','index_rank':'value','year':'date'&#125;,inplace = True)# 输出结果df.to_csv('university_ranking.csv',mode ='w',encoding='utf_8_sig', header=True, index=False)# index可以设置 上面需要注意两点： 可以提取包含港澳台在内的大中华区所有的大学，也可以只提取内地的大学，还可以提取世界、美国等各种排名。 定义了一个求Topn的函数，能够按年份分别求出各年的前20名大学名单。 打开输出的university_ranking.csv文件： 结果非常好，可以直接作为D3.js的导入文件了。 2.3.1. 完整代码将代码再稍微完善一下，完整地代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import pandas as pdimport csvimport requestsfrom requests.exceptions import RequestExceptionfrom bs4 import BeautifulSoupimport timeimport restart_time = time.time() #计算程序运行时间# 获取网页内容def get_one_page(year): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; # 英文版 # url = 'http://www.shanghairanking.com/ARWU%s.html' % (str(year)) # 中文版 url = 'http://www.zuihaodaxue.com/ARWU%s.html' % (str(year)) response = requests.get(url,headers = headers) # 2009-2015用'gbk'，2016-2018用'utf-8' if response.status_code == 200: # return response.text # text会乱码，content没有问题 # https://stackoverflow.com/questions/17011357/what-is-the-difference-between-content-and-text return response.content return None except RequestException: print('爬取失败')# 解析表格def parse_one_page(html,i): tb = pd.read_html(html)[0] # 重命名表格列，不需要的列用数字表示 tb.columns = ['world rank','university', 2,3, 'score',5,6,7,8,9,10] tb.drop([2,3,5,6,7,8,9,10],axis = 1,inplace = True) # 删除后面不需要的评分列 # rank列100名后是区间，需需唯一化，增加一列index作为排名 tb['index_rank'] = tb.index tb['index_rank'] = tb['index_rank'].astype(int) + 1 # 增加一列年份列 tb['year'] = i # read_html没有爬取country，需定义函数单独爬取 tb['country'] = get_country(html) # print(tb) # 测试表格ok return tb # print(tb.info()) # 查看表信息 # print(tb.columns.values) # 查看列表名称# 提取国家名称def get_country(html): soup = BeautifulSoup(html,'lxml') countries = soup.select('td &gt; a &gt; img') lst = [] for i in countries: src = i['src'] pattern = re.compile('flag.*\/(.*?).png') country = re.findall(pattern,src)[0] lst.append(country) return lst # print(lst) # 测试提取国家是否成功ok# 保存表格为csvdef save_csv(tb): tb.to_csv(r'university.csv', mode='a', encoding='utf_8_sig', header=True, index=0) endtime = time.time()-start_time # print('程序运行了%.2f秒' %endtime)def analysis(): df = pd.read_csv('university.csv') # 包含港澳台 # df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']] # 只包括内地 df = df.query("(country == 'China')")[['university','year','index_rank']] df['index_rank_score'] = df['index_rank'] # 将index_rank列转为整形 df['index_rank'] = df['index_rank'].astype(int) # 美国 # df = df.query("(country == 'UnitedStates')|(country == 'USA')")[['university','year','index_rank']] #求topn名 def topn(df): top = df.sort_values(['year','index_rank'],ascending = True) return top[:20].reset_index() df = df.groupby(by =['year']).apply(topn) # 更改列顺序 df = df[['university','index_rank_score','index_rank','year']] # 重命名列 df.rename (columns = &#123;'university':'name','index_rank_score':'type','index_rank':'value','year':'date'&#125;,inplace = True) # 输出结果 df.to_csv('university_ranking.csv',mode ='w',encoding='utf_8_sig', header=True, index=False) # index可以设置def main(year): # generate_mysql() for i in range(2009,year): #抓取10年 # get_one_page(i) html = get_one_page(i) # parse_one_page(html,i) # 测试表格ok tb = parse_one_page(html,i) save_csv(tb) print(i,'年排名提取完成完成') analysis()# # 单进程if __name__ == '__main__': main(2019) # 2016-2018采用gb2312编码，2009-2015采用utf-8编码 至此，我们已经有university_ranking.csv基础数据，下面就可以进行可视化呈现了。 2.4. 可视化呈现首先，到见齐的github主页：https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js 2.4.1. 克隆仓库文件如果你平常使用github或者Git软件的话，那么就找个合适文件存放目录，然后直接在 GitBash里分别输入下面3条命令就搭建好环境了： 123456# 克隆项目仓库git clone https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js# 切换到项目根目录cd Historical-ranking-data-visualization-based-on-d3.js# 安装依赖npm install 如果你此前没有用过上面的软件，你可以直接点击Download Zip下载下来然后解压即可，不过还是强烈建议使用第一种方法，因为后面如果要自定义可视化效果的话，需要修改代码然后执行npm run build命令才能够看到效果。 2.4.2. 效果呈现好，所有基本准备都已完成，下面就可以试试看效果了。任意浏览器打开bargraph.html网页，点击选择文件，然后选择：前面输出的university_ranking.csv文件，看下效果吧：https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0 可以看到，有了大致的可视化效果，但还存在很多瑕疵，比如：表顺序颠倒了、字体不合适、配色太花哨等。可不可以修改呢？ 当然是可以的，只需要分别修改文件夹中这几个文件的参数就可以了： config.js 全局设置各项功能的开关，比如配色、字体、文字名称、反转图表等等功能； color.css 修改柱形图的配色； stylesheet.css 具体修改配色、字体、文字名称等的css样式； visual.js 更进一步的修改，比如图表的透明度等。 知道在哪里修改了以后，那么，如何修改呢？很简单，只需要简单的几步就可以实现： 打开网页，右键-检查，箭头指向想要修改的元素，然后在右侧的css样式表里，双击各项参数修改参数，修改完元素就会发生变化，可以不断微调，直至满意为止。 把参数复制到四个文件中对应的文件里并保存。 Git Bash不断重复运行npm run build，之后刷新网页就可以看到优化后的效果。 最后，再添加一个合适的BGM就可以了。以下是我优化之后的效果：https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0BGM：ツナ覚醒 如果你不太会调整，没有关系，我会分享优化后的配置文件。 以上，就是实现动态可视化表的步骤。 同样地，只要更改数据源可以很方便地做出世界、美国等大学的动态效果，可以看看：中国（含港澳台）大学排名：http://pc1lljdwb.bkt.clouddn.com/Greater_China_uni_ranking.mp4美国大学排名：http://pc1lljdwb.bkt.clouddn.com/USA_uni_ranking.mp4 文章所有的素材，在公众号后台回复大学排名就可以得到，或者到我的github下载：https://github.com/makcyun/web_scraping_with_python感兴趣的话就动手试试吧。 本文完。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(4)：图片批量下载-以澎湃网信息图为例]]></title>
    <url>%2Fweb_scraping_withpython4.html</url>
    <content type="text"><![CDATA[澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。 摘要： 上一篇文章介绍了单页图片的爬取，但是当爬取多页时，难度会增加。同时，前几篇爬虫文章中的网站有一个明显的特点是：可以通过点击鼠标实现网页的翻页，并且url会发生相应的变化。除了此类网站以外，还有一类非常常见的网站特点是：没有”下一页”这样的按钮，而是”加载更多”或者会不断自动刷新从而呈现出更多的内容，同时网页url也不发生变化。这种类型的网页通常采用的是Ajax技术，要抓取其中的网页内容需要采取一定的技巧。本文以信息图做得非常棒的澎湃”美数课”为例，抓取该栏目至今所有文章的图片。栏目网址：https://www.thepaper.cn/list_25635 本文知识点： Ajax知识 多页图片爬取 1. Ajax知识在该主页上尝试不断下拉，会发现网页不断地加载出新的文章内容来，而并不需要通过点击”下一页”来实现，而且网址url也保持不变。也就是说在同一个网页中通过下拉源源不断地刷新出了网页内容。这种形式的网页在今天非常常见，它们普遍是采用了Ajax技术。 Ajax 全称是 Asynchronous JavaScript and XML（异步 JavaScript 和 XML）。它不是一门编程语言，而是利用 JavaScript 在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。 Ajax更多参考：https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html 采用了Ajax的网页和普通的网页有一定的区别，普通网页的爬虫代码放在这种类型的网页上就行不通了，必须另辟出路。下面我们就来尝试一下如何爬取网易”数读”所有的文章。 主页右键-检查，然后按f5刷新，会弹出很多链接文件。鼠标上拉回到第一个文件：list_25635，在右侧按ctrl+f搜索一下第一篇文章的标题：”娃娃机生意经”，可以看到在html网页中找到了对应的源代码。 接着，我们拖动下拉鼠标，显示出更多文章。然后再次搜索一篇文章的标题：”金砖峰会”，会发现搜不到相应的内容了。是不是感觉很奇怪？ 其实，这里就是用了Ajax的技术，和普通网页翻页是刷新整个网页不同，这种类型网页可以再保持url不变的前提下只刷新部分内容。这就为我们进行爬虫带来了麻烦。因为，我们通过解析网页的url：https://www.thepaper.cn/list_25635只能爬取前面部分的内容而后面通过下拉刷新出来的内容是爬取不到的。这显然不完美，那么怎么才能够爬取到后面不断刷新出来的网页内容呢？ 2. url分析我们把右侧的选项卡从ALL切换到Network，然后按再次按f5刷新，可以发现Name列有4个结果。选择第3个链接打开并点击Response，通过滑动可以看到一些文本内容和网页中的文章标题是一一对应的。比如第一个是：娃娃机生意经｜有没有好奇过抓娃娃机怎么又重新火起来了？，一直往下拖拽可以看到有很多篇文章。此时，再切换到headers选项卡，复制Request URL后面的链接并打开，会显示一部分文章的标题和图片内容。数一下的话，可以发现一共有20个文章标题，也就是对应着20篇文章。 这个链接其实和上面的list_25635链接的内容是一致的。这样看来，好像发现不了什么东西，不过不要着急。 接下来，回到Name列，尝试滚动下拉鼠标，会发现弹出好几个新的开头为load_index的链接来。选中第一个load_index的链接，点击Response查看一下html源代码，尝试在网页中搜索一下：十年金砖峰这个文章的标题，惊奇地发现，在网页中找到了对于的文章标题。而前面，我们搜索这个词时，是没有搜索到的。 这说明了什么呢？说明十年金砖峰这篇文章的内容不在第一个list_25635链接中，而在这个load_index的链接里。鼠标点击headers，复制Request URL后面的链接并打开，就可以再次看到包括这篇文章在内的新的20篇文章。 是不是发现了点了什么？接着，我们继续下拉，会发现弹出更多的load_index的链接。再搜索一个标题：地图湃｜海外港口热，可以发现在网页中也同样找到了文章标题。 回到我们的初衷：下载所有网页的图片内容。那么现在就有解决办法礼：一个个地把出现的这些url网址中图片下载下来就大功告成了。 好，我们先来分析一下这些url，看看有没有相似性，如果有很明显的相似性，那么就可以像普通网页那样，通过构造翻页页数的url，实现for循环就可以批量下载所有网页的图片了。复制前3个链接如下：123https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=2&amp;isList=true&amp;lastTime=1533169319712 https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=3&amp;isList=true&amp;lastTime=1528625875167 https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=4&amp;isList=true&amp;lastTime=1525499007926 发现pageidx键的值呈现规律的数字递增变化，看起来是个好消息。但同时发现后面的lastTime键的值看起来是随机变化的，这个有没有影响呢？ 来测试一下，复制第一个链接，删掉&amp;lastTime=1533169319712这一串字符，会发现网页一样能够正常打开，就说明着一对参数不影响网页内容，那就太好了。我们可以删除掉，这样所有url的区别只剩pageidx的值了，这时就可以构造url来实现for循环了。构造的url形式如下：123https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=2https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=3https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=4 同时，尝试把数字2改成1并打开链接看看会有什么变化，发现呈现的内容就是第1页的内容。这样，我们就可以从第一页开始构造url循环了。https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=1 既然确定了首页，那么也要相应地确定一下尾页。很简单，我们把数字改大然后打开链接看是否有内容即可。比如改为10 ，打开发现有内容显示，很好。接着，再改为30，发现没有内容了。说明该栏目的页数介于这两个数之间，尝试几次后，发现25是最后一个有内容的网页，也意味着能够爬取的页数一共是25页。 确定了首页和尾页后，下面我们就可以开始构造链接，先爬取第一篇文章网页里的图片（这个爬取过程，我们上一篇爬取网易”数读”已经尝试过了），然后爬取这一整页的图片，最后循环25页，爬取所有图片，下面开始吧。 3. 程序代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130import requestsfrom bs4 import BeautifulSoupimport reimport osfrom hashlib import md5from requests.exceptions import RequestExceptionfrom multiprocessing import Poolfrom urllib.parse import urlencodeheaders = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125;# 1 获取索引界面网页内容def get_page_index(i): # 下载1页 # url = 'https://www.thepaper.cn/newsDetail_forward_2370041' # 2下载多页，构造url paras = &#123; 'nodeids': 25635, 'pageidx': i &#125; url = 'https://www.thepaper.cn/load_index.jsp?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text # print(response.text) # 测试网页内容是否提取成功ok# 2 解析索引界面网页内容def parse_page_index(html): soup = BeautifulSoup(html,'lxml') # 获取每页文章数 num = soup.find_all(name = 'div',class_='news_li') for i in range(len(num)): yield&#123; # 获取title 'title':soup.select('h2 a')[i].get_text(), # 获取图片url，需加前缀 'url':'https://www.thepaper.cn/' + soup.select('h2 a')[i].attrs['href'] # print(url) # 测试图片链接 &#125;# 3 获取每条文章的详情页内容def get_page_detail(item): url = item.get('url') # 增加异常捕获语句 try: response = requests.get(url,headers = headers) if response.status_code == 200: return response.text # print(response.text) # 测试网页内容是否提取成功 except RequestException: print('网页请求失败') return None # 4 解析每条文章的详情页内容def parse_page_detail(html): soup = BeautifulSoup(html,'lxml') # 获取title if soup.h1: #有的网页没有h1节点，因此必须要增加判断，否则会报错 title = soup.h1.string # 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一 items = soup.find_all(name='img',width =['100%','600']) # 有的图片节点用width='100%'表示，有的用600表示，因此用list合并选择 # https://blog.csdn.net/w_xuechun/article/details/76093950 # print(items) # 测试返回的img节点ok for i in range(len(items)): pic = items[i].attrs['src'] # print(pic) #测试图片链接ok yield&#123; 'title':title, 'pic':pic, 'num':i # 图片添加编号顺序 &#125;# 5 下载图片def save_pic(pic): title = pic.get('title') # 标题规范命名：去掉符号非法字符| 等 title = re.sub('[\/:*?"&lt;&gt;|]','-',title).strip() url = pic.get('pic') # 设置图片编号顺序 num = pic.get('num') if not os.path.exists(title): os.mkdir(title) # 获取图片url网页信息 response = requests.get(url,headers = headers) try: # 建立图片存放地址 if response.status_code == 200: file_path = '&#123;0&#125;\&#123;1&#125;.&#123;2&#125;' .format(title,num,'jpg') # 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest() if not os.path.exists(file_path): # 开始下载图片 with open(file_path,'wb') as f: f.write(response.content) print('文章"&#123;0&#125;"的第&#123;1&#125;张图片下载完成' .format(title,num)) else: print('该图片%s 已下载' %title) except RequestException as e: print(e,'图片获取失败') return Nonedef main(i): # get_page_index(i) # 测试索引界面网页内容是否获取成功ok html = get_page_index(i) data = parse_page_index(html) # 测试索引界面url是否获取成功ok for item in data: # print(item) #测试返回的dict html = get_page_detail(item) data = parse_page_detail(html) for pic in data: save_pic(pic)# 单进程if __name__ == '__main__': for i in range(1,26): main(i)# 多进程if __name__ == '__main__': pool = Pool() pool.map(main,[i for i in range(1,26)]) pool.close() pool.join() 结果： 文章代码和栏目从2015年至今437篇文章共1509张图片资源，可在下方链接中得到。https://github.com/makcyun/web_scraping_with_python 本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Ajax</tag>
        <tag>图片下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫(3)：单页图片下载-网易"数读"信息图]]></title>
    <url>%2Fweb_scraping_withpython3.html</url>
    <content type="text"><![CDATA[下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。 本文知识点： 单张图片下载 单页图片下载 Ajax技术介绍 1. 单张图片下载以一篇最近比较热的涨房价的文章为例：暴涨的房租，正在摧毁中国年轻人的生活，从文章里随意挑选一张北京房租地图图片，通过Requests的content属性来实现单张图片的下载。12345import requestsurl = 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'response = requests.get(url)with open('北京房租地图.jpg', 'wb') as f: f.write(response.content) 5行代码就能将这张图片下载到电脑上。不只是该张图片，任意图片都可以下载，只要替换图片的url即可。这里用到了Requests的content属性，将图片存储为二进制数据。至于，图片为什么可以用二进制数据进行存储，可以参考这个教程：https://www.zhihu.com/question/36269548/answer/66734582 5行代码看起来很短，但如果只是下载一张图片显然没有必要写代码，”右键另存为”更快。现在，我们放大一下范围，去下载这篇文章中的所有图片。粗略数一下，网页里有超过15张图片，这时，如果再用”右键另存为”的方法，显然就比较繁琐了。下面，我们用代码来实现下载该网页中的所有图片。 2. 单页图片下载2.1. Requests获取网页内容首先，用堪称python”爬虫利器”的Requests库来获取该篇文章的html内容。Requests库可以说是一款python爬虫的利器，它的更多用法，可参考下面的教程：http://docs.python-requests.org/zh_CN/latest/index.htmlhttps://cuiqingcai.com/2556.html 1234567891011import requestsheaders = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125;url = 'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'response = requests.get(url,headers = headers)if response.status_code == 200: # return response.text print(response.text) # 测试网页内容是否提取成功ok 2.2. 解析网页内容通过上面方法可以获取到html内容，接下来解析html字符串内容，从中提取出网页内的图片url。解析和提取url的方法有很多种，常见的有5种，分别是：正则表达式、Xpath、BeautifulSoup、CSS、PyQuery。任选一种即可，这里为了再次加强练习，5种方法全部尝试一遍。首先，在网页中定位到图片url所在的位置，如下图所示： 从外到内定位url的位置：&lt;p&gt;节点-&lt;a&gt;节点-&lt;img&gt;节点里的src属性值。 2.2.1. 正则表达式12345678import repattern =re.compile('&lt;p&gt;.*?&lt;img alt="房租".*?src="(.*?)".*?style',re.S) items = re.findall(pattern,html) # print(items) for item in items: yield&#123; 'url':item &#125; 运行结果如下:12345678910111213141516&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/425eca61322a4f99837988bb78a001ac.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/d6cb58a6bb014b8683b232f3c00f0e39.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/88d2e535765a4ed09e03877238647aa5.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/09/01/98d2f9579e9e49aeb76ad6155e8fc4ea.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7410ed4041a94cab8f30e8de53aaaaa1.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/49a0c80a140b4f1aa03724654c5a39af.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3070964278bf4637ba3d92b6bb771cea.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/812b7a51475246a9b57f467940626c5c.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/8bcbc7d180f74397addc74e47eaa1f63.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/e593efca849744489096a77aafd10d3e.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7653feecbfd94758a8a0ff599915d435.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/edbaa24a17dc4cca9430761bfc557ffb.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/f768d440d9f14b8bb58e3c425345b97e.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3430043fd305411782f43d3d8635d632.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/111ba73d11084c68b8db85cdd6d474a7.png'&#125; 2.2.2. Xpath语法12345678from lxml import etree parse = etree.HTML(html) items = parse.xpath('*//p//img[@alt = "房租"]/@src') print(items) for item in items: yield&#123; 'url':item &#125; 结果同上。 2.2.3. CSS选择器1234567soup = BeautifulSoup(html,'lxml') items = soup.select('p &gt; a &gt; img') #&gt;表示下级绝对节点 # print(items) for item in items: yield&#123; 'url':item['src'] &#125; 2.2.4. BeautifulSoup find_all方法123456789soup = BeautifulSoup(html,'lxml')# 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一item = soup.find_all(name='img',width =['100%'])for i in range(len(item)): url = item[i].attrs['src'] yield&#123; 'url':url &#125; # print(pic) #测试图片链接ok 2.2.5. PyQuery123456789from pyquery import PyQuery as pqdata = pq(html)data2 = data('p &gt; a &gt; img')# print(items)for item in data2.items(): #注意这里和BeautifulSoup 的css用法不同 yield&#123; 'url':item.attr('src') # 或者'url':item.attr.src &#125; 以上用了5种方法提取出了该网页的url地址，任选一种即可。这里假设选择了第4种方法，接下来就可以下载图片了。提取出的网址是一个dict字典，通过dict的get方法调用里面的键和值。1234567891011121314151617181920title = pic.get('title')url = pic.get('pic')# 设置图片编号顺序num = pic.get('num')# 建立文件夹if not os.path.exists(title): os.mkdir(title)# 获取图片url网页信息response = requests.get(url,headers = headers)# 建立图片存放地址file_path = '&#123;0&#125;\&#123;1&#125;.&#123;2&#125;' .format(title,num,'jpg')# 文件名采用编号方便按顺序查看# 开始下载图片with open(file_path,'wb') as f: f.write(response.content) print('该图片已下载完成',title) 很快，15张图片就按着文章的顺序下载下来了。 将上述代码整理一下，增加一点异常处理和图片的标题、编号的代码以让爬虫更健壮，完整的代码如下所示： 2.3. 全部代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import requestsfrom bs4 import BeautifulSoupimport reimport osfrom hashlib import md5from requests.exceptions import RequestExceptionfrom multiprocessing import Poolfrom urllib.parse import urlencodeheaders = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125;def get_page(): # 下载1页 url = 'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html' # 增加异常捕获语句 try: response = requests.get(url,headers = headers) if response.status_code == 200: return response.text # print(response.text) # 测试网页内容是否提取成功 except RequestException: print('网页请求失败') return Nonedef parse_page(html): soup = BeautifulSoup(html,'lxml') # 获取title title = soup.h1.string # 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一 item = soup.find_all(name='img',width =['100%']) # print(item) # 测试 for i in range(len(item)): pic = item[i].attrs['src'] yield&#123; 'title':title, 'pic':pic, 'num':i # 图片添加编号顺序 &#125; # print(pic) #测试图片链接okdef save_pic(pic): title = pic.get('title') url = pic.get('pic') # 设置图片编号顺序 num = pic.get('num') if not os.path.exists(title): os.mkdir(title) # 获取图片url网页信息 response = requests.get(url,headers = headers) try: # 建立图片存放地址 if response.status_code == 200: file_path = '&#123;0&#125;\&#123;1&#125;.&#123;2&#125;' .format(title,num,'jpg') # 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest() if not os.path.exists(file_path): # 开始下载图片 with open(file_path,'wb') as f: f.write(response.content) print('该图片已下载完成',title) else: print('该图片%s 已下载' %title) except RequestException as e: print(e,'图片获取失败') return Nonedef main(): # get_page() # 测试网页内容是获取成功ok html = get_page() # parse_page(html) # 测试网页内容是否解析成功ok data = parse_page(html) for pic in data: # print(pic) #测试dict save_pic(pic)# 单进程if __name__ == '__main__': main() 小结上面通过爬虫实现下载一张图片延伸到下载一页图片，相比于手动操作，爬虫的优势逐渐显现。那么，能否实现多页循环批量下载更多的图片呢，当然可以，下一篇文章将进行介绍。 你也可以尝试一下，这里先放上”福利”：网易”数度”栏目从2012年至今350篇文章的全部图片已下载完成。 如果你需要，可以到我的github下载。https://github.com/makcyun/web_scraping_with_python 本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Ajax</tag>
        <tag>图片下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(2)：表格型数据抓取]]></title>
    <url>%2Fweb_scraping_withpython2.html</url>
    <content type="text"><![CDATA[python爬虫第2篇利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。 摘要： 我们平常在浏览网页中会遇到一些表格型的数据信息，除了表格本身体现的内容以外，你可能想透过表格再更进一步地进行汇总、筛选、处理分析等操作从而得到更多有价值的信息，这时可用python爬虫来实现。本文采用pandas库中的read_html方法来快速准确地抓取表格数据。 本文知识点： Table型表格抓取 DataFrame.read_html函数使用 爬虫数据存储到mysql数据库 Navicat数据库的使用 1. table型表格我们在网页上会经常看到这样一些表格，比如：QS2018世界大学排名： 财富世界500强企业排名： IMDB世界电影票房排行榜： 中国上市公司信息： 他们除了都是表格以外，还一个共同点就是当你点击右键-定位时，可以看到他们都是table类型的表格形式。 从中可以看到table类型的表格网页结构大致如下：123456789101112131415161718192021&lt;table class="..." id="..."&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;...&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;...&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; ... &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; 先来简单解释一下上文出现的几种标签含义：123456&lt;table&gt; : 定义表格&lt;thead&gt; : 定义表格的页眉&lt;tbody&gt; : 定义表格的主体&lt;tr&gt; : 定义表格的行&lt;th&gt; : 定义表格的表头&lt;td&gt; : 定义表格单元 这样的表格数据，就可以利用pandas模块里的read_html函数方便快捷地抓取下来。下面我们就来操作一下。 2. 快速抓取下面以中国上市公司信息这个网页中的表格为例，感受一下read_html函数的强大之处。12345678import pandas as pdimport csvfor i in range(1,178): # 爬取全部177页数据 url = 'http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s' % (str(i)) tb = pd.read_html(url)[3] #经观察发现所需表格是网页中第4个表格，故为[3] tb.to_csv(r'1.csv', mode='a', encoding='utf_8_sig', header=1, index=0) print('第'+str(i)+'页抓取完成') 只需不到十行代码，1分钟左右就可以将全部178页共3536家A股上市公司的信息干净整齐地抓取下来。比采用正则表达式、xpath这类常规方法要省心省力地多。如果采取人工一页页地复制粘贴到excel中，就得操作到猴年马月去了。上述代码除了能爬上市公司表格以外，其他几个网页的表格都可以爬，只需做简单的修改即可。因此，可作为一个简单通用的代码模板。但是，为了让代码更健壮更通用一些，接下来，以爬取177页的A股上市公司信息为目标，讲解一下详细的代码实现步骤。 3. 详细代码实现3.1. read_html函数先来了解一下read_html函数的api:1234567891011pandas.read_html(io, match='.+', flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, tupleize_cols=None, thousands=', ', encoding=None, decimal='.', converters=None, na_values=None, keep_default_na=True, displayed_only=True)常用的参数：io:可以是url、html文本、本地文件等；flavor：解析器；header：标题行；skiprows：跳过的行；attrs：属性，比如 attrs = &#123;'id': 'table'&#125;；parse_dates：解析日期注意：返回的结果是**DataFrame**组成的**list**。 参考： 1 http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html2 http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html 3.2. 分析网页url首先，观察一下中商情报网第1页和第2页的网址：12http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=1#QueryConditionhttp://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=2#QueryCondition 可以发现，只有pageNum的值随着翻页而变化，所以基本可以断定pageNum=1代表第1页，pageNum=10代表第10页，以此类推。这样比较容易用for循环构造爬取的网址。试着把#QueryCondition删除，看网页是否同样能够打开，经尝试发现网页依然能正常打开，因此在构造url时，可以使用这样的格式：http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i再注意一下其他参数：a：表示A股，把a替换为h，表示港股；把a替换为xsb，则表示新三板。那么，在网址分页for循环外部再加一个for循环，就可以爬取这三个股市的股票了。 3.3. 定义函数将整个爬取分为网页提取、内容解析、数据存储等步骤，依次建立相应的函数。123456789101112131415161718192021222324252627282930313233343536373839404142# 网页提取函数def get_one_page(i): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; paras = &#123; 'reportTime': '2017-12-31', #可以改报告日期，比如2018-6-30获得的就是该季度的信息 'pageNum': i #页码 &#125; url = 'http://s.askci.com/stock/a/?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None except RequestException: print('爬取失败')# beatutiful soup解析然后提取表格def parse_one_page(html): soup = BeautifulSoup(html,'lxml') content = soup.select('#myTable04')[0] #[0]将返回的list改为bs4类型 tbl = pd.read_html(content.prettify(),header = 0)[0] # prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame tbl.rename(columns = &#123;'序号':'serial_number', '股票代码':'stock_code', '股票简称':'stock_abbre', '公司名称':'company_name', '省份':'province', '城市':'city', '主营业务收入(201712)':'main_bussiness_income', '净利润(201712)':'net_profit', '员工人数':'employees', '上市日期':'listing_date', '招股书':'zhaogushu', '公司财报':'financial_report', '行业分类':'industry_classification', '产品类型':'industry_type', '主营业务':'main_business'&#125;,inplace = True) print(tbl) # return tbl # rename将表格15列的中文名改为英文名，便于存储到mysql及后期进行数据分析 # tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本# 主函数def main(page): for i in range(1,page): # page表示提取页数 html = get_one_page(i) parse_one_page(html)# 单进程if __name__ == '__main__': main(178) #共提取n页 上面两个函数相比于快速抓取的方法代码要多一些，如果需要抓的表格很少或只需要抓一次，那么推荐快速抓取法。如果页数比较多，这种方法就更保险一些。解析函数用了BeautifulSoup和css选择器，这种方法定位提取表格所在的id为#myTable04的table代码段，更为准确。 3.4. 存储到MySQL接下来，我们可以将结果保存到本地csv文件，也可以保存到MySQL数据库中。这里为了练习一下MySQL，因此选择保存到MySQL中。 首先，需要先在数据库建立存放数据的表格，这里命名为listed_company。代码如下：12345678910111213141516171819import pymysqldef generate_mysql(): conn = pymysql.connect( host='localhost', # 本地服务器 user='root', password='******', # 你的数据库密码 port=3306, # 默认端口 charset = 'utf8', db = 'wade') cursor = conn.cursor() sql = 'CREATE TABLE IF NOT EXISTS listed_company2 (serial_number INT(30) NOT NULL,stock_code INT(30) ,stock_abbre VARCHAR(30) ,company_name VARCHAR(30) ,province VARCHAR(30) ,city VARCHAR(30) ,main_bussiness_income VARCHAR(30) ,net_profit VARCHAR(30) ,employees INT(30) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(30) ,financial_report VARCHAR(30) , industry_classification VARCHAR(255) ,industry_type VARCHAR(255) ,main_business VARCHAR(255) ,PRIMARY KEY (serial_number))' # listed_company是要在wade数据库中建立的表，用于存放数据 cursor.execute(sql) conn.close() generate_mysql() 上述代码定义了generate_mysql()函数，用于在MySQL中wade数据库下生成一个listed_company的表。表格包含15个列字段。根据每列字段的属性，分别设置为INT整形（长度为30）、VARCHAR字符型(长度为30) 、DATETIME(0) 日期型等。在Navicat中查看建立好之后的表格： 接下来就可以往这个表中写入数据，代码如下：1234567891011import pymysqlfrom sqlalchemy import create_enginedef write_to_sql(tbl, db = 'wade'): engine = create_engine('mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'.format(db)) # db = 'wade'表示存储到wade这个数据库中,root后面的*是密码 try: tbl.to_sql('listed_company',con = engine,if_exists='append',index=False) # 因为要循环网页不断数据库写入内容，所以if_exists选择append，同时该表要有表头，parse_one_page（）方法中df.rename已设置 except Exception as e: print(e) 以上就完成了单个页面的表格爬取和存储工作，接下来只要在main()函数进行for循环，就可以完成所有总共178页表格的爬取和存储，完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import requestsimport pandas as pdfrom bs4 import BeautifulSoupfrom lxml import etreeimport timeimport pymysqlfrom sqlalchemy import create_enginefrom urllib.parse import urlencode # 编码 URL 字符串start_time = time.time() #计算程序运行时间def get_one_page(i): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; paras = &#123; 'reportTime': '2017-12-31', #可以改报告日期，比如2018-6-30获得的就是该季度的信息 'pageNum': i #页码 &#125; url = 'http://s.askci.com/stock/a/?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None except RequestException: print('爬取失败')def parse_one_page(html): soup = BeautifulSoup(html,'lxml') content = soup.select('#myTable04')[0] #[0]将返回的list改为bs4类型 tbl = pd.read_html(content.prettify(),header = 0)[0] # prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame tbl.rename(columns = &#123;'序号':'serial_number', '股票代码':'stock_code', '股票简称':'stock_abbre', '公司名称':'company_name', '省份':'province', '城市':'city', '主营业务收入(201712)':'main_bussiness_income', '净利润(201712)':'net_profit', '员工人数':'employees', '上市日期':'listing_date', '招股书':'zhaogushu', '公司财报':'financial_report', '行业分类':'industry_classification', '产品类型':'industry_type', '主营业务':'main_business'&#125;,inplace = True) # print(tbl) return tbl # rename将中文名改为英文名，便于存储到mysql及后期进行数据分析 # tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本def generate_mysql(): conn = pymysql.connect( host='localhost', user='root', password='******', port=3306, charset = 'utf8', db = 'wade') cursor = conn.cursor() sql = 'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))' # listed_company是要在wade数据库中建立的表，用于存放数据 cursor.execute(sql) conn.close() def write_to_sql(tbl, db = 'wade'): engine = create_engine('mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'.format(db)) try: # df = pd.read_csv(df) tbl.to_sql('listed_company2',con = engine,if_exists='append',index=False) # append表示在原有表基础上增加，但该表要有表头 except Exception as e: print(e)def main(page): generate_mysql() for i in range(1,page): html = get_one_page(i) tbl = parse_one_page(html) write_to_sql(tbl) # # 单进程if __name__ == '__main__': main(178) endtime = time.time()-start_time print('程序运行了%.2f秒' %endtime) # 多进程# from multiprocessing import Pool# if __name__ == '__main__':# pool = Pool(4)# pool.map(main, [i for i in range(1,178)]) #共有178页# endtime = time.time()-start_time# print('程序运行了%.2f秒' %(time.time()-start_time)) 最终，A股所有3535家企业的信息已经爬取到mysql中，如下图： 最后，需说明不是所有表格都可以用这种方法爬取，比如这个网站中的表格，表面是看起来是表格，但在html中不是前面的table格式，而是list列表格式。这种表格则不适用read_html爬取。得用其他的方法，比如selenium，以后再进行介绍。 本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>pandas</tag>
        <tag>数据抓取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(1):多种方法爬取猫眼top100电影]]></title>
    <url>%2Fweb_scraping_withpython1.html</url>
    <content type="text"><![CDATA[python爬虫第1篇利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。 摘要： 作为小白，爬虫可以说是入门python最快和最容易获得成就感的途径。因为初级爬虫的套路相对固定，常见的方法只有几种，比较好上手。最近，跟着崔庆才大佬的书：python3网络爬虫开发实战 学习爬虫。选取网页结构较为简单的猫眼top100电影为案例进行练习。 重点是用上述所说的4种方法提取出关键内容。一个问题采用不同的解决方法有助于拓展思维，通过不断练习就能够灵活运用。 本文知识点：Requsts 请求库的使用beautiful+lxml两大解析库使用正则表达式 、xpath、css选择器的使用 1. 为什么爬取该网页？ 比较懒，不想一页页地去翻100部电影的介绍，想在一个页面内进行总体浏览（比如在excel表格中）； 想深入了解一些比较有意思的信息，比如：哪部电影的评分最高？哪位演员的作品数量最多？哪个国家/地区上榜的电影数量最多？哪一年上榜的电影作品最多等。这些信息在网页上是不那么容易能直接获得的，所以需要爬虫。 2. 爬虫目标 从网页中提取出top100电影的电影名称、封面图片、排名、评分、演员、上映国家/地区、评分等信息，并保存为csv文本文件。 根据爬取结果，进行简单的可视化分析。 平台：windows7 + SublimeText3 3. 爬取步骤3.1. 网址URL分析首先，打开猫眼Top100的url网址： http://maoyan.com/board/4?offset=0。页面非常简单，所包含的信息就是上述所说的爬虫目标。下拉页面到底部，点击第2页可以看到网址变为：http://maoyan.com/board/4?offset=10。因此，可以推断出url的变化规律：offset表示偏移，10代表一个页面的电影偏移数量，即：第一页电影是从0-10，第二页电影是从11-20。因此，获取全部100部电影，只需要构造出10个url，然后依次获取网页内容，再用不同的方法提取出所需内容就可以了。下面，用requests方法获取第一个页面。 3.2. Requests获取首页数据先定义一个获取单个页面的函数：get_one_page()，传入url参数。 12345678910111213def get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; # 不加headers爬不了 response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except RequestException: return None # try-except语句捕获异常 接下来在main()函数中设置url。 12345678def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) print(html)if __name__ == '__main__': main() 运行上述程序后，首页的源代码就被爬取下来了。如下图所示： 接下来就需要从整个网页中提取出几项我们需要的内容，用到的方法就是上述所说的四种方法，下面分别进行说明。 3.3. 4种内容解析提取方法3.3.1. 正则表达式提取第一种是利用正则表达式提取。什么是正则表达式？ 下面这串看起来乱七八糟的符号就是正则表达式的语法。 1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&apos; 它是一种强大的字符串处理工具。之所以叫正则表达式，是因为它们可以识别正则字符串（regular string）。可以这么定义：“ 如果你给我的字符串符合规则，我就返回它”；“如果字符串不符合规则，我就忽略它”。通过requests抓取下来的网页是一堆大量的字符串，用它处理后便可提取出我们想要的内容。 如果还不了解它，可以参考下面的教程： http://www.runoob.com/regexp/regexp-syntax.htmlhttps://www.w3cschool.cn/regexp/zoxa1pq7.html 正则表达式常用语法： table th:nth-of-type(1) { width: 60px; } 模式 描述 \w 匹配字母数字及下划线 \W 匹配非字母数字及下划线 \s 匹配任意空白字符，等价于 [\t\n\r\f] \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9] \D 匹配任意非数字 \n 匹配一个换行符 \t 匹配一个制表符 ^ 匹配字符串开始位置的字符 $ 匹配字符串的末尾 . 匹配任意字符，除了换行符 […] 用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’ [^…] 不在 [ ] 中的字符 * 匹配前面的字符、子表达式或括号里的字符 0 次或多次 + 同上，匹配至少一次 ? 同上，匹配0到1次 {n} 匹配前面的字符、子表达式或括号里的字符 n 次 {n, m} 同上，匹配 m 到n 次（包含 m 或 n） ( ) 匹配括号内的表达式，也表示一个组 下面，开始提取关键内容。右键网页-检查-Network选项，选中左边第一个文件然后定位到电影信息的相应位置，如下图： 可以看到每部电影的相关信息都在dd这个节点之中。所以就可以从该节点运用正则进行提取。第1个要提取的内容是电影的排名。它位于class=”board-index”的i节点内。不需要提取的内容用’.*?’替代，需要提取的数字排名用（）括起来，（）里面的数字表示为（\d+）。正则表达式可以写为： 1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;&apos; 接着，第2个需要提取的是封面图片，图片网址位于img节点的’data-src’属性中，正则表达式可写为： 1&apos;data-src=&quot;(.*?)&quot;.*?&apos; 第1和第2个正则之间的代码是不需要的，用’.*?’替代，所以这两部分合起来写就是： 1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot; 同理，可以依次用正则写下主演、上映时间和评分等内容,完整的正则表达式如下： 1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos; 正则表达式写好以后，可以定义一个页面解析提取方法：parse_one_page（），用来提取内容： 12345678910111213141516171819def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) # re.S表示匹配任意字符，如果不加，则无法匹配换行符 items = re.findall(pattern, html) # print(items) for item in items: yield &#123; 'index': item[0], 'thumb': get_thumb(item[1]), # 定义get_thumb()方法进一步处理网址 'name': item[2], 'star': item[3].strip()[3:], # 'time': item[4].strip()[5:], # 用两个方法分别提取time里的日期和地区 'time': get_release_time(item[4].strip()[5:]), 'area': get_release_area(item[4].strip()[5:]), 'score': item[5].strip() + item[6].strip() # 评分score由整数+小数两部分组成 &#125; Tips:re.S:匹配任意字符，如果不加，则无法匹配换行符；yield:使用yield的好处是作为生成器，可以遍历迭代，并且将数据整理形成字典，输出结果美观。具体用法可参考：https://blog.csdn.net/zhangpinghao/article/details/18716275；.strip():用于去掉字符串中的空格。 上面程序为了便于提取内容，又定义了3个方法：get_thumb（）、get_release_time（）和 get_release_area（）： 1234567891011121314151617181920212223242526# 获取封面大图def get_thumb(url): pattern = re.compile(r'(.*?)@.*?') thumb = re.search(pattern, url) return thumb.group(1)# http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c# 去掉@160w_220h_1e_1c就是大图 # 提取上映时间函数def get_release_time(data): pattern = re.compile(r'(.*?)(\(|$)') items = re.search(pattern, data) if items is None: return '未知' return items.group(1) # 返回匹配到的第一个括号(.*?)中结果即时间# 提取国家/地区函数def get_release_area(data): pattern = re.compile(r'.*\((.*)\)') # $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?) items = re.search(pattern, data) if items is None: return '未知' return items.group(1) Tips:‘r’：正则前面加上’r’ 是为了告诉编译器这个string是个raw string，不要转意’\’。当一个字符串使用了正则表达式后，最好在前面加上’r’；‘|’ ‘$’： 正则’|’表示或’，’$’表示匹配一行字符串的结尾；.group(1)：意思是返回search匹配的第一个括号中的结果，即(.*?)，gropup()则返回所有结果2013-12-18(，group(1)返回’（’。 接下来，修改main()函数来输出爬取的内容： 12345678910def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) for item in parse_one_page(html): print(item)if __name__ == '__main__': main() Tips:if name == ‘_ _main__’:当.py文件被直接运行时，if name == ‘_ main__’之下的代码块将被运行；当.py文件以模块形式被导入时，if name == ‘ _main__’之下的代码块不被运行。参考：https://blog.csdn.net/yjk13703623757/article/details/77918633。 运行程序，就可成功地提取出所需内容，结果如下： 123456789&#123;'index': '1', 'thumb': 'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg', 'name': '霸王别姬', 'star': '张国荣,张丰毅,巩俐', 'time': '1993-01-01', 'area': '中国香港', 'score': '9.6'&#125;&#123;'index': '2', 'thumb': 'http://p0.meituan.net/movie/54617769d96807e4d81804284ffe2a27239007.jpg', 'name': '罗马假日', 'star': '格利高里·派克,奥黛丽·赫本,埃迪·艾伯特', 'time': '1953-09-02', 'area': '美国', 'score': '9.1'&#125;&#123;'index': '3', 'thumb': 'http://p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg', 'name': '肖申克的救赎', 'star': '蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿', 'time': '1994-10-14', 'area': '美国', 'score': '9.5'&#125;&#123;'index': '4', 'thumb': 'http://p0.meituan.net/movie/e55ec5d18ccc83ba7db68caae54f165f95924.jpg', 'name': '这个杀手不太冷', 'star': '让·雷诺,加里·奥德曼,娜塔莉·波特曼', 'time': '1994-09-14', 'area': '法国', 'score': '9.5'&#125;&#123;'index': '5', 'thumb': 'http://p1.meituan.net/movie/f5a924f362f050881f2b8f82e852747c118515.jpg', 'name': '教父', 'star': '马龙·白兰度,阿尔·帕西诺,詹姆斯·肯恩', 'time': '1972-03-24', 'area': '美国', 'score': '9.3'&#125;...&#125;[Finished in 1.9s] 以上是第1种提取方法，如果还不习惯正则表达式这种复杂的语法，可以试试下面的第2种方法。 3.3.2. lxml结合xpath提取该方法需要用到lxml这款解析利器，同时搭配xpath语法，利用它的的路径选择表达式，来高效提取所需内容。lxml包为第三方包，需要自行安装。如果对xpath的语法还不太熟悉，可参考下面的教程：http://www.w3school.com.cn/xpath/xpath_syntax.asp xpath常用的规则 表达式 描述 nodename 选取此节点的所有子节点 / 从当前节点选取直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 12345678910111213141516171819202122232425262728293031323334&lt;/div&gt; &lt;div class="container" id="app" class="page-board/index" &gt;&lt;div class="content"&gt; &lt;div class="wrapper"&gt; &lt;div class="main"&gt; &lt;p class="update-time"&gt;2018-08-18&lt;span class="has-fresh-text"&gt;已更新&lt;/span&gt;&lt;/p&gt; &lt;p class="board-content"&gt;榜单规则：将猫眼电影库中的经典影片，按照评分和评分人数从高到低综合排序取前100名，每天上午10点更新。相关数据来源于“猫眼电影库”。&lt;/p&gt; &lt;dl class="board-wrapper"&gt; &lt;dd&gt; &lt;i class="board-index board-index-1"&gt;1&lt;/i&gt; &lt;a href="/films/1203" title="霸王别姬" class="image-link" data-act="boarditem-click" data-val="&#123;movieId:1203&#125;"&gt; &lt;img src="//ms0.meituan.net/mywww/image/loading_2.e3d934bf.png" alt="" class="poster-default" /&gt; &lt;img data-src="http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c" alt="霸王别姬" class="board-img" /&gt; &lt;/a&gt; &lt;div class="board-item-main"&gt; &lt;div class="board-item-content"&gt; &lt;div class="movie-item-info"&gt; &lt;p class="name"&gt;&lt;a href="/films/1203" title="霸王别姬" data-act="boarditem-click" data-val="&#123;movieId:1203&#125;"&gt;霸王别姬&lt;/a&gt;&lt;/p&gt; &lt;p class="star"&gt; 主演：张国荣,张丰毅,巩俐 &lt;/p&gt;&lt;p class="releasetime"&gt;上映时间：1993-01-01(中国香港)&lt;/p&gt; &lt;/div&gt; &lt;div class="movie-item-number score-num"&gt;&lt;p class="score"&gt;&lt;i class="integer"&gt;9.&lt;/i&gt;&lt;i class="fraction"&gt;6&lt;/i&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/dd&gt; &lt;dd&gt; 根据截取的部分html网页，先来提取第1个电影排名信息，有两种方法。第一种：直接复制。右键-Copy-Copy Xpath，得到xpath路径为：//*[@id=”app”]/div/div/div[1]/dl/dd[1]/i,为了能够提取到页面所有的排名信息，需进一步修改为：//*[@id=”app”]/div/div/div[1]/dl/dd/i/text()，如果想要再精简一点，可以省去中间部分绝对路径’/‘然后用相对路径’//‘代替，最后进一步修改为：//*[@id=”app”]//div//dd/i/text()。 第二种：观察网页结构自己写。首先注意到id = app的div节点，因为在整个网页结构id是唯一的不会有第二个相同的，所有可以将该div节点作为xpath语法的起点，然后往下观察分别是3级div节点，可以省略写为：//div,再往下分别是是两个并列的p节点、dl节点、dd节点和最后的i节点文本。中间可以随意省略，只要保证该路径能够选择到唯一的文本值‘1’即可，例如省去p和dl节点，只保留后面的节点。这样，完整路径可以为：//*[@id=”app”]//div//dd/i/text()，和上式一样。 根据上述思路，可以写下其他内容的xpath路径。观察到路径的前一部分：//*[@id=”app”]//div//dd都是一样的，从后面才开始不同，因此为了能够精简代码，将前部分路径赋值为一个变量items，最终提取的代码如下： 12345678910111213141516171819202122232425# 2 用lxml结合xpath提取内容def parse_one_page2(html): parse = etree.HTML(html) items = parse.xpath('//*[@id="app"]//div//dd') # 完整的是//*[@id="app"]/div/div/div[1]/dl/dd # print(type(items)) # *代表匹配所有节点，@表示属性 # 第一个电影是dd[1],要提取页面所有电影则去掉[1] # xpath://*[@id="app"]/div/div/div[1]/dl/dd[1] for item in items: yield&#123; 'index': item.xpath('./i/text()')[0], #./i/text()前面的点表示从items节点开始 #/text()提取文本 'thumb': get_thumb(str(item.xpath('./a/img[2]/@data-src')[0].strip())), # 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。 'name': item.xpath('./a/@title')[0], 'star': item.xpath('.//p[@class = "star"]/text()')[0].strip(), 'time': get_release_time(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'area': get_release_area(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'score' : item.xpath('.//p[@class = "score"]/i[1]/text()')[0] + \ item.xpath('.//p[@class = "score"]/i[2]/text()')[0] &#125; Tips:[0]：xpath后面添加了[0]是因为返回的是只有1个字符串的list，添加[0]是将list提取为字符串，使其简洁；Network：要在最原始的Network选项卡中定位，而不是Elements中，不然提取不到相关内容；class属性：p[@class = “star”]/text()表示提取class属性为”star”的p节点的文本值；提取属性值：img[2]/@data-src’：提取img节点的data-src属性值，属性值后面无需添加’/text()’ 运行程序，就可成功地提取出所需内容，结果和第一种方法一样。 以上是第2种提取方法，如果也不太习惯xpath语法，可以试试下面的第3种方法。 3.3.3. Beautiful Soup + css选择器Beautiful Soup 同lxml一样，是一个非常强大的python解析库，可以从HTML或XML文件中提取效率非常高。关于它的用法，可参考下面的教程：https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ css选择器选是一种模式，用于选择需要添加样式的元素，使用它的语法同样能够快速定位到所需节点，然后提取相应内容。使用方法可参考下面的教程：http://www.w3school.com.cn/cssref/css_selectors.asp css选择器常用的规则 table th:nth-of-type(1) { width: 30%; } 选择器 例子 例子描述 .class .intro 选择 class=”intro” 的所有元素。 #id #firstname 选择 id=”firstname” 的所有元素。 * * 选择所有元素。 element p 选择所有p元素。 element,element div,p 选择所有div元素和所有p元素。 element?element div p 选择div元素内部的所有p元素。 element&gt;element div&gt;p 选择父元素为div元素的所有p元素。 element+element div+p 选择紧接在div元素之后的所有p元素。 [attribute] [target] 选择带有 target 属性所有元素。 [attribute=value] [target=_blank] 选择 target=”_blank” 的所有元素。 下面就利用这种方法进行提取：12345678910111213141516171819202122# 3 用beautifulsoup + css选择器提取def parse_one_page3(html): soup = BeautifulSoup(html, 'lxml') # print(content) # print(type(content)) # print('------------') items = range(10) for item in items: yield&#123; 'index': soup.select('dd i.board-index')[item].string, # iclass节点完整地为'board-index board-index-1',写board-index即可 'thumb': get_thumb(soup.select('a &gt; img.board-img')[item]["data-src"]), # 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值 'name': soup.select('.name a')[item].string, 'star': soup.select('.star')[item].string.strip()[3:], 'time': get_release_time(soup.select('.releasetime')[item].string.strip()[5:]), 'area': get_release_area(soup.select('.releasetime')[item].string.strip()[5:]), 'score': soup.select('.integer')[item].string + soup.select('.fraction')[item].string &#125; 运行上述程序，结果同第1种方法一样。 3.3.4. Beautiful Soup + find_all函数提取Beautifulsoup除了和css选择器搭配，还可以直接用它自带的find_all函数进行提取。find_all，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。它的API如下： 1find_all(name , attrs , recursive , text , **kwargs) 常用的语法规则如下：soup.find_all(name=’ul’)： 查找所有ul节点，ul节点内还可以嵌套；li.string和li.get_text()：都是获取li节点的文本，但推荐使用后者；soup.find_all(attrs={‘id’: ‘list-1’}))：传入 attrs 参数，参数的类型是字典类型，表示查询 id 为 list-1 的节点；常用的属性比如 id、class 等，可以省略attrs采用更简洁的形式，例如：soup.find_all(id=’list-1’)soup.find_all(class_=’element’) 根据上述常用语法，可以提取网页中所需内容： 12345678910111213141516def parse_one_page4(html): soup = BeautifulSoup(html,'lxml') items = range(10) for item in items: yield&#123; 'index': soup.find_all(class_='board-index')[item].string, 'thumb': soup.find_all(class_ = 'board-img')[item].attrs['data-src'], # 用.get('data-src')获取图片src链接，或者用attrs['data-src'] 'name': soup.find_all(name = 'p',attrs = &#123;'class' : 'name'&#125;)[item].string, 'star': soup.find_all(name = 'p',attrs = &#123;'class':'star'&#125;)[item].string.strip()[3:], 'time': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'area': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'score':soup.find_all(name = 'i',attrs = &#123;'class':'integer'&#125;)[item].string.strip() + soup.find_all(name = 'i',attrs = &#123;'class':'fraction'&#125;)[item].string.strip() &#125; 以上就是4种不同的内容提取方法。 3.4. 数据存储上述输出的结果为字典格式，可利用csv包的DictWriter函数将字典格式数据存储到csv文件中。 123456789# 数据存储到csvdef write_to_file3(item): with open('猫眼top100.csv', 'a', encoding='utf_8_sig',newline='') as f: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] w = csv.DictWriter(f,fieldnames = fieldnames) # w.writeheader() w.writerow(item) 然后修改一下main()方法：1234567891011def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) for item in parse_one_page(html): # print(item) write_to_csv(item)if __name__ == '__main__': main() 结果如下图： 再将封面的图片下载下来： 1234567891011def download_thumb(name, url,num): try: response = requests.get(url) with open('封面图/' + name + '.jpg', 'wb') as f: f.write(response.content) print('第%s部电影封面下载完毕' %num) print('------') except RequestException as e: print(e) pass # 不能是w，否则会报错，因为图片是二进制数据所以要用wb 3.5. 分页爬取上面完成了一页电影数据的提取，接下来还需提取剩下9页共90部电影的数据。对网址进行遍历，给网址传入一个offset参数即可，修改如下： 123456789101112def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): # print(item) write_to_csv(item)if __name__ == '__main__': for i in range(10): main(offset = i*10) 这样就完成了所有电影的爬取。结果如下： 4. 可视化分析俗话说“文不如表，表不如图”。下面根据excel的数据结果，进行简单的数据可视化分析，并用图表呈现。 4.1. 电影评分最高top10首先，想看一看评分最高的前10部电影是哪些？ 程序如下：123456789101112131415161718192021222324252627282930import pandas as pdimport matplotlib.pyplot as pltimport pylab as pl #用于修改x轴坐标plt.style.use('ggplot') #默认绘图风格很难看，替换为好看的ggplot风格fig = plt.figure(figsize=(8,5)) #设置图片大小colors1 = '#6D6D6D' #设置图表title、text标注的颜色columns = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] #设置表头df = pd.read_csv('maoyan_top100.csv',encoding = "utf-8",header = None,names =columns,index_col = 'index') #打开表格# index_col = 'index' 将索引设为indexdf_score = df.sort_values('score',ascending = False) #按得分降序排列name1 = df_score.name[:10] #x轴坐标score1 = df_score.score[:10] #y轴坐标 plt.bar(range(10),score1,tick_label = name1) #绘制条形图，用range()能搞保持x轴正确顺序plt.ylim ((9,9.8)) #设置纵坐标轴范围plt.title('电影评分最高top10',color = colors1) #标题plt.xlabel('电影名称') #x轴标题plt.ylabel('评分') #y轴标题# 为每个条形图添加数值标签for x,y in enumerate(list(score1)): plt.text(x,y+0.01,'%s' %round(y,1),ha = 'center',color = colors1)pl.xticks(rotation=270) #x轴名称太长发生重叠，旋转为纵向显示plt.tight_layout() #自动控制空白边缘，以全部显示x轴名称# plt.savefig('电影评分最高top10.png') #保存图片plt.show() 结果如下图：可以看到：排名最高的分别是两部国产片”霸王别姬”和”大话西游”，其他还包括”肖申克的救赎”、”教父”等。嗯，还好基本上都看过。 4.2. 各国家的电影数量比较然后，想看看100部电影都是来自哪些国家？程序如下：1234567891011121314151617area_count = df.groupby(by = 'area').area.count().sort_values(ascending = False)# 绘图方法1area_count.plot.bar(color = '#4652B1') #设置为蓝紫色pl.xticks(rotation=0) #x轴名称太长重叠，旋转为纵向# 绘图方法2# plt.bar(range(11),area_count.values,tick_label = area_count.index)for x,y in enumerate(list(area_count.values)): plt.text(x,y+0.5,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('各国/地区电影数量排名',color = colors1)plt.xlabel('国家/地区')plt.ylabel('数量(部)')plt.show()# plt.savefig('各国(地区)电影数量排名.png') 结果如下图：可以看到，除去网站自身没有显示国家的电影以外，上榜电影被10个国家/地区”承包”了。其中，美国以30部电影的绝对优势占据第1名，其次是8部的日本，韩国第3，居然有7部上榜。不得不说的是香港有5部，而内地一部都没有。。。 4.3. 电影作品数量集中的年份接下来站在漫长的百年电影史的时间角度上，分析一下哪些年份”贡献了”最多的电影数量，也可以说是”电影大年”。1234567891011121314151617181920212223# 从日期中提取年份df['year'] = df['time'].map(lambda x:x.split('/')[0])# print(df.info())# print(df.head())# 统计各年上映的电影数量grouped_year = df.groupby('year')grouped_year_amount = grouped_year.year.count()top_year = grouped_year_amount.sort_values(ascending = False)# 绘图top_year.plot(kind = 'bar',color = 'orangered') #颜色设置为橙红色for x,y in enumerate(list(top_year.values)): plt.text(x,y+0.1,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('电影数量年份排名',color = colors1)plt.xlabel('年份(年)')plt.ylabel('数量(部)')plt.tight_layout()# plt.savefig('电影数量年份排名.png')plt.show() 结果如下图： 可以看到，100部电影来自37个年份。其中2011年上榜电影数量最多，达到9部；其次是前一年的7部。回忆一下，那会儿正是上大学的头两年，可怎么感觉除了阿凡达之外，没有什么其他有印象的电影了。。。另外，网上传的号称”电影史奇迹年”的1994年仅排名第6。这让我进一步对猫眼榜单的权威性产生了质疑。再往后看，发现遥远的1939和1940年也有电影上榜。那会儿应该还是黑白电影时代吧，看来电影的口碑好坏跟外在的技术没有绝对的关系，质量才是王道。 4.4 拥有电影作品数量最多的演员最后，看看前100部电影中哪些演员的作品数量最多。程序如下：1234567891011121314151617181920212223242526272829303132333435363738#表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中starlist = []star_total = df.starfor i in df.star.str.replace(' ','').str.split(','): starlist.extend(i) # print(starlist)# print(len(starlist))# set去除重复的演员名starall = set(starlist)# print(starall)# print(len(starall))starall2 = &#123;&#125;for i in starall: if starlist.count(i)&gt;1: # 筛选出电影数量超过1部的演员 starall2[i] = starlist.count(i)starall2 = sorted(starall2.items(),key = lambda starlist:starlist[1] ,reverse = True)starall2 = dict(starall2[:10]) #将元组转为字典格式# 绘图x_star = list(starall2.keys()) #x轴坐标y_star = list(starall2.values()) #y轴坐标plt.bar(range(10),y_star,tick_label = x_star)pl.xticks(rotation = 270)for x,y in enumerate(y_star): plt.text(x,y+0.1,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('演员电影作品数量排名',color = colors1)plt.xlabel('演员')plt.ylabel('数量(部)')plt.tight_layout()plt.show() # plt.savefig('演员电影作品数量排名.png') 结果如下图： 张国荣排在了第一位，这是之前没有猜到的。其次是梁朝伟和星爷，再之后是布拉德·皮特。惊奇地发现，前十名影星中，香港影星居然占了6位。有点严重怀疑这是不是香港版的top100电影。。。 对张国荣以7部影片的巨大优势雄霸榜单第一位感到好奇，想看看是哪7部电影。12345df['star1'] = df['star'].map(lambda x:x.split(',')[0]) #提取1号演员df['star2'] = df['star'].map(lambda x:x.split(',')[1]) #提取2号演员star_most = df[(df.star1 == '张国荣') | (df.star2 == '张国荣')][['star','name']].reset_index('index')# |表示两个条件或查询，之后重置索引print(star_most) 可以看到包括排名第1的”霸王别姬”、第17名的”春光乍泄”、第27名的”射雕英雄传之东成西就”等。突然发现，好像只看过”英雄本色”。。。有时间，去看看他其他的作品。 12345678 index star name0 1 张国荣,张丰毅,巩俐 霸王别姬1 17 张国荣,梁朝伟,张震 春光乍泄2 27 张国荣,梁朝伟,张学友 射雕英雄传之东成西就3 37 张国荣,梁朝伟,刘嘉玲 东邪西毒4 70 张国荣,王祖贤,午马 倩女幽魂5 99 张国荣,张曼玉,刘德华 阿飞正传6 100 狄龙,张国荣,周润发 英雄本色 由于数据量有限，故仅作了上述简要的分析。 5. 完整程序最后，将前面爬虫的所有代码整理一下，完整的代码如下：一、爬虫部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193import urllibimport requestsfrom requests.exceptions import RequestExceptionimport refrom bs4 import BeautifulSoupimport jsonimport timefrom lxml import etree# -----------------------------------------------------------------------------def get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; # 不加headers爬不了 response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except RequestException: return None# 1 用正则提取内容def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) # re.S表示匹配任意字符，如果不加.无法匹配换行符 items = re.findall(pattern, html) # print(items) for item in items: yield &#123; 'index': item[0], 'thumb': get_thumb(item[1]), 'name': item[2], 'star': item[3].strip()[3:], # 'time': item[4].strip()[5:], # 用函数分别提取time里的日期和地区 'time': get_release_time(item[4].strip()[5:]), 'area': get_release_area(item[4].strip()[5:]), 'score': item[5].strip() + item[6].strip() &#125; # 2 用lxml结合xpath提取内容def parse_one_page2(html): parse = etree.HTML(html) items = parse.xpath('//*[@id="app"]//div//dd') # 完整的是//*[@id="app"]/div/div/div[1]/dl/dd # print(type(items)) # *代表匹配所有节点，@表示属性 # 第一个电影是dd[1],要提取页面所有电影则去掉[1] # xpath://*[@id="app"]/div/div/div[1]/dl/dd[1] # lst = [] for item in items: yield&#123; 'index': item.xpath('./i/text()')[0], #./i/text()前面的点表示从items节点开始 #/text()提取文本 'thumb': get_thumb(str(item.xpath('./a/img[2]/@data-src')[0].strip())), # 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。 'name': item.xpath('./a/@title')[0], 'star': item.xpath('.//p[@class = "star"]/text()')[0].strip(), 'time': get_release_time(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'area': get_release_area(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'score' : item.xpath('.//p[@class = "score"]/i[1]/text()')[0] + \ item.xpath('.//p[@class = "score"]/i[2]/text()')[0] &#125; # 3 用beautifulsoup + css选择器提取def parse_one_page3(html): soup = BeautifulSoup(html, 'lxml') # print(content) # print(type(content)) # print('------------') items = range(10) for item in items: yield&#123; 'index': soup.select('dd i.board-index')[item].string, # iclass节点完整地为'board-index board-index-1',写board-inde即可 'thumb': get_thumb(soup.select('a &gt; img.board-img')[item]["data-src"]), # 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值 'name': soup.select('.name a')[item].string, 'star': soup.select('.star')[item].string.strip()[3:], 'time': get_release_time(soup.select('.releasetime')[item].string.strip()[5:]), 'area': get_release_area(soup.select('.releasetime')[item].string.strip()[5:]), 'score': soup.select('.integer')[item].string + soup.select('.fraction')[item].string &#125;# 4 用beautifulsoup + find_all提取def parse_one_page4(html): soup = BeautifulSoup(html,'lxml') items = range(10) for item in items: yield&#123; 'index': soup.find_all(class_='board-index')[item].string, 'thumb': soup.find_all(class_ = 'board-img')[item].attrs['data-src'], # 用.get('data-src')获取图片src链接，或者用attrs['data-src'] 'name': soup.find_all(name = 'p',attrs = &#123;'class' : 'name'&#125;)[item].string, 'star': soup.find_all(name = 'p',attrs = &#123;'class':'star'&#125;)[item].string.strip()[3:], 'time': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'area': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'score':soup.find_all(name = 'i',attrs = &#123;'class':'integer'&#125;)[item].string.strip() + soup.find_all(name = 'i',attrs = &#123;'class':'fraction'&#125;)[item].string.strip() &#125;# -----------------------------------------------------------------------------# 提取时间函数def get_release_time(data): pattern = re.compile(r'(.*?)(\(|$)') items = re.search(pattern, data) if items is None: return '未知' return items.group(1) # 返回匹配到的第一个括号(.*?)中结果即时间# 提取国家/地区函数def get_release_area(data): pattern = re.compile(r'.*\((.*)\)') # $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?) items = re.search(pattern, data) if items is None: return '未知' return items.group(1)# 获取封面大图# http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c# 去掉@160w_220h_1e_1c就是大图def get_thumb(url): pattern = re.compile(r'(.*?)@.*?') thumb = re.search(pattern, url) return thumb.group(1)# 数据存储到csvdef write_to_file3(item): with open('猫眼top100.csv', 'a', encoding='utf_8_sig',newline='') as f: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] w = csv.DictWriter(f,fieldnames = fieldnames) # w.writeheader() w.writerow(item)# 封面下载def download_thumb(name, url,num): try: response = requests.get(url) with open('封面图/' + name + '.jpg', 'wb') as f: f.write(response.content) print('第%s部电影封面下载完毕' %num) print('------') except RequestException as e: print(e) pass # 存储格式是wb,因为图片是二进制数格式，不能用w，否则会报错# -----------------------------------------------------------------------------def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) # print(html) # parse_one_page2(html) for item in parse_one_page(html): # 切换内容提取方法 print(item) write_to_file(item) # 下载封面图 download_thumb(item['name'], item['thumb'],item['index'])# if __name__ == '__main__':# for i in range(10):# main(i * 10) # time.sleep(0.5) # 猫眼增加了反爬虫，设置0.5s的延迟时间# 2 使用多进程提升抓取效率from multiprocessing import Poolif __name__ == '__main__': pool = Pool() pool.map(main, [i * 10 for i in range(10)]) 二、可视化部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#-*- coding: utf-8 -*-# 可视化分析# -------------------------------import pandas as pdimport matplotlib.pyplot as pltimport pylab as pl # 用于修改x轴坐标plt.style.use('ggplot') # 默认绘图风格很难看，替换为好看的ggplot风格fig = plt.figure(figsize=(8, 5)) # 设置图片大小colors1 = '#6D6D6D' # 设置图表title、text标注的颜色columns = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] # 设置表头df = pd.read_csv('maoyan_top100.csv', encoding="utf-8", header=None, names=columns, index_col='index') # 打开表格# index_col = 'index' 将索引设为index# 1电影评分最高top10def annalysis_1(): df_score = df.sort_values('score', ascending=False) # 按得分降序排列 name1 = df_score.name[:10] # x轴坐标 score1 = df_score.score[:10] # y轴坐标 plt.bar(range(10), score1, tick_label=name1) # 绘制条形图，用range()能搞保持x轴正确顺序 plt.ylim((9, 9.8)) # 设置纵坐标轴范围 plt.title('电影评分最高top10', color=colors1) # 标题 plt.xlabel('电影名称') # x轴标题 plt.ylabel('评分') # y轴标题 # 为每个条形图添加数值标签 for x, y in enumerate(list(score1)): plt.text(x, y + 0.01, '%s' % round(y, 1), ha='center', color=colors1) pl.xticks(rotation=270) # x轴名称太长发生重叠，旋转为纵向显示 plt.tight_layout() # 自动控制空白边缘，以全部显示x轴名称 # plt.savefig('电影评分最高top10.png') #保存图片 plt.show()# ------------------------------# 2各国家的电影数量比较def annalysis_2(): area_count = df.groupby( by='area').area.count().sort_values(ascending=False) # 绘图方法1 area_count.plot.bar(color='#4652B1') # 设置为蓝紫色 pl.xticks(rotation=0) # x轴名称太长重叠，旋转为纵向 # 绘图方法2 # plt.bar(range(11),area_count.values,tick_label = area_count.index,color # = '#4652B1') for x, y in enumerate(list(area_count.values)): plt.text(x, y + 0.5, '%s' % round(y, 1), ha='center', color=colors1) plt.title('各国/地区电影数量排名', color=colors1) plt.xlabel('国家/地区') plt.ylabel('数量(部)') plt.show()# plt.savefig('各国(地区)电影数量排名.png')# ------------------------------# 3电影作品数量集中的年份# 从日期中提取年份def annalysis_3(): df['year'] = df['time'].map(lambda x: x.split('/')[0]) # print(df.info()) # print(df.head()) # 统计各年上映的电影数量 grouped_year = df.groupby('year') grouped_year_amount = grouped_year.year.count() top_year = grouped_year_amount.sort_values(ascending=False) # 绘图 top_year.plot(kind='bar', color='orangered') # 颜色设置为橙红色 for x, y in enumerate(list(top_year.values)): plt.text(x, y + 0.1, '%s' % round(y, 1), ha='center', color=colors1) plt.title('电影数量年份排名', color=colors1) plt.xlabel('年份(年)') plt.ylabel('数量(部)') plt.tight_layout() # plt.savefig('电影数量年份排名.png') plt.show()# ------------------------------# 4拥有电影作品数量最多的演员# 表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中def annalysis_4(): starlist = [] star_total = df.star for i in df.star.str.replace(' ', '').str.split(','): starlist.extend(i) # print(starlist) # print(len(starlist)) # set去除重复的演员名 starall = set(starlist) # print(starall) # print(len(starall)) starall2 = &#123;&#125; for i in starall: if starlist.count(i) &gt; 1: # 筛选出电影数量超过1部的演员 starall2[i] = starlist.count(i) starall2 = sorted(starall2.items(), key=lambda starlist: starlist[1], reverse=True) starall2 = dict(starall2[:10]) # 将元组转为字典格式 # 绘图 x_star = list(starall2.keys()) # x轴坐标 y_star = list(starall2.values()) # y轴坐标 plt.bar(range(10), y_star, tick_label=x_star) pl.xticks(rotation=270) for x, y in enumerate(y_star): plt.text(x, y + 0.1, '%s' % round(y, 1), ha='center', color=colors1) plt.title('演员电影作品数量排名', color=colors1) plt.xlabel('演员') plt.ylabel('数量(部)') plt.tight_layout() plt.show()# plt.savefig('演员电影作品数量排名.png')def main(): annalysis_1() annalysis_2() annalysis_3() annalysis_4()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>requests</tag>
        <tag>正则表达式</tag>
        <tag>beautifulsoup</tag>
        <tag>css</tag>
        <tag>xpath</tag>
        <tag>lxml</tag>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4块钱,用Github+Hexo搭建你的个人博客：美化篇]]></title>
    <url>%2Fhexo02.html</url>
    <content type="text"><![CDATA[上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。 摘要：搭建博客相对简单，而美化博客则要复杂一些，因为涉及到修改和增删源代码，对于没有前端基础的人来说，会比较费时间精力。为了尽可能在最短的时间里，打造一个总体看得过去的博客，本文以我的博客为例，介绍一些比较实用的博客美化操作和技巧。 1. 选择新的模板首先，是要更换非常难看的初始的博客界面。重新挑选一个好看的主题模板，然后在此基础上进行美化。 主题寻找：https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories 该网站按照模板的受欢迎程度进行排名，可以看到遥遥领先的第一名是一款叫作：next的主题，选用这款即可。进入到这个主题，可以阅读README.md模板使用说明，还可以查看模板示例网站。 模板使用：打开博客根目录下的themes文件夹(注：后文所说的根目录指：D:\blog)，右键Git Bash运行下述命令：git clone https://github.com/iissnan/hexo-theme-next themes/next就可以把这款主题的安装文件下载到电脑中。接着，打开D:\blog_config.yml文件，找到 theme字段，修改参数为：theme: hexo-theme-next，然后根目录运行下述命令：12hexo cleanhexo s -g 这样，便成功应用新的next主题，浏览器访问 :http://localhost:4000，查看一下新的博客页面。可以看到，博客变得非常清爽了，（可能和你实际看到的，略有不同，没有关系）。这款主题包含4种风格，默认的是Muse，也可以尝试其他风格。具体操作：打开D:\blog\_config.yml，定位到Schemes，想要哪款主题就取消前面的#，我的博客使用的是Pisces风格。12345# Schemes#scheme: Muse#scheme: Mistscheme: Pisces#scheme: Gemini 2. 模板美化接下来进行模板的美化。根据网页的结构布局，将从以下几个部分进行针对性地美化： 总体 侧边栏 页脚 文章 重要的文件美化需要主要是对几个模板文件进行修改和增删。为了便于后续进行操作，先列出文件名和所在的位置： 站点文件。位于站点文件夹根目录内：D:/blog/_config.yml 主题文件。位于主题文件夹根目录内：D:/blog/themes/next/_config.yml 自定义样式文件。位于主题文件夹内：D:\blog\themes\hexo-theme-next\source\css_custom\custom.styl 2.1. 总体布置 2.1.1. 设置中文界面站点文件: language: zh-Hans如果中文乱码，记事本另存为utf-8，最好不要用记事本编辑，用notepad。 2.1.2. 动态背景主题文件： canvas_nest: true背景的几何线条是采用的nest效果，一个基于html5 canvas绘制的网页背景效果，非常赞！来自github的开源项目canvas-nest：https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md 如果感觉默认的线条太多的话，可以这么设置：打开 next/layout/_layout.swig，在 &lt; /body&gt;之前添加代码(注意不要放在&lt; /head&gt;的后面)： 1234&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot;color=&quot;233,233,233&quot; opacity=&apos;0.9&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 说明：color ：线条颜色, 默认: ‘0,0,0’；三个数字分别为(R,G,B)opacity: 线条透明度（0~1）, 默认: 0.5count: 线条的总数量, 默认: 150zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 2.2. 侧边栏美化 2.2.1. 添加博客名字和slogan修改站点文件如下：123456789101112# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: 高级农民工 # 更改为你自己的subtitle: Beginner's Mind description:keywords: python,hexo,神器,软件author: 高级农民工language: zh-Hanstimezone: 2.2.2. 菜单设置文件路径：D:\blog\themes\hexo-theme-next\languages\zh-Hans.yml修改如下：1234567891011menu: home: 首&amp;emsp;&amp;emsp;页 archives: 归&amp;emsp;&amp;emsp;档 categories: 分&amp;emsp;&amp;emsp;类 tags: 标&amp;emsp;&amp;emsp;签 about: 关于博主 search: 站内搜索 top: 最受欢迎 schedule: 日程表 sitemap: 站点地图 # commonweal: 公益404 注意：两字的中间添加&amp;emsp;&amp;emsp;可实现列对齐。 2.2.3. 新建标签、分类、关于页面分别运行命令：123hexo new page &quot;tags&quot; hexo new page &quot;categories&quot; hexo new page &quot;about&quot; 然后，打开D:\blog\source就可以看到上述三个文件夹。要添加关于博主的介绍，只需要在/about/index.md文件中，用markdown书写内容即可，写完后运行：hexo d -g，便可看到效果。 2.2.4. 侧栏社交链接图标设置可以添加你的github、Email、知乎、简书等社交网站账号。主题文件：12345678910111213141516171819202122232425# ---------------------------------------------------------------# Sidebar Settings 侧栏社交链接图标设置# ---------------------------------------------------------------# Social Links.# Usage: `Key: permalink || icon`# Key is the link label showing to end users.# Value before `||` delimeter is the target permalink.# Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.social: GitHub: https://github.com/makcyun || github E-Mail: mailto:johnny824lee@gmail.com || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skypesocial_icons: enable: true icons_only: false transition: false 2.2.5. 添加头像并美化博客添加头像有两种方法：第一种是放在本地文件夹中：D:\blog\public\uploads，并且命名为avatar.jpg。第二种是将图片放在七牛云中，然后传入链接。推荐这种方式，可以加快网页打开速度。站点文件任意行添加下面代码：123456# 添加头像# avatar: /uploads/avatar.jpg #方法1本地图片avatar: http://pbscl931v.bkt.clouddn.com/18-8-3/40685653.jpg # 方法2网络图片注意：uppoads文件夹是在主题里的文件夹，没有则新建D:\blog\themes\hexo-theme-next\source\uploads\avatar.jpg 头像变圆形可参考：http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.htmlD:\blog\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;/*再进一步想点击产生旋转效果，就继续在该文件下方添加代码：*/img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125; 2.3. 页脚美化建站时间设置12345678910# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link.# Set rss to specific value if you have burned your feed already.rss:footer: # Specify the date when the site was setup. # If not defined, current year will be used. # 建站年份 since: 2018 #根据实际情况修改 2.3.1. 隐藏powered By Hexo/主题文件路径： D:\blog\themes\hexo-theme-next\layout_partials\ footer.swig更改该文件下面的代码：123456&lt;div class="theme-info"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt; 用注释两行如下语句，也可以直接删除掉这段代码：123456&lt;!--&lt;div class="theme-info"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;--&gt; 2.3.2. next版本隐藏继续在上面文件中修改代码如下：123456789# 用&lt;!--注释语句--&gt;&#123;% if theme.footer.theme.enable %&#125; &lt;!--&lt;div class="theme-info"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;--&gt;&#123;% endif %&#125; 2.3.3. 时间和用户名之间添加心形主题文件：建站时间下面修改icon: heart12345678910111213footer: # Specify the date when the site was setup. # If not defined, current year will be used. # 建站年份 since: 2018 # Icon between year and copyright info. # 年份后面的图标，为 Font Awesome 图标 # 自己去纠结 http://fontawesome.io/icons/ # 然后更改名字就行，下面的有关图标的设置都一样 # Icon between year and copyright info. #icon: user icon: heart 如果还想让心变成跳动的红心，则继续在:上面的footer.swig文件中修改：&lt;span class=&quot;with-love&quot;&gt;为 &lt;span class=&quot;with-love&quot; id=&quot;heart&quot;&gt; #一定要加id=”heart”12345678&lt;div class="copyright"&gt;&#123;##&#125;&#123;% set current = date(Date.now(), "YYYY") %&#125;&#123;##&#125;&amp;copy; &#123;% if theme.footer.since and theme.footer.since != current %&#125;&#123;&#123; theme.footer.since &#125;&#125; &amp;mdash; &#123;% endif %&#125;&#123;##&#125;&lt;span itemprop="copyrightYear"&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt; &lt;span class="with-love"&gt; &lt;i class="fa fa-&#123;&#123; theme.footer.icon &#125;&#125;"&gt;&lt;/i&gt; &lt;/span&gt; &lt;span class="author" itemprop="copyrightHolder"&gt;&#123;&#123; theme.footer.copyright || config.author &#125;&#125;&lt;/span&gt; 在自定义文件中添加如下代码：1234567891011121314// 1 页脚加闪烁红心// 自定义页脚跳动的心样式@keyframes heartAnimate &#123; 0%,100%&#123;transform:scale(1);&#125; 10%,30%&#123;transform:scale(0.9);&#125; 20%,40%,60%,80%&#123;transform:scale(1.1);&#125; 50%,70%&#123;transform:scale(1.1);&#125;&#125;#heart &#123; animation: heartAnimate 1.33s ease-in-out infinite;&#125;.with-love &#123; color: rgb(192, 0, 39);&#125; 接着在自定义custom.styl文件中，添加以下代码：1234567891011121314// 1 页脚加闪烁红心// 自定义页脚跳动的心样式@keyframes heartAnimate &#123; 0%,100%&#123;transform:scale(1);&#125; 10%,30%&#123;transform:scale(0.9);&#125; 20%,40%,60%,80%&#123;transform:scale(1.1);&#125; 50%,70%&#123;transform:scale(1.1);&#125;&#125;#heart &#123; animation: heartAnimate 1.33s ease-in-out infinite;&#125;.with-love &#123; color: rgb(192, 0, 39); # rgb可随意修改&#125; 2.3.4. 页脚显示总访客数和总浏览量首先，在上述footer.swig文件首行添加如下代码：1234567891011121314151617181920212223242526272829303132333435363738394041&lt;script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt;#接着修改相应代码：# 添加总访客量&lt;span id="busuanzi_container_site_uv"&gt; 访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次&lt;/span&gt;&#123;% if theme.footer.powered %&#125; &lt;!--&lt;div class="powered-by"&gt;&#123;# #&#125;&#123;&#123; __('footer.powered', '&lt;a class="theme-link" target="_blank" href="https://hexo.io"&gt;Hexo&lt;/a&gt;') &#125;&#125;&#123;##&#125;&lt;/div&gt;--&gt;&#123;% endif %&#125;# 添加'|'符号&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.custom_text %&#125; &lt;div class="footer-custom"&gt;&#123;# #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;# 添加总访问量&lt;span id="busuanzi_container_site_pv"&gt; 总访问量:&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次&lt;/span&gt;# 添加'|'符号&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125;# 添加博客全站共：&lt;div class="theme-info"&gt; &lt;div class="powered-by"&gt;&lt;/div&gt; &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 2.4. 文章美化 2.4.1. 显示统计字数和估计阅读时长修改主题文件：1234567891011# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcount# 显示统计字数和估计阅读时长# 注意：这个要安装插件，先进入站点文件夹根目录# 然后：npm install hexo-wordcount --savepost_wordcount: item_text: true wordcount: true min2read: true totalcount: false separated_meta: false 注意，做了以上修改后，发现字数只显示了数字并没有带相应的单位:字和分钟。因此，还需做如下修改：打开D:\blog\themes\hexo-theme-next\layout\_macro\ **post.swig**文件，添加单位：1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123;% if theme.post_wordcount.wordcount or theme.post_wordcount.min2read %&#125; &lt;div class="post-wordcount"&gt; &#123;% if theme.post_wordcount.wordcount %&#125; &#123;% if not theme.post_wordcount.separated_meta %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt; &#123;% endif %&#125; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-file-word-o"&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.post_wordcount.item_text %&#125; &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.wordcount') &#125;&#125;&amp;#58;&lt;/span&gt; &#123;% endif %&#125; &lt;span title="&#123;&#123; __('post.wordcount') &#125;&#125;"&gt; &#123;&#123; wordcount(post.content) &#125;&#125; 字 &lt;/span&gt; &#123;% endif %&#125; &#123;% if theme.post_wordcount.wordcount and theme.post_wordcount.min2read %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt; &#123;% endif %&#125; &#123;% if theme.post_wordcount.min2read %&#125; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-clock-o"&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.post_wordcount.item_text %&#125; &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.min2read') &#125;&#125; &amp;asymp;&lt;/span&gt; &#123;% endif %&#125; &lt;span title="&#123;&#123; __('post.min2read') &#125;&#125;"&gt; &#123;&#123; min2read(post.content) &#125;&#125; 分钟 &lt;/span&gt; &#123;% endif %&#125; &lt;/div&gt; &#123;% endif %&#125; &#123;% if post.description and (not theme.excerpt_description or not is_index) %&#125; &lt;div class="post-description"&gt; &#123;&#123; post.description &#125;&#125; &lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt; &lt;/header&gt; &#123;% endif %&#125; 2.4.2. 添加阅读全文实现在主页只展示部分文字，其他文字隐藏起来，通过点击’阅读更多’来阅读全文。方法就是写每一篇文章的时候，在必要的地方添加&lt;!-- more --&gt;即可。例如：1234567891011121314---title: 4块钱,用Github+Hexo搭建你的个人博客：搭建篇id: hexo01images: http://pbscl931v.bkt.clouddn.com/18-8-3/89578286.jpgcategories: hexo博客tags: [hexo,个人博客,github]keywords: hexo,搭建博客,github pages,next---4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。 &lt;!-- more --&gt;摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。 2.4.3. 显示每篇文章的阅读量参考这个教程即可：http://www.jeyzhang.com/hexo-next-add-post-views.html 在这个过程中发现了一个问题：pc端正常显示阅读量，但是移动端没有显示具体的阅读量。解决办法：在leancloud网站上，进入安全中心，检查web安全域名列表中是否添加了http：开头的域名，如果没有，则添加上应该就能解决，例如，我的：1http://makcyun.top/ 2.4.4. 文章摘要配图参考这个教程即可：http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/ 附上我的设置：在自定义文件中添加如下代码：1234567891011121314151617181920212223242526// img.img-topic &#123;// width: 100%;//&#125;//图片外部的容器方框.out-img-topic &#123; display: block; max-height:350px; //图片显示高度，如果不设置则每篇文章的图片高度会不一样，看起来不协调 margin-bottom: 24px; overflow: hidden;&#125;//图片img.img-topic &#123; display: block ; margin-left: .7em; margin-right: .7em; padding: 0; float: right; clear: right;&#125;// 去掉图片边框.posts-expand .post-body img &#123; border: none; padding: 0px;&#125; 2.4.5. 添加打赏功能参考下面的教程：https://www.cnblogs.com/mrwuzs/p/7943337.htmlhttps://blog.csdn.net/lcyaiym/article/details/76796545 以上，包括了博客美化的大部分操作。如果，你觉得还不够，想做得更精致一些，那么推荐一个非常详细的美化教程：https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl 本文完。]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4块钱,用Github+Hexo搭建你的个人博客：搭建篇]]></title>
    <url>%2Fhexo01.html</url>
    <content type="text"><![CDATA[4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。 【更新于2018/7/14】 摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。好在参考了很多大佬的教程后顺利搭建完成，但过程中还是踩了一些坑。这里及时进行总结，作为博客的第一篇文章。 一、前言1 网上有很多现成的博客不用，为什么要自己搭建?可能有人会说：很多网站都能写博客，而且博客现在其实都有点过时了，为什么还要自己去搞？ 这里我说一下我想自己搭建的两点原因：一、网上的多数博客大家都共用一套相同的模板界面，没有特点、界面也杂乱充斥着很多不相关的东西，不论是自己写还是给人看，体验都不好。二、拥有一个你自己可以起名字的博客网站，里面的任何内容完全由你自己决定，这感觉是件很酷的事。 这些普通的网站博客和一些个人博客，哪个好看，高下立判吧。 vs &nbsp; vs 更多个人博客：litten &nbsp; http://litten.me/Ryan &nbsp; http://ryane.top/liyin &nbsp; https://liyin.date/reuixiy &nbsp; https://reuixiy.github.io/Tranquilpeak &nbsp; https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/ 2 搭建博客难不难？我之前认为搭建博客是一件只有能程序猿才能做出来的高大上的活。其实，只要跟着网上的教程一步步做下去，一个小时不到就可以搭建好你自己的个人博客。所以，搭建博客其实很简单。只不过，如果你想把博客做得好看一些的话，才会花费一些精力。 二、开始搭建博客如果看到上面那些精美的博客，你已经心动了，那就开始动手吧。下面正式开始博客搭建步骤。 搭建教程参考搭建博客的教程网上一搜一大堆，为了节省你的搜索时间，这里我筛选出了下面几篇很棒的教程，我基本都是跟着一步步做下来的。 小白独立搭建博客 2018，你该搭建自己的博客了！ 手把手教你用Hexo+Github 搭建属于自己的博客 操作平台:Win7 64位。 相关名词解释：Hexo：一种常用的博客框架，有了它建立博客非常简单。你可以认为它是一种博客模板，只不过它比普通网站的那种博客模板要好看地多，并且有很高度的自定义性，只要你愿意，你可以建立一个独一无二的博客来。若想详细了解Hexo的使用，移步 Hexo官方网站 https://hexo.io/zh-cn/docs/。 Github：一个全世界程序猿聚集的知名网站。免费的远程仓库和开源协作社区。我们需要利用网站里的Github Pages功能来托管需发布到网上的博客的相关文件和代码。 Git： 一种版本控制系统。我们在自己的本地电脑写博客，如何把博客同步到Github，然后发布到网上去？就需要用这个软件去写几行代码然后就能搞定，后期用的最多的就是它。 Node.js： 提供JavaScript的开发环境，安装好以后就不用跟它再打交道，所以不用太关注它。 1 软件安装配置搭建博客需要先下载2个软件：Git和Nodejs。软件安装过程很简单，一直点击Next默认直到安装完成就行了。 Git官网：https://git-scm.com/download/win安装完，打开cmd窗口运行下面命令，如果有返回版本信息说明安装成功。 git –version Nodejs官网：https://nodejs.org/en/download/同样，安装完有返回版本信息说明安装成功，见下图。 node -v npm -v 至此，软件安装步骤完成。 2 安装Hexo博客框架 安装hexo 这里开始就要用到使用频率最高的Git软件了。 桌面右键点击git bash here选项，会打开Git软件界面，输入下面每行命令并回车： 12npm install hexo-cli -gnpm install hexo-deployer-git --save 第一句是安装hexo，第二句是安装hexo部署到git page的deployer。代码命令看不懂没关系，一是这些命令之后几乎不再用到，二是用多了你会慢慢记住。 设置博客存放文件夹 你需要预想一下你要把博客文件新建在哪个盘下的文件夹里，除了c盘都可以，例如d盘根目录下的blog文件夹，紧接着在桌面输入下面命令并回车： hexo init /d/blog cd /d/blog npm install *注：/d/bog可以更改为你自己的文件夹* 有的教程是先新建博客文件夹，在该文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，再执行以下操作。但是我操作过程中出现过这样的失败提示：hexo:conmand not found，但我执行上面的命令时就没有出现该问题。 hexo init npm install 查看博客效果 至此，博客初步搭建好，输入下面一行本地部署生成的命令： hexo s -g 然后打开浏览器在网址栏输入：localhost:4000就可以看到博客的样子，如果无法打开，则继续输入下面命令： npm install hexo-deployer-git --save hexo clean hexo s -g 打开该网址，你可以看到第一篇默认的博客：Hello World。但看起来很难看，后续会通过重新选择模板来对博客进行美化。 现在你就可以开始写博客了，但是博客只能在你自己的电脑上看到，别人无法在网上看到你的博客。接下来需要利用前面提到的的Github Pages功能进行设置，设置完成之后别人通过搜索就可以看到你的博客。 3 把你的博客部署到Github Pages上去这是搭建博客相对比较复杂也是容易出错的一部分。 1. Github账号注册及配置 如果你没有github帐号，就新建一个，然后去邮箱进行验证；如果你有帐号则直接登录。官网：https://github.com/ 配置步骤： 建立new repository 只填写username.github.io即可，然后点击create repositrory。注意：username.github.io 的username要和用户名保持一致，不然后面会失败。以我的为例： 开启gh-pages功能 点击github主页点击头像下面的profile,找到新建立的username.github.io文件打开，点击settings，往下拉动鼠标到GitHub Pages。如果你看到上方出现以下警告： GitHub Pages is currently disabled. You must first add content to your repository before you can publish a GitHub Pages site 不用管他，点击选择choose a theme，随便选择一个，（之后我们要更改这些丑陋的模板），然后select theme保存就行了。 接下来的几个步骤参考教程1即可。 主要步骤包括： git创建SSH密钥 在GitHub账户中添加你的公钥 测试成功并设置用户信息 将本地的Hexo文件更新到Github库中 hexo部署更新博客 经过以上几步的操作，顺利的话，你的博客可以发布到网上，其他人也可以通过你的网址username.github.io（我的是makcyun.github.io）访问到你的博客。 4 赶紧新建个博客试试接下来你可以自己新建一个文档来写下你的第一篇博客并在网页上测试。 同样在根目录D:\blog中运行下面命令： hexo new 第一篇博客 *注：第一篇博客名称可以随便修改* 然后打开D:\blog\source\_posts文件夹，就可以看到一个第一篇博客.md的文件。用支持markdown语法的软件打开该文件进行编辑即可。 编辑好以后，运行下述命令： hexo clean hexo d -g 然后，在网址中输入username.github.io即可看到你的博客上，出现第一篇博客这篇新的文章。 至此，你的个人博客初步搭建过程就完成了。 但是，现在还存在两个问题你可能想解决： markdown语法是什么，如何用软件编写博客？ 网址是username.github.io，感觉很奇怪，而我的博客网址怎么是www开头的？ 好，下面来讲解一下。 第一个问题 关于markdown语法介绍：markdown——入门指南 当你大致了解markdown语法后，如何用markdown写博客呢？不妨参考这两篇详细教程： Markdown語法說明HEXO下的Markdown语法(GFM)写博客 接下来你要一个可以写markdown语法的软件，这里推荐两款软件。 Windows下使用Markdown Pad2, Mac下使用Mou。 我用的是Markdown Pad2，这款软件是付费软件，但网上有很多破解版，我这里将软件上传到了百度网盘，如需请取。MarkdownPad2： https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA 密码：y9zh 安装好后，就可以打开刚才的第一篇博客.md，开始尝试写你的第一篇博客了。 比如这是我用markdownpad写的博客原稿。 可以看到文档里面都是字符，没有图片这些。所以只需要用键盘专注于打字就行了，不需要像word那么复杂，还要用鼠标插入标题样式、图片这些操作。 第二个问题 我的网址不是默认的username.github.io，是因为我购买了一个域名，然后和username.github.io进行了关联，这样我的博客网址变成了我的域名。 在哪里购买域名呢？首推去 阿里云官网 购买。 你可以随意起你喜欢的名字，然后在该网站进行搜索，没有人占用的话你就可以购买该域名。不同的后缀价格不同。可以看到.com、 .net等会比较贵，最便宜的这两年新出的.top域名，只要4块钱一年，我购买的就是这种。 购买完域名以后，需要做以下几个步骤： 实名认证 修改DNS 域名解析 新建CNAME文件 1 实名认证在修改DNS之前，必须要阿里云官网实名认证成功，用淘宝账户登录然后填写相关信息即可。 2 修改DNS实名认证成功后，进入管理界面，依次点击： 修改DNS为：f1g1ns1.dnspod.netf1g1ns2.dnspod.net 3 域名解析DNS修改好以后，到DNSPOD这个网站去解析你的域名。 首先，微信登录并注册 https://www.dnspod.cn/，点击域名解析，添加上你的域名。 接着，添加以下两条记录即可。 注意：makcyun.github.io.需换成你自己的名称，另外最后有一个“.” 4 新建CNAME文件在博客根目录文件夹下,例如我的D:\blog\source，新建名为CNAME的记事本文件，去掉后缀。在里面输入你的域名，例如我的：www.makcyun.top即可，保存并关闭。 注意： 这里填不填写www前缀都是可以的，区别在于填写www，那么博客网址就会以www开头，例如：www.makcyun.top；如果不填写，博客网址是：makcyun.top，二者都可以，看你喜欢。 完成以上4步之后，根目录下再次运行： hexo d -g 这时，输入你在记事本里的域名网址，即可打开你的博客。至此，你的博客就换成了你想要的网址，别人也可以通过这个网址访问到你的博客。 到这里，博客的初步搭建就算完成了，顺利的话不到1小时就能完成，如果中间出现差错，保持耐心多试几次应该就没问题。 此时，还有一个比较重要的问题就是，你可能会觉得你的博客不够美观，不如我博客里前面提到的那几个博客，甚至也没有我的好看。 如果你还愿意折腾的话，下一篇文章，我会以我的博客为例，讲一讲如何进行博客的美化。]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
