<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Requests+多种方法爬取猫眼top100电影]]></title>
    <url>%2Fweb_scraping_withpython1.html</url>
    <content type="text"><![CDATA[python爬虫第1篇利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。 摘要： 作为小白，爬虫可以说是入门python最快和最容易获得成就感的途径。因为初级爬虫的套路相对固定，常见的方法只有几种，比较好上手。最近，跟着崔庆才大佬的书：python3网络爬虫开发实战 学习爬虫。选取网页结构较为简单的猫眼top100电影为案例进行练习。 重点是用上述所说的4种方法提取出关键内容。一个问题采用不同的解决方法有助于拓展思维，通过不断练习就能够灵活运用。 本文知识点：Requsts 请求库的使用beautiful+lxml两大解析库使用正则表达式 、xpath、css选择器的使用 1. 为什么爬取该网页？ 比较懒，不想一页页地去翻100部电影的介绍，想在一个页面内进行总体浏览（比如在excel表格中）； 想深入了解一些比较有意思的信息，比如：哪部电影的评分最高？哪位演员的作品数量最多？哪个国家/地区上榜的电影数量最多？哪一年上榜的电影作品最多等。这些信息在网页上是不那么容易能直接获得的，所以需要爬虫。 2. 爬虫目标 从网页中提取出top100电影的电影名称、封面图片、排名、评分、演员、上映国家/地区、评分等信息，并保存为csv文本文件。 根据爬取结果，进行简单的可视化分析。 平台：windows7 + SublimeText3 3. 爬取步骤3.1. 网址URL分析首先，打开猫眼Top100的url网址： http://maoyan.com/board/4?offset=0。页面非常简单，所包含的信息就是上述所说的爬虫目标。下拉页面到底部，点击第2页可以看到网址变为：http://maoyan.com/board/4?offset=10。因此，可以推断出url的变化规律：offset表示偏移，10代表一个页面的电影偏移数量，即：第一页电影是从0-10，第二页电影是从11-20。因此，获取全部100部电影，只需要构造出10个url，然后依次获取网页内容，再用不同的方法提取出所需内容就可以了。下面，用requests方法获取第一个页面。 3.2. Requests获取首页数据先定义一个获取单个页面的函数：get_one_page()，传入url参数。 12345678910111213def get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; # 不加headers爬不了 response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except RequestException: return None # try-except语句捕获异常 接下来在main()函数中设置url。 12345678def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) print(html)if __name__ == '__main__': main() 运行上述程序后，首页的源代码就被爬取下来了。如下图所示： 接下来就需要从整个网页中提取出几项我们需要的内容，用到的方法就是上述所说的四种方法，下面分别进行说明。 3.3. 4种内容解析提取方法3.3.1. 正则表达式提取第一种是利用正则表达式提取。什么是正则表达式？ 下面这串看起来乱七八糟的符号就是正则表达式的语法。 1'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?' 它是一种强大的字符串处理工具。之所以叫正则表达式，是因为它们可以识别正则字符串（regular string）。可以这么定义：“ 如果你给我的字符串符合规则，我就返回它”；“如果字符串不符合规则，我就忽略它”。通过requests抓取下来的网页是一堆大量的字符串，用它处理后便可提取出我们想要的内容。 如果还不了解它，可以参考下面的教程： http://www.runoob.com/regexp/regexp-syntax.htmlhttps://www.w3cschool.cn/regexp/zoxa1pq7.html 正则表达式常用语法： table th:nth-of-type(1) { width: 60px; } 模式 描述 \w 匹配字母数字及下划线 \W 匹配非字母数字及下划线 \s 匹配任意空白字符，等价于 [\t\n\r\f] \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9] \D 匹配任意非数字 \n 匹配一个换行符 \t 匹配一个制表符 ^ 匹配字符串开始位置的字符 $ 匹配字符串的末尾 . 匹配任意字符，除了换行符 […] 用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’ [^…] 不在 [ ] 中的字符 * 匹配前面的字符、子表达式或括号里的字符 0 次或多次 + 同上，匹配至少一次 ? 同上，匹配0到1次 {n} 匹配前面的字符、子表达式或括号里的字符 n 次 {n, m} 同上，匹配 m 到n 次（包含 m 或 n） ( ) 匹配括号内的表达式，也表示一个组 下面，开始提取关键内容。右键网页-检查-Network选项，选中左边第一个文件然后定位到电影信息的相应位置，如下图： 可以看到每部电影的相关信息都在dd这个节点之中。所以就可以从该节点运用正则进行提取。第1个要提取的内容是电影的排名。它位于class=”board-index”的i节点内。不需要提取的内容用’.*?’替代，需要提取的数字排名用（）括起来，（）里面的数字表示为（\d+）。正则表达式可以写为： 1'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;' 接着，第2个需要提取的是封面图片，图片网址位于img节点的’data-src’属性中，正则表达式可写为： 1'data-src="(.*?)".*?' 第1和第2个正则之间的代码是不需要的，用’.*?’替代，所以这两部分合起来写就是： 1'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)" 同理，可以依次用正则写下主演、上映时间和评分等内容,完整的正则表达式如下： 1'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;' 正则表达式写好以后，可以定义一个页面解析提取方法：parse_one_page（），用来提取内容： 12345678910111213141516171819def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) # re.S表示匹配任意字符，如果不加，则无法匹配换行符 items = re.findall(pattern, html) # print(items) for item in items: yield &#123; 'index': item[0], 'thumb': get_thumb(item[1]), # 定义get_thumb()方法进一步处理网址 'name': item[2], 'star': item[3].strip()[3:], # 'time': item[4].strip()[5:], # 用两个方法分别提取time里的日期和地区 'time': get_release_time(item[4].strip()[5:]), 'area': get_release_area(item[4].strip()[5:]), 'score': item[5].strip() + item[6].strip() # 评分score由整数+小数两部分组成 &#125; Tips:re.S:匹配任意字符，如果不加，则无法匹配换行符；yield:使用yield的好处是作为生成器，可以遍历迭代，并且将数据整理形成字典，输出结果美观。具体用法可参考：https://blog.csdn.net/zhangpinghao/article/details/18716275；.strip():用于去掉字符串中的空格。 上面程序为了便于提取内容，又定义了3个方法：get_thumb（）、get_release_time（）和 get_release_area（）： 1234567891011121314151617181920212223242526# 获取封面大图def get_thumb(url): pattern = re.compile(r'(.*?)@.*?') thumb = re.search(pattern, url) return thumb.group(1)# http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c# 去掉@160w_220h_1e_1c就是大图 # 提取上映时间函数def get_release_time(data): pattern = re.compile(r'(.*?)(\(|$)') items = re.search(pattern, data) if items is None: return '未知' return items.group(1) # 返回匹配到的第一个括号(.*?)中结果即时间# 提取国家/地区函数def get_release_area(data): pattern = re.compile(r'.*\((.*)\)') # $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?) items = re.search(pattern, data) if items is None: return '未知' return items.group(1) Tips:‘r’：正则前面加上’r’ 是为了告诉编译器这个string是个raw string，不要转意’\’。当一个字符串使用了正则表达式后，最好在前面加上’r’；‘|’ ‘$’： 正则’|’表示或’，’$’表示匹配一行字符串的结尾；.group(1)：意思是返回search匹配的第一个括号中的结果，即(.*?)，gropup()则返回所有结果2013-12-18(，group(1)返回’（’。 接下来，修改main()函数来输出爬取的内容： 12345678910def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) for item in parse_one_page(html): print(item)if __name__ == '__main__': main() Tips:if name == ‘_ _main__’:当.py文件被直接运行时，if name == ‘_ main__’之下的代码块将被运行；当.py文件以模块形式被导入时，if name == ‘ _main__’之下的代码块不被运行。参考：https://blog.csdn.net/yjk13703623757/article/details/77918633。 运行程序，就可成功地提取出所需内容，结果如下： 123456789&#123;'index': '1', 'thumb': 'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg', 'name': '霸王别姬', 'star': '张国荣,张丰毅,巩俐', 'time': '1993-01-01', 'area': '中国香港', 'score': '9.6'&#125;&#123;'index': '2', 'thumb': 'http://p0.meituan.net/movie/54617769d96807e4d81804284ffe2a27239007.jpg', 'name': '罗马假日', 'star': '格利高里·派克,奥黛丽·赫本,埃迪·艾伯特', 'time': '1953-09-02', 'area': '美国', 'score': '9.1'&#125;&#123;'index': '3', 'thumb': 'http://p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg', 'name': '肖申克的救赎', 'star': '蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿', 'time': '1994-10-14', 'area': '美国', 'score': '9.5'&#125;&#123;'index': '4', 'thumb': 'http://p0.meituan.net/movie/e55ec5d18ccc83ba7db68caae54f165f95924.jpg', 'name': '这个杀手不太冷', 'star': '让·雷诺,加里·奥德曼,娜塔莉·波特曼', 'time': '1994-09-14', 'area': '法国', 'score': '9.5'&#125;&#123;'index': '5', 'thumb': 'http://p1.meituan.net/movie/f5a924f362f050881f2b8f82e852747c118515.jpg', 'name': '教父', 'star': '马龙·白兰度,阿尔·帕西诺,詹姆斯·肯恩', 'time': '1972-03-24', 'area': '美国', 'score': '9.3'&#125;...&#125;[Finished in 1.9s] 以上是第1种提取方法，如果还不习惯正则表达式这种复杂的语法，可以试试下面的第2种方法。 3.3.2. lxml结合xpath提取该方法需要用到lxml这款解析利器，同时搭配xpath语法，利用它的的路径选择表达式，来高效提取所需内容。lxml包为第三方包，需要自行安装。如果对xpath的语法还不太熟悉，可参考下面的教程：http://www.w3school.com.cn/xpath/xpath_syntax.asp xpath常用的规则 表达式 描述 nodename 选取此节点的所有子节点 / 从当前节点选取直接子节点 // 从当前节点选取子孙节点 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 12345678910111213141516171819202122232425262728293031323334&lt;/div&gt; &lt;div class="container" id="app" class="page-board/index" &gt;&lt;div class="content"&gt; &lt;div class="wrapper"&gt; &lt;div class="main"&gt; &lt;p class="update-time"&gt;2018-08-18&lt;span class="has-fresh-text"&gt;已更新&lt;/span&gt;&lt;/p&gt; &lt;p class="board-content"&gt;榜单规则：将猫眼电影库中的经典影片，按照评分和评分人数从高到低综合排序取前100名，每天上午10点更新。相关数据来源于“猫眼电影库”。&lt;/p&gt; &lt;dl class="board-wrapper"&gt; &lt;dd&gt; &lt;i class="board-index board-index-1"&gt;1&lt;/i&gt; &lt;a href="/films/1203" title="霸王别姬" class="image-link" data-act="boarditem-click" data-val="&#123;movieId:1203&#125;"&gt; &lt;img src="//ms0.meituan.net/mywww/image/loading_2.e3d934bf.png" alt="" class="poster-default" /&gt; &lt;img data-src="http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c" alt="霸王别姬" class="board-img" /&gt; &lt;/a&gt; &lt;div class="board-item-main"&gt; &lt;div class="board-item-content"&gt; &lt;div class="movie-item-info"&gt; &lt;p class="name"&gt;&lt;a href="/films/1203" title="霸王别姬" data-act="boarditem-click" data-val="&#123;movieId:1203&#125;"&gt;霸王别姬&lt;/a&gt;&lt;/p&gt; &lt;p class="star"&gt; 主演：张国荣,张丰毅,巩俐 &lt;/p&gt;&lt;p class="releasetime"&gt;上映时间：1993-01-01(中国香港)&lt;/p&gt; &lt;/div&gt; &lt;div class="movie-item-number score-num"&gt;&lt;p class="score"&gt;&lt;i class="integer"&gt;9.&lt;/i&gt;&lt;i class="fraction"&gt;6&lt;/i&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/dd&gt; &lt;dd&gt; 根据截取的部分html网页，先来提取第1个电影排名信息，有两种方法。第一种：直接复制。右键-Copy-Copy Xpath，得到xpath路径为：//*[@id=”app”]/div/div/div[1]/dl/dd[1]/i,为了能够提取到页面所有的排名信息，需进一步修改为：//*[@id=”app”]/div/div/div[1]/dl/dd/i/text()，如果想要再精简一点，可以省去中间部分绝对路径’/‘然后用相对路径’//‘代替，最后进一步修改为：//*[@id=”app”]//div//dd/i/text()。 第二种：观察网页结构自己写。首先注意到id = app的div节点，因为在整个网页结构id是唯一的不会有第二个相同的，所有可以将该div节点作为xpath语法的起点，然后往下观察分别是3级div节点，可以省略写为：//div,再往下分别是是两个并列的p节点、dl节点、dd节点和最后的i节点文本。中间可以随意省略，只要保证该路径能够选择到唯一的文本值‘1’即可，例如省去p和dl节点，只保留后面的节点。这样，完整路径可以为：//*[@id=”app”]//div//dd/i/text()，和上式一样。 根据上述思路，可以写下其他内容的xpath路径。观察到路径的前一部分：//*[@id=”app”]//div//dd都是一样的，从后面才开始不同，因此为了能够精简代码，将前部分路径赋值为一个变量items，最终提取的代码如下： 12345678910111213141516171819202122232425# 2 用lxml结合xpath提取内容def parse_one_page2(html): parse = etree.HTML(html) items = parse.xpath('//*[@id="app"]//div//dd') # 完整的是//*[@id="app"]/div/div/div[1]/dl/dd # print(type(items)) # *代表匹配所有节点，@表示属性 # 第一个电影是dd[1],要提取页面所有电影则去掉[1] # xpath://*[@id="app"]/div/div/div[1]/dl/dd[1] for item in items: yield&#123; 'index': item.xpath('./i/text()')[0], #./i/text()前面的点表示从items节点开始 #/text()提取文本 'thumb': get_thumb(str(item.xpath('./a/img[2]/@data-src')[0].strip())), # 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。 'name': item.xpath('./a/@title')[0], 'star': item.xpath('.//p[@class = "star"]/text()')[0].strip(), 'time': get_release_time(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'area': get_release_area(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'score' : item.xpath('.//p[@class = "score"]/i[1]/text()')[0] + \ item.xpath('.//p[@class = "score"]/i[2]/text()')[0] &#125; Tips:[0]：xpath后面添加了[0]是因为返回的是只有1个字符串的list，添加[0]是将list提取为字符串，使其简洁；Network：要在最原始的Network选项卡中定位，而不是Elements中，不然提取不到相关内容；class属性：p[@class = “star”]/text()表示提取class属性为”star”的p节点的文本值；提取属性值：img[2]/@data-src’：提取img节点的data-src属性值，属性值后面无需添加’/text()’ 运行程序，就可成功地提取出所需内容，结果和第一种方法一样。 以上是第2种提取方法，如果也不太习惯xpath语法，可以试试下面的第3种方法。 3.3.3. Beautiful Soup + css选择器Beautiful Soup 同lxml一样，是一个非常强大的python解析库，可以从HTML或XML文件中提取效率非常高。关于它的用法，可参考下面的教程：https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ css选择器选是一种模式，用于选择需要添加样式的元素，使用它的语法同样能够快速定位到所需节点，然后提取相应内容。使用方法可参考下面的教程：http://www.w3school.com.cn/cssref/css_selectors.asp css选择器常用的规则 选择器 例子 例子描述 .class .intro 选择 class=”intro” 的所有元素。 #id #firstname 选择 id=”firstname” 的所有元素。 * * 选择所有元素。 element p 选择所有p元素。 element,element div,p 选择所有div元素和所有p元素。 element?element div p 选择div元素内部的所有p元素。 element&gt;element div&gt;p 选择父元素为div元素的所有p元素。 element+element div+p 选择紧接在div元素之后的所有p元素。 [attribute] [target] 选择带有 target 属性所有元素。 [attribute=value] [target=_blank] 选择 target=”_blank” 的所有元素。 下面就利用这种方法进行提取：12345678910111213141516171819202122# 3 用beautifulsoup + css选择器提取def parse_one_page3(html): soup = BeautifulSoup(html, 'lxml') # print(content) # print(type(content)) # print('------------') items = range(10) for item in items: yield&#123; 'index': soup.select('dd i.board-index')[item].string, # iclass节点完整地为'board-index board-index-1',写board-index即可 'thumb': get_thumb(soup.select('a &gt; img.board-img')[item]["data-src"]), # 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值 'name': soup.select('.name a')[item].string, 'star': soup.select('.star')[item].string.strip()[3:], 'time': get_release_time(soup.select('.releasetime')[item].string.strip()[5:]), 'area': get_release_area(soup.select('.releasetime')[item].string.strip()[5:]), 'score': soup.select('.integer')[item].string + soup.select('.fraction')[item].string &#125; 运行上述程序，结果同第1种方法一样。 3.3.4. Beautiful Soup + find_all函数提取Beautifulsoup除了和css选择器搭配，还可以直接用它自带的find_all函数进行提取。find_all，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。它的API如下： 1find_all(name , attrs , recursive , text , **kwargs) 常用的语法规则如下：soup.find_all(name=’ul’)： 查找所有ul节点，ul节点内还可以嵌套；li.string和li.get_text()：都是获取li节点的文本，但推荐使用后者；soup.find_all(attrs={‘id’: ‘list-1’}))：传入 attrs 参数，参数的类型是字典类型，表示查询 id 为 list-1 的节点；常用的属性比如 id、class 等，可以省略attrs采用更简洁的形式，例如：soup.find_all(id=’list-1’)soup.find_all(class_=’element’) 根据上述常用语法，可以提取网页中所需内容： 12345678910111213141516def parse_one_page4(html): soup = BeautifulSoup(html,'lxml') items = range(10) for item in items: yield&#123; 'index': soup.find_all(class_='board-index')[item].string, 'thumb': soup.find_all(class_ = 'board-img')[item].attrs['data-src'], # 用.get('data-src')获取图片src链接，或者用attrs['data-src'] 'name': soup.find_all(name = 'p',attrs = &#123;'class' : 'name'&#125;)[item].string, 'star': soup.find_all(name = 'p',attrs = &#123;'class':'star'&#125;)[item].string.strip()[3:], 'time': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'area': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'score':soup.find_all(name = 'i',attrs = &#123;'class':'integer'&#125;)[item].string.strip() + soup.find_all(name = 'i',attrs = &#123;'class':'fraction'&#125;)[item].string.strip() &#125; 以上就是4种不同的内容提取方法。 3.4. 数据存储上述输出的结果为字典格式，可利用csv包的DictWriter函数将字典格式数据存储到csv文件中。 123456789# 数据存储到csvdef write_to_file3(item): with open('猫眼top100.csv', 'a', encoding='utf_8_sig',newline='') as f: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] w = csv.DictWriter(f,fieldnames = fieldnames) # w.writeheader() w.writerow(item) 然后修改一下main()方法：1234567891011def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) for item in parse_one_page(html): # print(item) write_to_csv(item)if __name__ == '__main__': main() 结果如下图： 再将封面的图片下载下来： 1234567891011def download_thumb(name, url,num): try: response = requests.get(url) with open('封面图/' + name + '.jpg', 'wb') as f: f.write(response.content) print('第%s部电影封面下载完毕' %num) print('------') except RequestException as e: print(e) pass # 不能是w，否则会报错，因为图片是二进制数据所以要用wb 3.5. 分页爬取上面完成了一页电影数据的提取，接下来还需提取剩下9页共90部电影的数据。对网址进行遍历，给网址传入一个offset参数即可，修改如下： 123456789101112def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): # print(item) write_to_csv(item)if __name__ == '__main__': for i in range(10): main(offset = i*10) 这样就完成了所有电影的爬取。结果如下： 4. 可视化分析俗话说“文不如表，表不如图”。下面根据excel的数据结果，进行简单的数据可视化分析，并用图表呈现。 4.1. 电影评分最高top10首先，想看一看评分最高的前10部电影是哪些？ 程序如下：123456789101112131415161718192021222324252627282930import pandas as pdimport matplotlib.pyplot as pltimport pylab as pl #用于修改x轴坐标plt.style.use('ggplot') #默认绘图风格很难看，替换为好看的ggplot风格fig = plt.figure(figsize=(8,5)) #设置图片大小colors1 = '#6D6D6D' #设置图表title、text标注的颜色columns = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] #设置表头df = pd.read_csv('maoyan_top100.csv',encoding = "utf-8",header = None,names =columns,index_col = 'index') #打开表格# index_col = 'index' 将索引设为indexdf_score = df.sort_values('score',ascending = False) #按得分降序排列name1 = df_score.name[:10] #x轴坐标score1 = df_score.score[:10] #y轴坐标 plt.bar(range(10),score1,tick_label = name1) #绘制条形图，用range()能搞保持x轴正确顺序plt.ylim ((9,9.8)) #设置纵坐标轴范围plt.title('电影评分最高top10',color = colors1) #标题plt.xlabel('电影名称') #x轴标题plt.ylabel('评分') #y轴标题# 为每个条形图添加数值标签for x,y in enumerate(list(score1)): plt.text(x,y+0.01,'%s' %round(y,1),ha = 'center',color = colors1)pl.xticks(rotation=270) #x轴名称太长发生重叠，旋转为纵向显示plt.tight_layout() #自动控制空白边缘，以全部显示x轴名称# plt.savefig('电影评分最高top10.png') #保存图片plt.show() 结果如下图：可以看到：排名最高的分别是两部国产片”霸王别姬”和”大话西游”，其他还包括”肖申克的救赎”、”教父”等。嗯，还好基本上都看过。 4.2. 各国家的电影数量比较然后，想看看100部电影都是来自哪些国家？程序如下：1234567891011121314151617area_count = df.groupby(by = 'area').area.count().sort_values(ascending = False)# 绘图方法1area_count.plot.bar(color = '#4652B1') #设置为蓝紫色pl.xticks(rotation=0) #x轴名称太长重叠，旋转为纵向# 绘图方法2# plt.bar(range(11),area_count.values,tick_label = area_count.index)for x,y in enumerate(list(area_count.values)): plt.text(x,y+0.5,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('各国/地区电影数量排名',color = colors1)plt.xlabel('国家/地区')plt.ylabel('数量(部)')plt.show()# plt.savefig('各国(地区)电影数量排名.png') 结果如下图：可以看到，除去网站自身没有显示国家的电影以外，上榜电影被10个国家/地区”承包”了。其中，美国以30部电影的绝对优势占据第1名，其次是8部的日本，韩国第3，居然有7部上榜。不得不说的是香港有5部，而内地一部都没有。。。 4.3. 电影作品数量集中的年份接下来站在漫长的百年电影史的时间角度上，分析一下哪些年份”贡献了”最多的电影数量，也可以说是”电影大年”。1234567891011121314151617181920212223# 从日期中提取年份df['year'] = df['time'].map(lambda x:x.split('/')[0])# print(df.info())# print(df.head())# 统计各年上映的电影数量grouped_year = df.groupby('year')grouped_year_amount = grouped_year.year.count()top_year = grouped_year_amount.sort_values(ascending = False)# 绘图top_year.plot(kind = 'bar',color = 'orangered') #颜色设置为橙红色for x,y in enumerate(list(top_year.values)): plt.text(x,y+0.1,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('电影数量年份排名',color = colors1)plt.xlabel('年份(年)')plt.ylabel('数量(部)')plt.tight_layout()# plt.savefig('电影数量年份排名.png')plt.show() 结果如下图： 可以看到，100部电影来自37个年份。其中2011年上榜电影数量最多，达到9部；其次是前一年的7部。回忆一下，那会儿正是上大学的头两年，可怎么感觉除了阿凡达之外，没有什么其他有印象的电影了。。。另外，网上传的号称”电影史奇迹年”的1994年仅排名第6。这让我进一步对猫眼榜单的权威性产生了质疑。再往后看，发现遥远的1939和1940年也有电影上榜。那会儿应该还是黑白电影时代吧，看来电影的口碑好坏跟外在的技术没有绝对的关系，质量才是王道。 4.4 拥有电影作品数量最多的演员最后，看看前100部电影中哪些演员的作品数量最多。程序如下：1234567891011121314151617181920212223242526272829303132333435363738#表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中starlist = []star_total = df.starfor i in df.star.str.replace(' ','').str.split(','): starlist.extend(i) # print(starlist)# print(len(starlist))# set去除重复的演员名starall = set(starlist)# print(starall)# print(len(starall))starall2 = &#123;&#125;for i in starall: if starlist.count(i)&gt;1: # 筛选出电影数量超过1部的演员 starall2[i] = starlist.count(i)starall2 = sorted(starall2.items(),key = lambda starlist:starlist[1] ,reverse = True)starall2 = dict(starall2[:10]) #将元组转为字典格式# 绘图x_star = list(starall2.keys()) #x轴坐标y_star = list(starall2.values()) #y轴坐标plt.bar(range(10),y_star,tick_label = x_star)pl.xticks(rotation = 270)for x,y in enumerate(y_star): plt.text(x,y+0.1,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('演员电影作品数量排名',color = colors1)plt.xlabel('演员')plt.ylabel('数量(部)')plt.tight_layout()plt.show() # plt.savefig('演员电影作品数量排名.png') 结果如下图： 张国荣排在了第一位，这是之前没有猜到的。其次是梁朝伟和星爷，再之后是布拉德·皮特。惊奇地发现，前十名影星中，香港影星居然占了6位。有点严重怀疑这是不是香港版的top100电影。。。 对张国荣以7部影片的巨大优势雄霸榜单第一位感到好奇，想看看是哪7部电影。12345df['star1'] = df['star'].map(lambda x:x.split(',')[0]) #提取1号演员df['star2'] = df['star'].map(lambda x:x.split(',')[1]) #提取2号演员star_most = df[(df.star1 == '张国荣') | (df.star2 == '张国荣')][['star','name']].reset_index('index')# |表示两个条件或查询，之后重置索引print(star_most) 可以看到包括排名第1的”霸王别姬”、第17名的”春光乍泄”、第27名的”射雕英雄传之东成西就”等。突然发现，好像只看过”英雄本色”。。。有时间，去看看他其他的作品。 12345678 index star name0 1 张国荣,张丰毅,巩俐 霸王别姬1 17 张国荣,梁朝伟,张震 春光乍泄2 27 张国荣,梁朝伟,张学友 射雕英雄传之东成西就3 37 张国荣,梁朝伟,刘嘉玲 东邪西毒4 70 张国荣,王祖贤,午马 倩女幽魂5 99 张国荣,张曼玉,刘德华 阿飞正传6 100 狄龙,张国荣,周润发 英雄本色 由于数据量有限，故仅作了上述简要的分析。 5. 完整程序最后，将前面爬虫的所有代码整理一下，完整的代码如下：一、爬虫部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193import urllibimport requestsfrom requests.exceptions import RequestExceptionimport refrom bs4 import BeautifulSoupimport jsonimport timefrom lxml import etree# -----------------------------------------------------------------------------def get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; # 不加headers爬不了 response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except RequestException: return None# 1 用正则提取内容def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) # re.S表示匹配任意字符，如果不加.无法匹配换行符 items = re.findall(pattern, html) # print(items) for item in items: yield &#123; 'index': item[0], 'thumb': get_thumb(item[1]), 'name': item[2], 'star': item[3].strip()[3:], # 'time': item[4].strip()[5:], # 用函数分别提取time里的日期和地区 'time': get_release_time(item[4].strip()[5:]), 'area': get_release_area(item[4].strip()[5:]), 'score': item[5].strip() + item[6].strip() &#125; # 2 用lxml结合xpath提取内容def parse_one_page2(html): parse = etree.HTML(html) items = parse.xpath('//*[@id="app"]//div//dd') # 完整的是//*[@id="app"]/div/div/div[1]/dl/dd # print(type(items)) # *代表匹配所有节点，@表示属性 # 第一个电影是dd[1],要提取页面所有电影则去掉[1] # xpath://*[@id="app"]/div/div/div[1]/dl/dd[1] # lst = [] for item in items: yield&#123; 'index': item.xpath('./i/text()')[0], #./i/text()前面的点表示从items节点开始 #/text()提取文本 'thumb': get_thumb(str(item.xpath('./a/img[2]/@data-src')[0].strip())), # 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。 'name': item.xpath('./a/@title')[0], 'star': item.xpath('.//p[@class = "star"]/text()')[0].strip(), 'time': get_release_time(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'area': get_release_area(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'score' : item.xpath('.//p[@class = "score"]/i[1]/text()')[0] + \ item.xpath('.//p[@class = "score"]/i[2]/text()')[0] &#125; # 3 用beautifulsoup + css选择器提取def parse_one_page3(html): soup = BeautifulSoup(html, 'lxml') # print(content) # print(type(content)) # print('------------') items = range(10) for item in items: yield&#123; 'index': soup.select('dd i.board-index')[item].string, # iclass节点完整地为'board-index board-index-1',写board-inde即可 'thumb': get_thumb(soup.select('a &gt; img.board-img')[item]["data-src"]), # 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值 'name': soup.select('.name a')[item].string, 'star': soup.select('.star')[item].string.strip()[3:], 'time': get_release_time(soup.select('.releasetime')[item].string.strip()[5:]), 'area': get_release_area(soup.select('.releasetime')[item].string.strip()[5:]), 'score': soup.select('.integer')[item].string + soup.select('.fraction')[item].string &#125;# 4 用beautifulsoup + find_all提取def parse_one_page4(html): soup = BeautifulSoup(html,'lxml') items = range(10) for item in items: yield&#123; 'index': soup.find_all(class_='board-index')[item].string, 'thumb': soup.find_all(class_ = 'board-img')[item].attrs['data-src'], # 用.get('data-src')获取图片src链接，或者用attrs['data-src'] 'name': soup.find_all(name = 'p',attrs = &#123;'class' : 'name'&#125;)[item].string, 'star': soup.find_all(name = 'p',attrs = &#123;'class':'star'&#125;)[item].string.strip()[3:], 'time': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'area': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'score':soup.find_all(name = 'i',attrs = &#123;'class':'integer'&#125;)[item].string.strip() + soup.find_all(name = 'i',attrs = &#123;'class':'fraction'&#125;)[item].string.strip() &#125;# -----------------------------------------------------------------------------# 提取时间函数def get_release_time(data): pattern = re.compile(r'(.*?)(\(|$)') items = re.search(pattern, data) if items is None: return '未知' return items.group(1) # 返回匹配到的第一个括号(.*?)中结果即时间# 提取国家/地区函数def get_release_area(data): pattern = re.compile(r'.*\((.*)\)') # $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?) items = re.search(pattern, data) if items is None: return '未知' return items.group(1)# 获取封面大图# http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c# 去掉@160w_220h_1e_1c就是大图def get_thumb(url): pattern = re.compile(r'(.*?)@.*?') thumb = re.search(pattern, url) return thumb.group(1)# 数据存储到csvdef write_to_file3(item): with open('猫眼top100.csv', 'a', encoding='utf_8_sig',newline='') as f: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] w = csv.DictWriter(f,fieldnames = fieldnames) # w.writeheader() w.writerow(item)# 封面下载def download_thumb(name, url,num): try: response = requests.get(url) with open('封面图/' + name + '.jpg', 'wb') as f: f.write(response.content) print('第%s部电影封面下载完毕' %num) print('------') except RequestException as e: print(e) pass # 存储格式是wb,因为图片是二进制数格式，不能用w，否则会报错# -----------------------------------------------------------------------------def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) # print(html) # parse_one_page2(html) for item in parse_one_page(html): # 切换内容提取方法 print(item) write_to_file(item) # 下载封面图 download_thumb(item['name'], item['thumb'],item['index'])# if __name__ == '__main__':# for i in range(10):# main(i * 10) # time.sleep(0.5) # 猫眼增加了反爬虫，设置0.5s的延迟时间# 2 使用多进程提升抓取效率from multiprocessing import Poolif __name__ == '__main__': pool = Pool() pool.map(main, [i * 10 for i in range(10)]) 二、可视化部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#-*- coding: utf-8 -*-# 可视化分析# -------------------------------import pandas as pdimport matplotlib.pyplot as pltimport pylab as pl # 用于修改x轴坐标plt.style.use('ggplot') # 默认绘图风格很难看，替换为好看的ggplot风格fig = plt.figure(figsize=(8, 5)) # 设置图片大小colors1 = '#6D6D6D' # 设置图表title、text标注的颜色columns = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] # 设置表头df = pd.read_csv('maoyan_top100.csv', encoding="utf-8", header=None, names=columns, index_col='index') # 打开表格# index_col = 'index' 将索引设为index# 1电影评分最高top10def annalysis_1(): df_score = df.sort_values('score', ascending=False) # 按得分降序排列 name1 = df_score.name[:10] # x轴坐标 score1 = df_score.score[:10] # y轴坐标 plt.bar(range(10), score1, tick_label=name1) # 绘制条形图，用range()能搞保持x轴正确顺序 plt.ylim((9, 9.8)) # 设置纵坐标轴范围 plt.title('电影评分最高top10', color=colors1) # 标题 plt.xlabel('电影名称') # x轴标题 plt.ylabel('评分') # y轴标题 # 为每个条形图添加数值标签 for x, y in enumerate(list(score1)): plt.text(x, y + 0.01, '%s' % round(y, 1), ha='center', color=colors1) pl.xticks(rotation=270) # x轴名称太长发生重叠，旋转为纵向显示 plt.tight_layout() # 自动控制空白边缘，以全部显示x轴名称 # plt.savefig('电影评分最高top10.png') #保存图片 plt.show()# ------------------------------# 2各国家的电影数量比较def annalysis_2(): area_count = df.groupby( by='area').area.count().sort_values(ascending=False) # 绘图方法1 area_count.plot.bar(color='#4652B1') # 设置为蓝紫色 pl.xticks(rotation=0) # x轴名称太长重叠，旋转为纵向 # 绘图方法2 # plt.bar(range(11),area_count.values,tick_label = area_count.index,color # = '#4652B1') for x, y in enumerate(list(area_count.values)): plt.text(x, y + 0.5, '%s' % round(y, 1), ha='center', color=colors1) plt.title('各国/地区电影数量排名', color=colors1) plt.xlabel('国家/地区') plt.ylabel('数量(部)') plt.show()# plt.savefig('各国(地区)电影数量排名.png')# ------------------------------# 3电影作品数量集中的年份# 从日期中提取年份def annalysis_3(): df['year'] = df['time'].map(lambda x: x.split('/')[0]) # print(df.info()) # print(df.head()) # 统计各年上映的电影数量 grouped_year = df.groupby('year') grouped_year_amount = grouped_year.year.count() top_year = grouped_year_amount.sort_values(ascending=False) # 绘图 top_year.plot(kind='bar', color='orangered') # 颜色设置为橙红色 for x, y in enumerate(list(top_year.values)): plt.text(x, y + 0.1, '%s' % round(y, 1), ha='center', color=colors1) plt.title('电影数量年份排名', color=colors1) plt.xlabel('年份(年)') plt.ylabel('数量(部)') plt.tight_layout() # plt.savefig('电影数量年份排名.png') plt.show()# ------------------------------# 4拥有电影作品数量最多的演员# 表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中def annalysis_4(): starlist = [] star_total = df.star for i in df.star.str.replace(' ', '').str.split(','): starlist.extend(i) # print(starlist) # print(len(starlist)) # set去除重复的演员名 starall = set(starlist) # print(starall) # print(len(starall)) starall2 = &#123;&#125; for i in starall: if starlist.count(i) &gt; 1: # 筛选出电影数量超过1部的演员 starall2[i] = starlist.count(i) starall2 = sorted(starall2.items(), key=lambda starlist: starlist[1], reverse=True) starall2 = dict(starall2[:10]) # 将元组转为字典格式 # 绘图 x_star = list(starall2.keys()) # x轴坐标 y_star = list(starall2.values()) # y轴坐标 plt.bar(range(10), y_star, tick_label=x_star) pl.xticks(rotation=270) for x, y in enumerate(y_star): plt.text(x, y + 0.1, '%s' % round(y, 1), ha='center', color=colors1) plt.title('演员电影作品数量排名', color=colors1) plt.xlabel('演员') plt.ylabel('数量(部)') plt.tight_layout() plt.show()# plt.savefig('演员电影作品数量排名.png')def main(): annalysis_1() annalysis_2() annalysis_3() annalysis_4()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>requests</tag>
        <tag>正则表达式</tag>
        <tag>beautifulsoup</tag>
        <tag>css</tag>
        <tag>xpath</tag>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4块钱,用Github+Hexo搭建你的个人博客：美化篇]]></title>
    <url>%2Fhexo02.html</url>
    <content type="text"><![CDATA[上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。 博客界面的美化需要修改源代码，对于没有前端基础的人来说，会比较费时间精力。即使一个小小的细节调整，可能就需要好一会儿，所以，需保持耐心。这里，以我的博客为例，介绍一些比较实用的元素美化操作。 1 选择新的模板初始的博客界面使用的是landscape主题模板，非常难看。你需重新挑选一个好看的主题模板，然后在此基础上进行美化。 在哪里找主题?到这个hexo主题排名网站 可以看到，排名第一的是一款叫next的主题，那么就可以尝试用它作为博客主题。事实上，这款主题功能相当齐全，我使用的即是这款。 进入到这个主题，阅读README.md说明，可以详细了解这款主题如何使用，还可以查看一些使用了这款主题的博客。 如何应用到自己的博客？ 打开博客根目录下的themes文件夹，Git软件运行下述命令： git clone https://github.com/iissnan/hexo-theme-next themes/next 就可以把这款主题的安装文件下载到电脑中。然后还打开D:\blog\_config.yml文件，找到 theme 字段，字段，并将其值更改为 hexo-theme-next（注意冒号后要留一个空格）。这样，博客主题就切换为next主题了，下面进行一下验证。 验证博客主题 在博客根目录下，运行下述命令： hexo clean hexo s -g 使用浏览器访问 http://localhost:4000,可以看到，你的博客已经完全变样，至少非常清爽了。 这是 NexT 默认的 scheme —— Muse。NexT 一共有四个 scheme,分别是： Muse: 默认 scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist: Muse 的紧凑版本，整洁有序的单栏外观 Pisces: 双栏 Scheme，小家碧玉似的清新 Gemini: 左侧网站信息及目录，块+片段结构布局 我选用的是Pisces，下面以我的博客作为参照，介绍如何进行美化。 声明两个重要文件Hexo 中有两份主要的配置文件，其名称都是 _config.yml， 后续美化主要需要在这两个文件中进行内容修改。 站点配置文件，位于站点文件夹根目录内，主要包含 Hexo 本身的配置： /blog/_config.yml 主题配置文件，位于主题文件夹根目录内，主要用于配置主题相关的选项: ~/blog/themes/next/_config.yml 为了描述方便，在以下说明中，将前者称为 站点配置文件， 后者称为 主题配置文件。 2 布局修改设置将博客]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4块钱,用Github+Hexo搭建你的个人博客：搭建篇]]></title>
    <url>%2Fhexo01.html</url>
    <content type="text"><![CDATA[4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。 【更新于2018/7/14】 摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。好在参考了很多大佬的教程后顺利搭建完成，但过程中还是踩了一些坑。这里及时进行总结，作为博客的第一篇文章。 一、前言1 网上有很多现成的博客不用，为什么要自己搭建?可能有人会说：很多网站都能写博客，而且博客现在其实都有点过时了，为什么还要自己去搞？ 这里我说一下我想自己搭建的两点原因：一、网上的多数博客大家都共用一套相同的模板界面，没有特点、界面也杂乱充斥着很多不相关的东西，不论是自己写还是给人看，体验都不好。二、拥有一个你自己可以起名字的博客网站，里面的任何内容完全由你自己决定，这感觉是件很酷的事。 这些普通的网站博客和一些个人博客，哪个好看，高下立判吧。 vs &nbsp; vs 更多个人博客：litten &nbsp; http://litten.me/Ryan &nbsp; http://ryane.top/liyin &nbsp; https://liyin.date/reuixiy &nbsp; https://reuixiy.github.io/Tranquilpeak &nbsp; https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/ 2 搭建博客难不难？我之前认为搭建博客是一件只有能程序猿才能做出来的高大上的活。其实，只要跟着网上的教程一步步做下去，一个小时不到就可以搭建好你自己的个人博客。所以，搭建博客其实很简单。只不过，如果你想把博客做得好看一些的话，才会花费一些精力。 二、开始搭建博客如果看到上面那些精美的博客，你已经心动了，那就开始动手吧。下面正式开始博客搭建步骤。 搭建教程参考搭建博客的教程网上一搜一大堆，为了节省你的搜索时间，这里我筛选出了下面几篇很棒的教程，我基本都是跟着一步步做下来的。 小白独立搭建博客 2018，你该搭建自己的博客了！ 手把手教你用Hexo+Github 搭建属于自己的博客 操作平台:Win7 64位。 相关名词解释：Hexo：一种常用的博客框架，有了它建立博客非常简单。你可以认为它是一种博客模板，只不过它比普通网站的那种博客模板要好看地多，并且有很高度的自定义性，只要你愿意，你可以建立一个独一无二的博客来。若想详细了解Hexo的使用，移步 Hexo官方网站 https://hexo.io/zh-cn/docs/。 Github：一个全世界程序猿聚集的知名网站。免费的远程仓库和开源协作社区。我们需要利用网站里的Github Pages功能来托管需发布到网上的博客的相关文件和代码。 Git： 一种版本控制系统。我们在自己的本地电脑写博客，如何把博客同步到Github，然后发布到网上去？就需要用这个软件去写几行代码然后就能搞定，后期用的最多的就是它。 Node.js： 提供JavaScript的开发环境，安装好以后就不用跟它再打交道，所以不用太关注它。 1 软件安装配置搭建博客需要先下载2个软件：Git和Nodejs。软件安装过程很简单，一直点击Next默认直到安装完成就行了。 Git官网：https://git-scm.com/download/win安装完，打开cmd窗口运行下面命令，如果有返回版本信息说明安装成功。 git –version Nodejs官网：https://nodejs.org/en/download/同样，安装完有返回版本信息说明安装成功，见下图。 node -v npm -v 至此，软件安装步骤完成。 2 安装Hexo博客框架 安装hexo 这里开始就要用到使用频率最高的Git软件了。 桌面右键点击git bash here选项，会打开Git软件界面，输入下面每行命令并回车： 12npm install hexo-cli -gnpm install hexo-deployer-git --save 第一句是安装hexo，第二句是安装hexo部署到git page的deployer。代码命令看不懂没关系，一是这些命令之后几乎不再用到，二是用多了你会慢慢记住。 设置博客存放文件夹 你需要预想一下你要把博客文件新建在哪个盘下的文件夹里，除了c盘都可以，例如d盘根目录下的blog文件夹，紧接着在桌面输入下面命令并回车： hexo init /d/blog cd /d/blog npm install *注：/d/bog可以更改为你自己的文件夹* 有的教程是先新建博客文件夹，在该文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，再执行以下操作。但是我操作过程中出现过这样的失败提示：hexo:conmand not found，但我执行上面的命令时就没有出现该问题。 hexo init npm install 查看博客效果 至此，博客初步搭建好，输入下面一行本地部署生成的命令： hexo s -g 然后打开浏览器在网址栏输入：localhost:4000就可以看到博客的样子，如果无法打开，则继续输入下面命令： npm install hexo-deployer-git --save hexo clean hexo s -g 打开该网址，你可以看到第一篇默认的博客：Hello World。但看起来很难看，后续会通过重新选择模板来对博客进行美化。 现在你就可以开始写博客了，但是博客只能在你自己的电脑上看到，别人无法在网上看到你的博客。接下来需要利用前面提到的的Github Pages功能进行设置，设置完成之后别人通过搜索就可以看到你的博客。 3 把你的博客部署到Github Pages上去这是搭建博客相对比较复杂也是容易出错的一部分。 1. Github账号注册及配置 如果你没有github帐号，就新建一个，然后去邮箱进行验证；如果你有帐号则直接登录。官网：https://github.com/ 配置步骤： 建立new repository 只填写username.github.io即可，然后点击create repositrory。注意：username.github.io 的username要和用户名保持一致，不然后面会失败。以我的为例： 开启gh-pages功能 点击github主页点击头像下面的profile,找到新建立的username.github.io文件打开，点击settings，往下拉动鼠标到GitHub Pages。如果你看到上方出现以下警告： GitHub Pages is currently disabled. You must first add content to your repository before you can publish a GitHub Pages site 不用管他，点击选择choose a theme，随便选择一个，（之后我们要更改这些丑陋的模板），然后select theme保存就行了。 接下来的几个步骤参考教程1即可。 主要步骤包括： git创建SSH密钥 在GitHub账户中添加你的公钥 测试成功并设置用户信息 将本地的Hexo文件更新到Github库中 hexo部署更新博客 经过以上几步的操作，顺利的话，你的博客可以发布到网上，其他人也可以通过你的网址username.github.io（我的是makcyun.github.io）访问到你的博客。 4 赶紧新建个博客试试接下来你可以自己新建一个文档来写下你的第一篇博客并在网页上测试。 同样在根目录D:\blog中运行下面命令： hexo new 第一篇博客 *注：第一篇博客名称可以随便修改* 然后打开D:\blog\source\_posts文件夹，就可以看到一个第一篇博客.md的文件。用支持markdown语法的软件打开该文件进行编辑即可。 编辑好以后，运行下述命令： hexo clean hexo d -g 然后，在网址中输入username.github.io即可看到你的博客上，出现第一篇博客这篇新的文章。 至此，你的个人博客初步搭建过程就完成了。 但是，现在还存在两个问题你可能想解决： markdown语法是什么，如何用软件编写博客？ 网址是username.github.io，感觉很奇怪，而我的博客网址怎么是www开头的？ 好，下面来讲解一下。 第一个问题 关于markdown语法介绍：markdown——入门指南 当你大致了解markdown语法后，如何用markdown写博客呢？不妨参考这两篇详细教程： Markdown語法說明HEXO下的Markdown语法(GFM)写博客 接下来你要一个可以写markdown语法的软件，这里推荐两款软件。 Windows下使用Markdown Pad2, Mac下使用Mou。 我用的是Markdown Pad2，这款软件是付费软件，但网上有很多破解版，我这里将软件上传到了百度网盘，如需请取。MarkdownPad2： https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA 密码：y9zh 安装好后，就可以打开刚才的第一篇博客.md，开始尝试写你的第一篇博客了。 比如这是我用markdownpad写的博客原稿。 可以看到文档里面都是字符，没有图片这些。所以只需要用键盘专注于打字就行了，不需要像word那么复杂，还要用鼠标插入标题样式、图片这些操作。 第二个问题 我的网址不是默认的username.github.io，是因为我购买了一个域名，然后和username.github.io进行了关联，这样我的博客网址变成了我的域名。 在哪里购买域名呢？首推去 阿里云官网 购买。 你可以随意起你喜欢的名字，然后在该网站进行搜索，没有人占用的话你就可以购买该域名。不同的后缀价格不同。可以看到.com、 .net等会比较贵，最便宜的这两年新出的.top域名，只要4块钱一年，我购买的就是这种。 购买完域名以后，需要做以下几个步骤： 实名认证 修改DNS 域名解析 新建CNAME文件 1 实名认证在修改DNS之前，必须要阿里云官网实名认证成功，用淘宝账户登录然后填写相关信息即可。 2 修改DNS实名认证成功后，进入管理界面，依次点击： 修改DNS为：f1g1ns1.dnspod.netf1g1ns2.dnspod.net 3 域名解析DNS修改好以后，到DNSPOD这个网站去解析你的域名。 首先，微信登录并注册 https://www.dnspod.cn/，点击域名解析，添加上你的域名。 接着，添加以下两条记录即可。 注意：makcyun.github.io.需换成你自己的名称，另外最后有一个“.” 4 新建CNAME文件在博客根目录文件夹下,例如我的D:\blog\source，新建名为CNAME的记事本文件，去掉后缀。在里面输入你的域名，例如我的：www.makcyun.top即可，保存并关闭。 注意： 这里填不填写www前缀都是可以的，区别在于填写www，那么博客网址就会以www开头，例如：www.makcyun.top；如果不填写，博客网址是：makcyun.top，二者都可以，看你喜欢。 完成以上4步之后，根目录下再次运行： hexo d -g 这时，输入你在记事本里的域名网址，即可打开你的博客。至此，你的博客就换成了你想要的网址，别人也可以通过这个网址访问到你的博客。 到这里，博客的初步搭建就算完成了，顺利的话不到1小时就能完成，如果中间出现差错，保持耐心多试几次应该就没问题。 此时，还有一个比较重要的问题就是，你可能会觉得你的博客不够美观，不如我博客里前面提到的那几个博客，甚至也没有我的好看。 如果你还愿意折腾的话，下一篇文章，我会以我的博客为例，讲一讲如何进行博客的美化。]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
