<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  
<meta name="google-site-verification" content="s0oCkquJSvsBetEUl3d8nDj5jYzNitxDJALA37MiIyM" />


<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Python爬虫" />





  <link rel="alternate" href="/atom.xml" title="高级农民工" type="application/atom+xml" />






<meta name="description" content="数据抓取、清洗、分析一条龙 数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。">
<meta name="keywords" content="Python爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="pyspider 爬取并分析虎嗅网 5 万篇文章">
<meta property="og:url" content="https://www.makcyun.top/web_scraping_withpython9.html">
<meta property="og:site_name" content="高级农民工">
<meta property="og:description" content="数据抓取、清洗、分析一条龙 数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-4/22792947.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-4/38979674.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-5/33467436.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-5/80730129.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-5/65831925.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-5/51865737.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-5/80480010.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-5/13729703.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-6/80607479.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-6/36910200.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-6/47981964.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-8/86110562.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-8/9931236.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-7/59717401.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-6/85687626.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-6/93399033.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-8/24683879.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/18-11-7/78186420.jpg">
<meta property="og:image" content="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg">
<meta property="og:updated_time" content="2018-11-08T07:26:05.243Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pyspider 爬取并分析虎嗅网 5 万篇文章">
<meta name="twitter:description" content="数据抓取、清洗、分析一条龙 数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。">
<meta name="twitter:image" content="http://pbscl931v.bkt.clouddn.com/18-11-4/22792947.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.makcyun.top/web_scraping_withpython9.html"/>





  <title>pyspider 爬取并分析虎嗅网 5 万篇文章 | 高级农民工</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <meta name="baidu-site-verification" content="E65frtf6P6" />
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">高级农民工</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Beginner's Mind</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首&emsp;&emsp;页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于博主
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归&emsp;&emsp;档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标&emsp;&emsp;签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分&emsp;&emsp;类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-top">
          <a href="/top/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-signal"></i> <br />
            
            最受欢迎
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            站内搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.makcyun.top/web_scraping_withpython9.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="高级农民工">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://pbscl931v.bkt.clouddn.com/18-8-3/40685653.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="高级农民工">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">pyspider 爬取并分析虎嗅网 5 万篇文章</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T16:16:24+08:00">
                2018-11-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python爬虫/" itemprop="url" rel="index">
                    <span itemprop="name">Python爬虫</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/web_scraping_withpython9.html" class="leancloud_visitors" data-flag-title="pyspider 爬取并分析虎嗅网 5 万篇文章">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">热度&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>℃</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9,049 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  36 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>数据抓取、清洗、分析一条龙</p>
<p>数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。</p>
<a id="more"></a>  
<p><strong>摘要：</strong> 不少时候，一篇文章能否得到广泛的传播，除了文章本身实打实的质量以外，一个好的标题也至关重要。本文爬取了虎嗅网建站至今共 5 万条新闻标题内容，助你找到起文章标题的技巧与灵感。同时，分享一些值得关注的文章和作者。</p>
<h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1. 分析背景"></a>1. 分析背景</h2><h3 id="1-1-为什么选择虎嗅"><a href="#1-1-为什么选择虎嗅" class="headerlink" title="1.1. 为什么选择虎嗅"></a>1.1. 为什么选择虎嗅</h3><p>在众多新媒体网站中，「虎嗅」网的文章内容和质量还算不错。在「新榜」科技类公众号排名中，它位居榜单第 3 名，还是比较受欢迎的。所以选择爬取该网站的文章信息，顺便从中了解一下这几年科技互联网都出现了哪些热点信息。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-4/22792947.jpg" alt=""></p>
<blockquote>
<p>「关于虎嗅」</p>
<p>虎嗅网创办于 2012 年 5 月，是一个聚合优质创新信息与人群的新媒体平台。该平台专注于贡献原创、深度、犀利优质的商业资讯，围绕创新创业的观点进行剖析与交流。虎嗅网的核心，是关注互联网及传统产业的融合、明星公司的起落轨迹、产业潮汐的动力与趋势。</p>
</blockquote>
<h3 id="1-2-分析内容"><a href="#1-2-分析内容" class="headerlink" title="1.2. 分析内容"></a>1.2. 分析内容</h3><ul>
<li>分析虎嗅网 5 万篇文章的基本情况，包括收藏数、评论数等</li>
<li>发掘最受欢迎和最不受欢迎的文章及作者</li>
<li>分析文章标题形式（长度、句式）与受欢迎程度之间的关系</li>
<li>展现近些年科技互联网行业的热门词汇</li>
</ul>
<h3 id="1-3-分析工具"><a href="#1-3-分析工具" class="headerlink" title="1.3. 分析工具"></a>1.3. 分析工具</h3><ul>
<li>Python</li>
<li>pyspider </li>
<li>MongoDB</li>
<li>Matplotlib</li>
<li>WordCloud</li>
<li>Jieba</li>
</ul>
<h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2. 数据抓取"></a>2. 数据抓取</h2><p>使用 pyspider 抓取了虎嗅网的主页文章，文章抓取时期为 2012 年建站至 2018 年 11 月 1 日，共计约 5 万篇文章。抓取 了 7 个字段信息：文章标题、作者、发文时间、评论数、收藏数、摘要和文章链接。</p>
<h3 id="2-1-目标网站分析"><a href="#2-1-目标网站分析" class="headerlink" title="2.1. 目标网站分析"></a>2.1. 目标网站分析</h3><p>这是要爬取的 <a href="https://www.huxiu.com/" target="_blank" rel="noopener">网页界面</a>，可以看到是通过 AJAX 加载的。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-4/38979674.jpg" alt=""></p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-5/33467436.jpg" alt=""></p>
<p>右键打开开发者工具查看翻页规律，可以看到 URL 请求是 POST 类型，下拉到底部查看 Form Data，表单需提交参数只有 3 项。经尝试， 只提交 page 参数就能成功获取页面的信息，其他两项参数无关紧要，所以构造分页爬取非常简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huxiu_hash_code: <span class="number">39</span>bcd9c3fe9bc69a6b682343ee3f024a</span><br><span class="line">page: <span class="number">4</span></span><br><span class="line">last_dateline: <span class="number">1541123160</span></span><br></pre></td></tr></table></figure>
<p>接着，切换选项卡到 Preview 和 Response 查看网页内容，可以看到数据都位于 data 字段里。total_page 为 2004，表示一共有 2004 页的文章内容，每一页有 25 篇文章，总共约 5 万篇，也就是我们要爬取的数量。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-5/80730129.jpg" alt=""></p>
<p>以上，我们就找到了所需内容，接下来可以开始构造爬虫，整个爬取思路比较简单。之前我们也练习过这一类 Ajax 文章的爬取，可以参考：</p>
<p><a href="https://www.makcyun.top/web_scraping_withpython4.html">抓取澎湃网建站至今 1500 期信息图栏目图片</a></p>
<h3 id="2-2-pyspider-介绍"><a href="#2-2-pyspider-介绍" class="headerlink" title="2.2. pyspider 介绍"></a>2.2. pyspider 介绍</h3><p>和之前文章不同的是，这里我们使用一种新的工具来进行爬取，叫做：pyspider 框架。由国人 binux 大神开发，GitHub Star 数超过 12 K，足以证明它的知名度。可以说，学习爬虫不能不会使用这个框架。</p>
<p>网上关于这个框架的介绍和实操案例非常多，这里仅简单介绍一下。</p>
<p>我们之前的爬虫都是在 Sublime 、PyCharm 这种 IDE 窗口中执行的，整个爬取过程可以说是处在黑箱中，内部运行的些细节并不太清楚。而 pyspider 一大亮点就在于提供了一个可视化的 WebUI 界面，能够清楚地查看爬虫的运行情况。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-5/65831925.jpg" alt=""></p>
<p>pyspider 的架构主要分为 Scheduler(调度器)、Fetcher(抓取器)、Processer(处理器)三个部分。Monitor(监控器)对整个爬取过程进行监控，Result Worker(结果处理器)处理最后抓取的结果。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-5/51865737.jpg" alt=""></p>
<p>我们看看该框架的运行流程大致是怎么样的：</p>
<ul>
<li>一个 pyppider 爬虫项目对应一个 Python 脚本，脚本里定义了一个 Handler 主类。爬取时首先调用 on_start() 方法生成最初的抓取任务，然后发送给 Scheduler。</li>
<li>Scheduler 将抓取任务分发给 Fetcher 进行抓取，Fetcher 执行然后得到 Response、随后将 Response 发送给 Processer。</li>
<li>Processer 处理响应并提取出新的 URL 然后生成新的抓取任务，然后通过消息队列的方式通知 Scheduler 当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待 Result Worker 处理。</li>
<li>Scheduler 接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回 Fetcher 进行抓取。</li>
<li>不断重复以上工作、直到所有的任务都执行完毕，抓取结束。</li>
<li>抓取结束后、程序会回调 on_finished() 方法，这里可以定义后处理过程。</li>
</ul>
<p>该框架比较容易上手，网页右边是代码区，先定义类（Class）然后在里面添加爬虫的各种方法（也可以称为函数），运行的过程会在左上方显示，左下方则是输出结果的区域。</p>
<p>这里，分享几个不错的教程以供参考：</p>
<p>GitHub 项目地址：<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a></p>
<p>官方主页：<a href="http://docs.pyspider.org/en/latest/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/</a></p>
<p>pyspider 中文网：<a href="http://www.pyspider.cn/page/1.html" target="_blank" rel="noopener">http://www.pyspider.cn/page/1.html</a></p>
<p>pyspider 爬虫原理剖析：<a href="http://python.jobbole.com/81109/" target="_blank" rel="noopener">http://python.jobbole.com/81109/</a></p>
<p>pyspider 爬淘宝图案例实操：<a href="https://cuiqingcai.com/2652.html" target="_blank" rel="noopener">https://cuiqingcai.com/2652.html</a></p>
<p>安装好该框架后，下面我们可以就开始爬取了。</p>
<h3 id="2-3-抓取数据"><a href="#2-3-抓取数据" class="headerlink" title="2.3. 抓取数据"></a>2.3. 抓取数据</h3><p>CMD 命令窗口执行：pyspider all 命令，然后浏览器输入：<a href="http://localhost:5000/" target="_blank" rel="noopener">http://localhost:5000/</a> 就可以启动 pyspider 。</p>
<p>点击 Create 新建一个项目，Project Name 命名为：huxiu，因为要爬取的 URL 是 POST 类型，所以这里可以先不填写，之后可以在代码中添加，再次点击 Creat 便完成了该项目的新建。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-5/80480010.jpg" alt=""></p>
<p>新项目建立好后会自动生成一部分模板代码，我们只需在此基础上进行修改和完善，然后就可以运行爬虫项目了。现在，简单梳理下代码编写步骤。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-5/13729703.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config:&#123;</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span></span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">3</span>): <span class="comment"># 先循环1页</span></span><br><span class="line">            print(<span class="string">'正在爬取第 %s 页'</span> % page)</span><br><span class="line">            self.crawl(<span class="string">'https://www.huxiu.com/v2_action/article_list'</span>,method=<span class="string">'POST'</span>,data=&#123;<span class="string">'page'</span>:page&#125;, callback=self.index_page)</span><br></pre></td></tr></table></figure>
<p>这里，首先定义了一个 Handler 主类，整个爬虫项目都主要在该类下完成。 接着，可以将爬虫基本的一些基本配置，比如 Headers、代理等设置写在下面的 crawl_config 属性中。（如果你还没有习惯从函数（def）转换到类（Class）的代码写法，那么需要先了解一下类的相关知识，之后我也会单独用一篇文章介绍一下。）</p>
<p>下面的 on_start()  方法是程序的入口，也就是说程序启动后会首先从这里开始运行。首先，我们将要爬取的 URL传入 crawl() 方法，同时将 URL 修改成虎嗅网的：<a href="https://www.huxiu.com/v2_action/article_list。由于" target="_blank" rel="noopener">https://www.huxiu.com/v2_action/article_list。由于</a> URL 是 POST 请求，所以我们还需要增加两个参数：method 和 data。method 表示 HTTP 请求方式，默认是 GET，这里我们需要设置为 POST；data 是 POST 请求表单参数，只需要添加一个 page 参数即可。</p>
<p>接着，通过 callback 参数定义一个 index_page() 方法，用来解析 crawl() 方法爬取 URL 成功后返回的 Response 响应。在后面的 index_page() 方法中，可以使用 PyQuery 提取响应中的所需内容。具体提取方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        content = response.json[<span class="string">'data'</span>]</span><br><span class="line">        <span class="comment"># 注意，在sublime中，json后面需要添加()，pyspider 中则不用</span></span><br><span class="line">        doc = pq(content)</span><br><span class="line">        lis = doc(<span class="string">'.mod-art'</span>).items()</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'title'</span>: item(<span class="string">'.msubstr-row2'</span>).text(),</span><br><span class="line">            <span class="string">'url'</span>:<span class="string">'https://www.huxiu.com'</span>+ str(item(<span class="string">'.msubstr-row2'</span>).attr(<span class="string">'href'</span>)),</span><br><span class="line">            <span class="string">'name'</span>: item(<span class="string">'.author-name'</span>).text(),</span><br><span class="line">            <span class="string">'write_time'</span>:item(<span class="string">'.time'</span>).text(),</span><br><span class="line">            <span class="string">'comment'</span>:item(<span class="string">'.icon-cmt+ em'</span>).text(),</span><br><span class="line">            <span class="string">'favorites'</span>:item(<span class="string">'.icon-fvr+ em'</span>).text(),</span><br><span class="line">            <span class="string">'abstract'</span>:item(<span class="string">'.mob-sub'</span>).text()</span><br><span class="line">            &#125; <span class="keyword">for</span> item <span class="keyword">in</span> lis ]   <span class="comment"># 列表生成式结果返回每页提取出25条字典信息构成的list</span></span><br><span class="line">        print(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
<p>这里，网页返回的 Response 是 json 格式，待提取的信息存放在其中的 data 键值中，由一段 HTML 代码构成。我们可以使用 response.json[‘data’] 获取该 HTML 信息，接着使用 PyQuery 搭配 CSS 语法提取出文章标题、链接、作者等所需信息。这里使用了列表生成式，能够精简代码并且转换为方便的 list 格式，便于后续存储到 MongoDB 中。我们输出并查看一下第 2 页的提取结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由25个 dict 构成的 list</span></span><br><span class="line">[&#123;<span class="string">'title'</span>: <span class="string">'想要长生不老？杀死体内的“僵尸细胞”吧'</span>, <span class="string">'url'</span>: <span class="string">'https://www.huxiu.com/article/270086.html'</span>, <span class="string">'name'</span>: <span class="string">'造就Talk'</span>, <span class="string">'write_time'</span>: <span class="string">'19小时前'</span>, <span class="string">'comment'</span>: <span class="string">'4'</span>, <span class="string">'favorites'</span>: <span class="string">'28'</span>, <span class="string">'abstract'</span>: <span class="string">'如果有了最终疗法，也不应该是每天都需要接受治疗'</span>&#125;, </span><br><span class="line"> &#123;<span class="string">'title'</span>: <span class="string">'日本步入下流社会，我们还在买买买'</span>, <span class="string">'url'</span>: <span class="string">'https://www.huxiu.com/article/270112.html'</span>, <span class="string">'name'</span>: <span class="string">'腾讯《大家》©'</span>, <span class="string">'write_time'</span>: <span class="string">'20小时前'</span>, <span class="string">'comment'</span>: <span class="string">'13'</span>, <span class="string">'favorites'</span>: <span class="string">'142'</span>, <span class="string">'abstract'</span>: <span class="string">'我买，故我在'</span>&#125;</span><br><span class="line">...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>可以看到，成功得到所需数据，然后就可以保存了，可以选择输出为 CSV、MySQL、MongoDB 等方式，这里我们选择保存到  MongoDB 中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>,<span class="number">27017</span>)</span><br><span class="line">db = client.Huxiu</span><br><span class="line">mongo_collection = db.huxiu_news</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_result</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            self.save_to_mongo(result)  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self,result)</span>:</span></span><br><span class="line">    df = pd.DataFrame(result)</span><br><span class="line">    <span class="comment">#print(df)</span></span><br><span class="line">    content = json.loads(df.T.to_json()).values()</span><br><span class="line">    <span class="keyword">if</span> mongo_collection.insert_many(content):</span><br><span class="line">        print(<span class="string">'存储到 mongondb 成功'</span>)</span><br><span class="line">        <span class="comment"># 随机暂停</span></span><br><span class="line">        sleep = np.random.randint(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">        time.sleep(sleep)</span><br></pre></td></tr></table></figure>
<p>上面，定义了一个 on_result() 方法，该方法专门用来获取 return 的结果数据。这里用来接收上面 index_page() 返回的 data 数据，在该方法里再定义一个存储到 MongoDB 的方法就可以保存到 MongoDB 中。关于数据如何存储到 MongoDB 中，我们在之前的 <a href="https://www.makcyun.top/web_scraping_withpython7.html">一篇文章</a> 中有过介绍，如果忘记了可以回顾一下。</p>
<p>下面，我们来测试一下整个爬取和存储过程。点击左上角的 run 就可以顺利运行单个网页的抓取、解析和存储，结果如下：</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-6/80607479.jpg" alt=""></p>
<p>上面完成了单页面的爬取，接下来，我们需要爬取全部 2000 余页内容。</p>
<p>需要修改两个地方，首先在 on_start() 方法中将 for 循环页数 3 改为 2002。改好以后，如果我们直接点击 run ，会发现还是只能爬取第 2 页的结果。这是因为，pyspider 以 URL的 MD5 值作为 唯一 ID 编号，ID 编号相同的话就视为同一个任务，便不会再重复爬取。由于 GET 请求的 分页URL 通常是有差异的，所以 ID 编号会不同，也就自然能够爬取多页。但这里 POST 请求的分页 URL 是相同的，所以爬完第 2 页，后面的页数便不会再爬取。</p>
<p>那有没有解决办法呢？ 当然是有的，我们需要重新写下 ID 编号的生成方式，方法很简单，在 on_start() 方法前面添加下面 2 行代码即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self,task)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> md5string(task[<span class="string">'url'</span>]+json.dumps(task[<span class="string">'fetch'</span>].get(<span class="string">'data'</span>,<span class="string">''</span>)))</span><br></pre></td></tr></table></figure>
<p>这样，我们再点击 run 就能够顺利爬取 2000 页的结果了，我这里一共抓取了 49,996 条结果，耗时 2 小时左右完成。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-6/36910200.jpg" alt=""></p>
<p>以上，就完成了数据的获取。有了数据我们就可以着手分析，不过这之前还需简单地进行一下数据的清洗、处理。</p>
<h2 id="3-数据清洗处理"><a href="#3-数据清洗处理" class="headerlink" title="3. 数据清洗处理"></a>3. 数据清洗处理</h2><p>首先，我们需要从 MongoDB 中读取数据，并转换为 DataFrame。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'Huxiu'</span>]</span><br><span class="line">collection = db[<span class="string">'huxiu_news'</span>]</span><br><span class="line"><span class="comment"># 将数据库数据转为DataFrame</span></span><br><span class="line">data = pd.DataFrame(list(collection.find()))</span><br></pre></td></tr></table></figure>
<p>下面我们看一下数据的总体情况，可以看到数据的维度是 49996 行 × 8 列。发现多了一列无用的 _id 需删除，同时 name 列有一些特殊符号，比如© 需删除。另外，数据格式全部为 Object 字符串格式，需要将 comment 和 favorites 两列更改为数值格式、 write_time 列更改为日期格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">print(data.shape)  <span class="comment"># 查看行数和列数</span></span><br><span class="line">print(data.info()) <span class="comment"># 查看总体情况</span></span><br><span class="line">print(data.head()) <span class="comment"># 输出前5行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">(<span class="number">49996</span>, <span class="number">8</span>)</span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line">_id           <span class="number">49996</span> non-null object</span><br><span class="line">abstract      <span class="number">49996</span> non-null object</span><br><span class="line">comment       <span class="number">49996</span> non-null object</span><br><span class="line">favorites     <span class="number">49996</span> non-null object</span><br><span class="line">name          <span class="number">49996</span> non-null object</span><br><span class="line">title         <span class="number">49996</span> non-null object</span><br><span class="line">url           <span class="number">49996</span> non-null object</span><br><span class="line">write_time    <span class="number">49996</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">8</span>)</span><br><span class="line">    </span><br><span class="line">	_id	abstract	comment	favorites	name	title	url	write_time</span><br><span class="line"><span class="number">0</span>	<span class="number">5</span>bdc2	“在你们看到…	<span class="number">22</span>	<span class="number">50</span>	普象工业设计小站©	看了苹果屌	https://	<span class="number">10</span>小时前</span><br><span class="line"><span class="number">1</span>	<span class="number">5</span>bdc2	中国”绿卡”号称“世界最难拿”	<span class="number">9</span>	<span class="number">16</span>	经济观察报©	递交材料厚	https://	<span class="number">10</span>小时前</span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>bdc2	鲜衣怒马少年时	<span class="number">2</span>	<span class="number">13</span>	小马宋	金庸小说陪	https://	<span class="number">11</span>小时前</span><br><span class="line"><span class="number">3</span>	<span class="number">5</span>bdc2	预告还是预警？	<span class="number">3</span>	<span class="number">10</span>	Cuba Libre	阿里即将发	https://	<span class="number">11</span>小时前</span><br><span class="line"><span class="number">4</span>	<span class="number">5</span>bdc2	库克：咋回事？	<span class="number">2</span>	<span class="number">3</span>	Cuba Libre	【虎嗅早报	https://	<span class="number">11</span>小时前</span><br></pre></td></tr></table></figure>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除无用_id列</span></span><br><span class="line">data.drop([<span class="string">'_id'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 替换掉特殊字符©</span></span><br><span class="line">data[<span class="string">'name'</span>].replace(<span class="string">'©'</span>,<span class="string">''</span>,inplace=<span class="keyword">True</span>,regex=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 字符更改为数值</span></span><br><span class="line">data = data.apply(pd.to_numeric,errors=<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># 更该日期格式</span></span><br><span class="line">data[<span class="string">'write_time'</span>] = data[<span class="string">'write_time'</span>].replace(<span class="string">'.*前'</span>,<span class="string">'2018-10-31'</span>,regex=<span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># 为了方便，将write_time列，包含几小时前和几天前的行，都替换为10月31日最后1天。</span></span><br><span class="line">data[<span class="string">'write_time'</span>] = pd.to_datetime(data[<span class="string">'write_time'</span>])</span><br></pre></td></tr></table></figure>
<p>下面，我们看一下数据是否有重复，如果有，那么需要删除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断整行是否有重复值</span></span><br><span class="line">print(any(data.duplicated()))</span><br><span class="line"><span class="comment"># 显示True，表明有重复值，进一步提取出重复值数量</span></span><br><span class="line">data_duplicated = data.duplicated().value_counts()</span><br><span class="line">print(data_duplicated) <span class="comment"># 显示2 True ，表明有2个重复值</span></span><br><span class="line"><span class="comment"># 删除重复值</span></span><br><span class="line">data = data.drop_duplicates(keep=<span class="string">'first'</span>)</span><br><span class="line"><span class="comment"># 删除部分行后，index中断，需重新设置index</span></span><br><span class="line">data = data.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#结果：</span></span><br><span class="line"><span class="keyword">True</span> </span><br><span class="line"><span class="keyword">False</span>    <span class="number">49994</span></span><br><span class="line"><span class="keyword">True</span>         <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>然后，我们再增加两列数据，一列是文章标题长度列，一列是年份列，便于后面进行分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'title_length'</span>] = data[<span class="string">'title'</span>].apply(len)</span><br><span class="line">data[<span class="string">'year'</span>] = data[<span class="string">'write_time'</span>].dt.year</span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line">abstract        <span class="number">49994</span> non-null object</span><br><span class="line">comment         <span class="number">49994</span> non-null int64</span><br><span class="line">favorites       <span class="number">49994</span> non-null int64</span><br><span class="line">name            <span class="number">49994</span> non-null object</span><br><span class="line">title           <span class="number">49994</span> non-null object</span><br><span class="line">url             <span class="number">49994</span> non-null object</span><br><span class="line">write_time      <span class="number">49994</span> non-null datetime64[ns]</span><br><span class="line">title_length    <span class="number">49994</span> non-null int64</span><br><span class="line">year            <span class="number">49994</span> non-null int64</span><br></pre></td></tr></table></figure>
<p>以上，就完成了基本的数据清洗处理过程，针对这 9 列数据可以开始进行分析了。</p>
<h2 id="4-描述性数据分析"><a href="#4-描述性数据分析" class="headerlink" title="4. 描述性数据分析"></a>4. 描述性数据分析</h2><p>通常，数据分析主要分为四类： 「描述型分析」、「诊断型分析」「预测型分析」「规范型分析」。「描述型分析」是用来概括、表述事物整体状况以及事物间关联、类属关系的统计方法，是这四类中最为常见的数据分析类型。通过统计处理可以简洁地用几个统计值来表示一组数据地集中性（如平均值、中位数和众数等）和离散型(反映数据的波动性大小，如方差、标准差等)。</p>
<p>这里，我们主要进行描述性分析，数据主要为数值型数据（包括离散型变量和连续型变量）和文本数据。</p>
<h3 id="4-1-总体情况"><a href="#4-1-总体情况" class="headerlink" title="4.1. 总体情况"></a>4.1. 总体情况</h3><p>先来看一下总体情况。</p>
<p>​        </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(data.describe())</span><br><span class="line">             comment     favorites  title_length </span><br><span class="line">count  <span class="number">49994.000000</span>  <span class="number">49994.000000</span>  <span class="number">49994.000000</span>  </span><br><span class="line">mean      <span class="number">10.860203</span>     <span class="number">34.081810</span>     <span class="number">22.775333</span>  </span><br><span class="line">std       <span class="number">24.085969</span>     <span class="number">48.276213</span>      <span class="number">9.540142</span>  </span><br><span class="line">min        <span class="number">0.000000</span>      <span class="number">0.000000</span>      <span class="number">1.000000</span>  </span><br><span class="line"><span class="number">25</span>%        <span class="number">3.000000</span>      <span class="number">9.000000</span>     <span class="number">17.000000</span>  </span><br><span class="line"><span class="number">50</span>%        <span class="number">6.000000</span>     <span class="number">19.000000</span>     <span class="number">22.000000</span>  </span><br><span class="line"><span class="number">75</span>%       <span class="number">12.000000</span>     <span class="number">40.000000</span>     <span class="number">28.000000</span>  </span><br><span class="line">max     <span class="number">2376.000000</span>   <span class="number">1113.000000</span>    <span class="number">224.000000</span></span><br></pre></td></tr></table></figure>
<p>这里，使用了 data.describe() 方法对数值型变量进行统计分析。从上面可以简要得出以下几个结论：</p>
<ul>
<li>读者的评论和收藏热情都不算太高，大部分文章（75 %）的评论数量为十几条，收藏数量不过几十个。这和一些微信大 V 公众号动辄百万级阅读、数万级评论和收藏量相比，虎嗅网的确相对小众一些。不过也正是因为小众，也才深得部分人的喜欢。</li>
<li>评论数最多的文章有 2376 条，收藏数最多的文章有 1113 个收藏量，说明还是有一些潜在的比较火或者质量比较好的文章。</li>
<li>最长的文章标题长达 224 个字，大部分文章标题长度在 20 来个字左右，所以标题最好不要太长或过短。</li>
</ul>
<p>对于非数值型变量（name、write_time），使用 describe() 方法会产生另外一种汇总统计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(data[<span class="string">'name'</span>].describe())</span><br><span class="line">print(data[<span class="string">'write_time'</span>].describe())</span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">count     <span class="number">49994</span></span><br><span class="line">unique     <span class="number">3162</span></span><br><span class="line">top          虎嗅</span><br><span class="line">freq      <span class="number">10513</span></span><br><span class="line">Name: name, dtype: object</span><br><span class="line">count                   <span class="number">49994</span></span><br><span class="line">unique                   <span class="number">2397</span></span><br><span class="line">top       <span class="number">2014</span><span class="number">-07</span><span class="number">-10</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">freq                      <span class="number">274</span></span><br><span class="line">first     <span class="number">2012</span><span class="number">-04</span><span class="number">-03</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">last      <span class="number">2018</span><span class="number">-10</span><span class="number">-31</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br></pre></td></tr></table></figure>
<p>unique 表示唯一值数量，top 表示出现次数最多的变量，freq 表示该变量出现的次数，所以可以简单得出以下几个结论：</p>
<ul>
<li>在文章来源方面，3162 个作者贡献了这 5 万篇文章，其中自家官网「虎嗅」写的数量最多，超过了 1 万篇，这也很自然。</li>
<li>在文章发表时间方面，最早的一篇文章来自于 2012年 4 月 3 日。 6 年多时间，发文数最多的 1 天 是 2014 年 7 月 10 日，一共发了 274 篇文章。</li>
</ul>
<h3 id="4-2-不同时期文章发布的数量变化"><a href="#4-2-不同时期文章发布的数量变化" class="headerlink" title="4.2. 不同时期文章发布的数量变化"></a>4.2. 不同时期文章发布的数量变化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-6/47981964.jpg" alt=""></p>
<p>可以看到 ，以季度为时间尺度的6 年间，前几年发文数量比较稳定，大概在1750 篇左右，个别季度数量激增到 2000 篇以上。2016 年之后文章开始增加到 2000 篇以上，可能跟网站知名度提升有关。首尾两个季度日期不全，所以数量比较少。</p>
<p>具体代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis1</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># # 汇总统计</span></span><br><span class="line">    <span class="comment"># print(data.describe())</span></span><br><span class="line">    <span class="comment"># print(data['name'].describe())</span></span><br><span class="line">    <span class="comment"># print(data['write_time'].describe())</span></span><br><span class="line">    </span><br><span class="line">    data.set_index(data[<span class="string">'write_time'</span>],inplace=<span class="keyword">True</span>)</span><br><span class="line">    data = data.resample(<span class="string">'Q'</span>).count()[<span class="string">'name'</span>]  <span class="comment"># 以季度汇总</span></span><br><span class="line">    data = data.to_period(<span class="string">'Q'</span>)</span><br><span class="line">    <span class="comment"># 创建x,y轴标签</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>,len(data),<span class="number">1</span>)</span><br><span class="line">    ax1.plot(x,data.values, <span class="comment">#x、y坐标</span></span><br><span class="line">        color = color_line , <span class="comment">#折线图颜色为红色</span></span><br><span class="line">        marker = <span class="string">'o'</span>,markersize = <span class="number">4</span> <span class="comment">#标记形状、大小设置</span></span><br><span class="line">        )</span><br><span class="line">    ax1.set_xticks(x) <span class="comment"># 设置x轴标签为自然数序列</span></span><br><span class="line">    ax1.set_xticklabels(data.index) <span class="comment"># 更改x轴标签值为年份</span></span><br><span class="line">    plt.xticks(rotation=<span class="number">90</span>) <span class="comment"># 旋转90度，不至太拥挤</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data.values):</span><br><span class="line">        plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors,fontsize=fontsize_text )</span><br><span class="line">        <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line">    <span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">    plt.title(<span class="string">'虎嗅网文章数量发布变化(2012-2018)'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.xlabel(<span class="string">'时期'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章(篇)'</span>)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘</span></span><br><span class="line">    plt.savefig(<span class="string">'虎嗅网文章数量发布变化.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="4-3-文章收藏量-TOP-10"><a href="#4-3-文章收藏量-TOP-10" class="headerlink" title="4.3. 文章收藏量 TOP 10"></a>4.3. 文章收藏量 TOP 10</h3><p>接下来，到了我们比较关心的问题：几万篇文章里，到底哪些文章写得比较好或者比较火？</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>title</th>
<th>favorites</th>
<th>comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>读完这10本书，你就能站在智商鄙视链的顶端了</td>
<td>1113</td>
<td>13</td>
</tr>
<tr>
<td>2</td>
<td>京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利</td>
<td>867</td>
<td>10</td>
</tr>
<tr>
<td>3</td>
<td>离职创业？先读完这22本书再说</td>
<td>860</td>
<td>9</td>
</tr>
<tr>
<td>4</td>
<td>货币如水，覆水难收</td>
<td>784</td>
<td>39</td>
</tr>
<tr>
<td>5</td>
<td>自杀经济学</td>
<td>778</td>
<td>119</td>
</tr>
<tr>
<td>6</td>
<td>2016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里</td>
<td>774</td>
<td>39</td>
</tr>
<tr>
<td>7</td>
<td>真正强大的商业分析能力是怎样炼成的？</td>
<td>746</td>
<td>18</td>
</tr>
<tr>
<td>8</td>
<td>腾讯没有梦想</td>
<td>705</td>
<td>32</td>
</tr>
<tr>
<td>9</td>
<td>段永平连答53问，核心是“不为清单”</td>
<td>703</td>
<td>27</td>
</tr>
<tr>
<td>10</td>
<td>王健林的滑铁卢</td>
<td>701</td>
<td>92</td>
</tr>
</tbody>
</table>
<p>此处选取了「favorites」(收藏数量)作为衡量标准。毕竟，一般好的文章，我们都会有收藏的习惯。</p>
<p>第一名「<a href="https://www.huxiu.com/article/123650.html" target="_blank" rel="noopener">读完这10本书，你就能站在智商鄙视链的顶端了</a> 」以 1113 次收藏位居第一，并且遥遥领先于后者，看来大家都怀有「想早日攀上人生巅峰，一览众人小」的想法啊。打开这篇文章的链接，文中提到了这几本书：《思考，快与慢》、《思考的技术》、《麦肯锡入职第一课：让职场新人一生受用的逻辑思考力》等。一本都没看过，看来这辈子是很难登上人生巅峰了。</p>
<p>发现两个有意思的地方。</p>
<p>第一，<strong>文章标题都比较短小精炼。</strong></p>
<p>第二，文章收藏量虽然比较高，但评论数都不多，猜测这是因为 <strong>大家都喜欢做伸手党</strong>？</p>
<h3 id="4-4-历年文章收藏量-TOP3"><a href="#4-4-历年文章收藏量-TOP3" class="headerlink" title="4.4. 历年文章收藏量 TOP3"></a>4.4. 历年文章收藏量 TOP3</h3><p>在了解文章的总体排名之后，我们来看看历年的文章排名是怎样的。这里，每年选取了收藏量最多的 3 篇文章。</p>
<table>
<thead>
<tr>
<th>year</th>
<th>title</th>
<th>favorites</th>
</tr>
</thead>
<tbody>
<tr>
<td>2012</td>
<td>产品的思路——来自腾讯张小龙的分享（全版）</td>
<td>187</td>
</tr>
<tr>
<td></td>
<td>Fab CEO：创办四家公司教给我的90件事</td>
<td>163</td>
</tr>
<tr>
<td></td>
<td>张小龙：微信背后的产品观</td>
<td>162</td>
</tr>
<tr>
<td>2013</td>
<td>创业者手记：我所犯的那些入门错误</td>
<td>473</td>
</tr>
<tr>
<td></td>
<td>马化腾三小时讲话实录：千亿美金这个线，其实很恐怖</td>
<td>391</td>
</tr>
<tr>
<td></td>
<td>雕爷亲身谈：白手起家的我如何在30岁之前赚到1000万。读《MBA教不了的创富课》</td>
<td>354</td>
</tr>
<tr>
<td>2014</td>
<td>85后，突变的一代</td>
<td>528</td>
</tr>
<tr>
<td></td>
<td>雕爷自述：什么是我做餐饮时琢磨、而大部分“外人”无法涉猎的思考？</td>
<td>521</td>
</tr>
<tr>
<td></td>
<td>据说这40张PPT是蚂蚁金服的内部培训资料……</td>
<td>485</td>
</tr>
<tr>
<td>2015</td>
<td>读完这10本书，你就能站在智商鄙视链的顶端了</td>
<td>1113</td>
</tr>
<tr>
<td></td>
<td>京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利</td>
<td>867</td>
</tr>
<tr>
<td></td>
<td>离职创业？先读完这22本书再说</td>
<td>860</td>
</tr>
<tr>
<td>2016</td>
<td>蝗虫般的刷客大军：手握千万手机号，分秒间薅干一家平台</td>
<td>554</td>
</tr>
<tr>
<td></td>
<td>准CEO必读的这20本书，你读过几本？</td>
<td>548</td>
</tr>
<tr>
<td></td>
<td>运营简史：一文读懂互联网运营的20年发展与演变</td>
<td>503</td>
</tr>
<tr>
<td>2017</td>
<td>2016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里</td>
<td>774</td>
</tr>
<tr>
<td></td>
<td>真正强大的商业分析能力是怎样炼成的？</td>
<td>746</td>
</tr>
<tr>
<td></td>
<td>王健林的滑铁卢</td>
<td>701</td>
</tr>
<tr>
<td>2018</td>
<td>货币如水，覆水难收</td>
<td>784</td>
</tr>
<tr>
<td></td>
<td>自杀经济学</td>
<td>778</td>
</tr>
<tr>
<td></td>
<td>腾讯没有梦想</td>
<td>705</td>
</tr>
</tbody>
</table>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-8/86110562.jpg" alt=""></p>
<p>可以看到，文章收藏量基本是逐年递增的，但 2015 年的 3 篇文章的收藏量却是最高的，包揽了总排名的前 3 名，不知道这一年的文章有什么特别之处。</p>
<p>以上只罗列了一小部分文章的标题，可以看到标题起地都蛮有水准的。关于标题的重要性，有这样通俗的说法：「<code>一篇好文章，标题占一半</code>」，一个好的标题可以大大增强文章的传播力和吸引力。文章标题虽只有短短数十字，但要想起好，里面也是很有很多技巧的。</p>
<p>好在，这里提供了 5 万个标题可以参考。<code>如需，可以在公众号后台回复「虎嗅」得到这份 CSV 文件。</code></p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis2</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># # 总收藏排名</span></span><br><span class="line">    <span class="comment"># top = data.sort_values(['favorites'],ascending = False)</span></span><br><span class="line">    <span class="comment"># # 收藏前10</span></span><br><span class="line">    <span class="comment"># top.index = (range(1,len(top.index)+1)) # 重置index，并从1开始编号</span></span><br><span class="line">    <span class="comment"># print(top[:10][['title','favorites','comment']])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按年份排名</span></span><br><span class="line">    <span class="comment"># # 增加一列年份列</span></span><br><span class="line">    <span class="comment"># data['year'] = data['write_time'].dt.year</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(data)</span>:</span></span><br><span class="line">        top = data.sort_values(<span class="string">'favorites'</span>,ascending=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> top[:<span class="number">3</span>]</span><br><span class="line">    data = data.groupby(by=[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line">    print(data[[<span class="string">'title'</span>,<span class="string">'favorites'</span>]])</span><br><span class="line">    <span class="comment"># 增加每年top123列，列依次值为1、2、3</span></span><br><span class="line">    data[<span class="string">'add'</span>] = <span class="number">1</span> <span class="comment"># 辅助</span></span><br><span class="line">    data[<span class="string">'top'</span>] = data.groupby(by=<span class="string">'year'</span>)[<span class="string">'add'</span>].cumsum()</span><br><span class="line">    data_reshape = data.pivot_table(index=<span class="string">'year'</span>,columns=<span class="string">'top'</span>,values=<span class="string">'favorites'</span>).reset_index()</span><br><span class="line">    <span class="comment"># print(data_reshape)  # ok</span></span><br><span class="line">    data_reshape.plot(</span><br><span class="line">        <span class="comment"># x='year',</span></span><br><span class="line">        y=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">        kind=<span class="string">'bar'</span>,</span><br><span class="line">        width=<span class="number">0.3</span>,</span><br><span class="line">        color=[<span class="string">'#1362A3'</span>,<span class="string">'#3297EA'</span>,<span class="string">'#8EC6F5'</span>]  <span class="comment"># 设置不同的颜色</span></span><br><span class="line">        <span class="comment"># title='虎嗅网历年收藏数最多的3篇文章'</span></span><br><span class="line">        )</span><br><span class="line">    plt.xlabel(<span class="string">'Year'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章收藏数量'</span>)</span><br><span class="line">    plt.title(<span class="string">'历年 TOP3 文章收藏量比较'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line">    <span class="comment"># plt.savefig('历年 Top3 文章收藏量比较.png',dpi=200)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="4-4-1-最高产作者-TOP20"><a href="#4-4-1-最高产作者-TOP20" class="headerlink" title="4.4.1. 最高产作者 TOP20"></a>4.4.1. 最高产作者 TOP20</h4><p>上面，我们从收藏量指标进行了分析,下面，我们关注一下发布文章的作者（个人/媒体）。前面提到发文最多的是虎嗅官方，有一万多篇文章，这里我们筛除官媒，看看还有哪些比较高产的作者。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-8/9931236.jpg" alt=""></p>
<p>可以看到，前 20 名作者的发文量差距都不太大。发文比较多的有「娱乐资本论」、「Eastland」、「发条橙子」这类媒体号；也有虎嗅官网团队的作者：发条橙子、周超臣、张博文等；还有部分独立作者：假装FBI、孙永杰等。可以尝试关注一下这些高产作者。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis3</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = data.groupby(data[<span class="string">'name'</span>])[<span class="string">'title'</span>].count()</span><br><span class="line">    data = data.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># pandas 直接绘制,.invert_yaxis()颠倒顺序</span></span><br><span class="line">    data[<span class="number">1</span>:<span class="number">21</span>].plot(kind=<span class="string">'barh'</span>,color=color_line).invert_yaxis()</span><br><span class="line">    <span class="keyword">for</span> y,x <span class="keyword">in</span> enumerate(list(data[<span class="number">1</span>:<span class="number">21</span>].values)):</span><br><span class="line">        plt.text(x+<span class="number">12</span>,y+<span class="number">0.2</span>,<span class="string">'%s'</span> %round(x,<span class="number">1</span>),ha=<span class="string">'center'</span>,color=colors)</span><br><span class="line">    plt.xlabel(<span class="string">'文章数量'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'作者'</span>)</span><br><span class="line">    plt.title(<span class="string">'发文数量最多的 TOP20 作者'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(<span class="string">'发文数量最多的TOP20作者.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="4-4-2-平均文章收藏量最多作者-TOP-10"><a href="#4-4-2-平均文章收藏量最多作者-TOP-10" class="headerlink" title="4.4.2. 平均文章收藏量最多作者 TOP 10"></a>4.4.2. 平均文章收藏量最多作者 TOP 10</h4><p>我们关注一个作者除了是因为文章高产以外，可能更看重的是其文章水准。这里我们选择「文章平均收藏量」（总收藏量/文章数）这个指标，来看看文章水准比较高的作者是哪些人。</p>
<p>这里，为了避免出现「某作者只写了一篇高收藏率的文章」这种不能代表其真实水准的情况，我们将筛选范围定在至少发布过 5 篇文章的作者们。</p>
<table>
<thead>
<tr>
<th>name</th>
<th>total_favorites</th>
<th>ariticls_num</th>
<th>avg_favorites</th>
</tr>
</thead>
<tbody>
<tr>
<td>重读</td>
<td>1947</td>
<td>6</td>
<td>324</td>
</tr>
<tr>
<td>楼台</td>
<td>2302</td>
<td>8</td>
<td>287</td>
</tr>
<tr>
<td>彭萦</td>
<td>2487</td>
<td>9</td>
<td>276</td>
</tr>
<tr>
<td>曹山石</td>
<td>1187</td>
<td>5</td>
<td>237</td>
</tr>
<tr>
<td>饭统戴老板</td>
<td>7870</td>
<td>36</td>
<td>218</td>
</tr>
<tr>
<td>笔记侠</td>
<td>1586</td>
<td>8</td>
<td>198</td>
</tr>
<tr>
<td>辩手李慕阳</td>
<td>11989</td>
<td>62</td>
<td>193</td>
</tr>
<tr>
<td>李录</td>
<td>2370</td>
<td>13</td>
<td>182</td>
</tr>
<tr>
<td>高晓松</td>
<td>889</td>
<td>5</td>
<td>177</td>
</tr>
<tr>
<td>宁南山</td>
<td>2827</td>
<td>16</td>
<td>176</td>
</tr>
</tbody>
</table>
<p>可以看到，前 10 名作者包括：遥遥领先的 <strong>重读</strong>、两位高产又有质量的 <strong>辩手李慕阳</strong> 和 <strong>饭统戴老板</strong>  ，还有大众比较熟悉的 <strong>高晓松</strong>、<strong>宁南山 </strong>等。</p>
<p>如果你将这份名单和上面那份高产作者名单进行对比，会发现他们没有出现在这个名单中。相比于数量，质量可能更重要吧。</p>
<p>下面，我们就来看看排名第一的 <strong>重读</strong> 都写了哪些高收藏量文章。</p>
<table>
<thead>
<tr>
<th>order</th>
<th>title</th>
<th>favorites</th>
<th>write_time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>我采访出200多万字素材，还原了阿里系崛起前传</td>
<td>231</td>
<td>2018/10/31</td>
</tr>
<tr>
<td>2</td>
<td>阿里史上最强人事地震回顾：中供铁军何以被生生解体</td>
<td>494</td>
<td>2018/4/9</td>
</tr>
<tr>
<td>3</td>
<td>马云“斩”卫哲：复原阿里史上最震撼的人事地震</td>
<td>578</td>
<td>2018/3/15</td>
</tr>
<tr>
<td>4</td>
<td>重读一场马云发起、针对卫哲的批斗会</td>
<td>269</td>
<td>2017/8/31</td>
</tr>
<tr>
<td>5</td>
<td>阿里“中供系”前世今生：马云麾下最神秘的子弟兵</td>
<td>203</td>
<td>2017/5/10</td>
</tr>
<tr>
<td>6</td>
<td>揭秘马云麾下最神秘的子弟兵：阿里“中供系”的前世今生</td>
<td>172</td>
<td>2017/4/26</td>
</tr>
</tbody>
</table>
<p>居然写的都是清一色关于马老板家的文章。</p>
<p>了解了前十名作者之后，我们顺便也看看那些处于最后十名的都是哪些作者。</p>
<table>
<thead>
<tr>
<th>name</th>
<th>total_favorites</th>
<th>ariticls_num</th>
<th>avg_favorites</th>
</tr>
</thead>
<tbody>
<tr>
<td>于斌</td>
<td>25</td>
<td>11</td>
<td>2</td>
</tr>
<tr>
<td>朝克图</td>
<td>33</td>
<td>23</td>
<td>1</td>
</tr>
<tr>
<td>东风日产</td>
<td>24</td>
<td>13</td>
<td>1</td>
</tr>
<tr>
<td>董晓常</td>
<td>14</td>
<td>8</td>
<td>1</td>
</tr>
<tr>
<td>蔡钰</td>
<td>31</td>
<td>16</td>
<td>1</td>
</tr>
<tr>
<td>马继华</td>
<td>12</td>
<td>11</td>
<td>1</td>
</tr>
<tr>
<td>angeljie</td>
<td>7</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>薛开元</td>
<td>6</td>
<td>6</td>
<td>1</td>
</tr>
<tr>
<td>pookylee</td>
<td>15</td>
<td>24</td>
<td>0</td>
</tr>
<tr>
<td>Yang Yemeng</td>
<td>0</td>
<td>7</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>一对比，就能看到他们的文章收藏量就比较寒碜了。尤其好奇最后一位作者 <strong>Yang Yemeng</strong> ，他写了 7 篇文章，竟然一个收藏都没有。</p>
<p>来看看他究竟写了些什么文章。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-7/59717401.jpg" alt=""></p>
<p>原来写的全都是英文文章，看来大家并不太钟意阅读英文类的文章啊。</p>
<p>具体实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis4</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = pd.pivot_table(data,values=[<span class="string">'favorites'</span>],index=<span class="string">'name'</span>,aggfunc=[np.sum,np.size])</span><br><span class="line">    data[<span class="string">'avg'</span>] = data[(<span class="string">'sum'</span>,<span class="string">'favorites'</span>)]/data[(<span class="string">'size'</span>,<span class="string">'favorites'</span>)]</span><br><span class="line">    <span class="comment"># 平均收藏数取整</span></span><br><span class="line">    <span class="comment"># data['avg'] = data['avg'].round(decimals=1)</span></span><br><span class="line">    data[<span class="string">'avg'</span>] = data[<span class="string">'avg'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">    <span class="comment"># flatten 平铺列</span></span><br><span class="line">    data.columns = data.columns.get_level_values(<span class="number">0</span>)</span><br><span class="line">    data.columns = [<span class="string">'total_favorites'</span>,<span class="string">'ariticls_num'</span>,<span class="string">'avg_favorites'</span>]</span><br><span class="line">    <span class="comment"># 筛选出文章数至少5篇的</span></span><br><span class="line">    data=data.query(<span class="string">'ariticls_num &gt; 4'</span>)</span><br><span class="line">    data = data.sort_values(by=[<span class="string">'avg_favorites'</span>],ascending=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># # 查看平均收藏率第一名详情</span></span><br><span class="line">    <span class="comment"># data = data.query('name == "重读"')</span></span><br><span class="line">    <span class="comment"># # 查看平均收藏率倒数第一名详情</span></span><br><span class="line">    <span class="comment"># data = data.query('name == "Yang Yemeng"')</span></span><br><span class="line">    <span class="comment"># print(data[['title','favorites','write_time']])</span></span><br><span class="line">    print(data[:<span class="number">10</span>]) 	<span class="comment"># 前10名</span></span><br><span class="line">    print(data[<span class="number">-10</span>:])	<span class="comment"># 后10名</span></span><br></pre></td></tr></table></figure>
<h3 id="4-5-文章评论数最多-TOP10"><a href="#4-5-文章评论数最多-TOP10" class="headerlink" title="4.5. 文章评论数最多 TOP10"></a>4.5. 文章评论数最多 TOP10</h3><p>说完了收藏量。下面，我们再来看看评论数量最多的文章是哪些。</p>
<table>
<thead>
<tr>
<th>order</th>
<th>title</th>
<th>comment</th>
<th>favorites</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>喜瓜2.0—明星社交应用的中国式引进与创新</td>
<td>2376</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>百度，请给“儿子们”好好起个名字</td>
<td>1297</td>
<td>9</td>
</tr>
<tr>
<td>3</td>
<td>三星S5为什么对凤凰新闻客户端下注？</td>
<td>1157</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>三星Tab S：马是什么样的马？鞍又是什么样的鞍？</td>
<td>951</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>三星，正在重塑你的营销观</td>
<td>914</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>马化腾，你就把微信卖给运营商得了！</td>
<td>743</td>
<td>20</td>
</tr>
<tr>
<td>7</td>
<td>【文字直播】罗永浩 VS 王自如 网络公开辩论</td>
<td>711</td>
<td>33</td>
</tr>
<tr>
<td>8</td>
<td>看三星Hub如何推动数字内容消费变革</td>
<td>684</td>
<td>1</td>
</tr>
<tr>
<td>9</td>
<td>三星要重新定义软件与内容商店新模式，SO?</td>
<td>670</td>
<td>0</td>
</tr>
<tr>
<td>10</td>
<td>三星Hub——数字内容交互新模式</td>
<td>611</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>基本上都是和 <strong>三星</strong> 有关的文章，这些文章大多来自 2014 年，那几年 <strong>三星</strong> 好像是挺火的，不过这两年国内基本上都见不到三星的影子了，世界变化真快。</p>
<p>发现了两个有意思的现象。</p>
<p>第一，上面关于 <strong>三星</strong> 和前面 <strong>阿里</strong> 的这些批量文章，它们「霸占」了评论和收藏榜，结合知乎上曾经的一篇关于介绍虎嗅这个网站的文章：<a href="https://www.zhihu.com/question/20799239/answer/20698562" target="_blank" rel="noopener">虎嗅网其实是这样的</a> ，貌似能发现些微妙的事情。</p>
<p>第二，这些文章评论数和收藏数两个指标几乎呈极端趋势，评论量多的文章收藏量却很少，评论量少的文章收藏量却很多。</p>
<p>我们进一步观察下这两个参数的关系。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-6/85687626.jpg" alt=""></p>
<p>可以看到，大多数点都位于左下角，意味着这些文章收藏量和评论数都比较低。但也存在少部分位于上方和右侧的异常值，表明这些文章呈现 「多评论、少收藏」或者「少评论、多收藏」的特点。</p>
<h3 id="4-6-文章标题长度"><a href="#4-6-文章标题长度" class="headerlink" title="4.6. 文章标题长度"></a>4.6. 文章标题长度</h3><p>下面，我们再来看看文章标题的长度和收藏量之间有没有什么关系。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-6/93399033.jpg" alt=""></p>
<p>大致可以看出两点现象：</p>
<p>第一，<strong>收藏量高的文章，他们的标题都比较短</strong>（右侧的部分散点）。</p>
<p>第二，<strong>标题很长的文章，它们的收藏量都非常低</strong>（左边形成了一条垂直线）。</p>
<p>看来，文章起标题时最好不要起太长的。</p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis5</span><span class="params">(data)</span>:</span></span><br><span class="line">    plt.scatter(</span><br><span class="line">        x=data[<span class="string">'favorites'</span>],</span><br><span class="line">        y =data[<span class="string">'comment'</span>],</span><br><span class="line">        s=data[<span class="string">'title_length'</span>]/<span class="number">2</span>,</span><br><span class="line">        )</span><br><span class="line">    plt.xlabel(<span class="string">'文章收藏量'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章评论数'</span>)</span><br><span class="line">    plt.title(<span class="string">'文章标题长度与收藏量和评论数之间的关系'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout() </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="4-7-文本分析"><a href="#4-7-文本分析" class="headerlink" title="4.7. 文本分析"></a>4.7. 文本分析</h3><p>最后，我们从这 5 万篇文章中的标题和摘要中，来看看虎嗅网的文章主要关注的都是哪些主题领域。</p>
<p>这里首先运用了 jieba 分词包对标题进行了分词，然后用 WordCloud 做成了词云图，因虎嗅网含有「虎」字，故选取了一张老虎头像。（关于 jieba 和 WordCloud 两个包，之后再详细介绍）</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-8/24683879.jpg" alt=""></p>
<p>可以看到文章的主题内容侧重于：互联网、知名公司、电商、投资这些领域。这和网站本身对外宣传的核心内容，即「关注互联网与移动互联网一系列明星公司的起落轨迹、产业潮汐的动力与趋势，以及互联网与移动互联网如何改造传统产业」大致相符合。</p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis6</span><span class="params">(data)</span>:</span></span><br><span class="line">    text=<span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="string">'title'</span>].values:</span><br><span class="line">        symbol_to_replace = <span class="string">'[!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@，。?★、…【】《》？“”‘’！[\\]^_`&#123;|&#125;~]+'</span></span><br><span class="line">        i = re.sub(symbol_to_replace,<span class="string">''</span>,i)</span><br><span class="line">        text+=<span class="string">' '</span>.join(jieba.cut(i,cut_all=<span class="keyword">False</span>))</span><br><span class="line">    d = path.dirname(__file__) <span class="keyword">if</span> <span class="string">"__file__"</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> os.getcwd()</span><br><span class="line"></span><br><span class="line">    background_Image = np.array(Image.open(path.join(d, <span class="string">"tiger.png"</span>)))</span><br><span class="line">    font_path = <span class="string">'C:\Windows\Fonts\SourceHanSansCN-Regular.otf'</span>  <span class="comment"># 思源黑字体</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加stopswords</span></span><br><span class="line">    stopwords = set()</span><br><span class="line">    <span class="comment"># 先运行对text进行词频统计再排序，再选择要增加的停用词</span></span><br><span class="line">    stopwords.update([<span class="string">'如何'</span>,<span class="string">'怎么'</span>,<span class="string">'一个'</span>,<span class="string">'什么'</span>,<span class="string">'为什么'</span>,<span class="string">'还是'</span>,<span class="string">'我们'</span>,<span class="string">'为何'</span>,<span class="string">'可能'</span>,<span class="string">'不是'</span>,<span class="string">'没有'</span>,<span class="string">'哪些'</span>,<span class="string">'成为'</span>,<span class="string">'可以'</span>,<span class="string">'背后'</span>,<span class="string">'到底'</span>,<span class="string">'就是'</span>,<span class="string">'这么'</span>,<span class="string">'不要'</span>,<span class="string">'怎样'</span>,<span class="string">'为了'</span>,<span class="string">'能否'</span>,<span class="string">'你们'</span>,<span class="string">'还有'</span>,<span class="string">'这样'</span>,<span class="string">'这个'</span>,<span class="string">'真的'</span>,<span class="string">'那些'</span>])</span><br><span class="line">    wc = WordCloud(</span><br><span class="line">        background_color = <span class="string">'black'</span>,</span><br><span class="line">        font_path = font_path,</span><br><span class="line">        mask = background_Image,</span><br><span class="line">        stopwords = stopwords,</span><br><span class="line">        max_words = <span class="number">2000</span>,</span><br><span class="line">        margin =<span class="number">2</span>,</span><br><span class="line">        max_font_size = <span class="number">100</span>,</span><br><span class="line">        random_state = <span class="number">42</span>,</span><br><span class="line">        scale = <span class="number">2</span>,</span><br><span class="line">    )</span><br><span class="line">    wc.generate_from_text(text)</span><br><span class="line">    process_word = WordCloud.process_text(wc, text)</span><br><span class="line">    <span class="comment"># 下面是字典排序</span></span><br><span class="line">    sort = sorted(process_word.items(),key=<span class="keyword">lambda</span> e:e[<span class="number">1</span>],reverse=<span class="keyword">True</span>) <span class="comment"># sort为list</span></span><br><span class="line">    print(sort[:<span class="number">50</span>])  <span class="comment"># 输出前词频最高的前50个，然后筛选出不需要的stopwords，添加到前面的stopwords.update()方法中</span></span><br><span class="line">    img_colors = ImageColorGenerator(background_Image)</span><br><span class="line">    wc.recolor(color_func=img_colors)  <span class="comment"># 颜色跟随图片颜色</span></span><br><span class="line">    plt.imshow(wc,interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘</span></span><br><span class="line">    plt.savefig(<span class="string">'huxiu20.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>上面的关键词是这几年总体的概况，而科技互联网行业每年的发展都是不同的，所以，我们再来看看历年的一些关键词，透过这些关键词看看这几年互联网行业、科技热点、知名公司都有些什么不同变化。</p>
<p><img src="http://pbscl931v.bkt.clouddn.com/18-11-7/78186420.jpg" alt=""></p>
<p>可以看到每年的关键词都有一些相同之处，但也不同的地方：</p>
<ul>
<li>中国互联网、公司、苹果、腾讯、阿里等这些热门关键词一直都是热门，这几家公司真是稳地一批啊。</li>
<li>每年会有新热点涌现：比如 2013 年的微信（刚开始火）、2016 年的直播（各大直播平台如雨后春笋般出现）、2017年的 iPhone（上市十周年）、2018年的小米（上市）。</li>
<li>不断有新的热门技术出现：2013 - 2015 年的 O2O、2016 年的 VR、2017 年的 AI 、2018 年的「区块链」。这些科技前沿技术也是这几年大家口耳相传的热门词汇。</li>
</ul>
<p>通过这一幅图，就看出了这几年科技互联网行业、明星公司、热点信息的风云变化。</p>
<h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><ul>
<li>本文简要分析了虎嗅网 5 万篇文章信息，大致了解了近些年科技互联网的千变万化。</li>
<li>发掘了那些优秀的文章和作者，能够节省宝贵的时间成本。</li>
<li>一篇文章要想传播广泛，文章本身的质量和标题各占一半，文中的5 万个标题相信能够带来一些灵感。</li>
<li>本文尚未做深入的文本挖掘，而文本挖掘可能比数据挖掘涵盖的信息量更大，更有价值。进行这些分析需要机器学习和深度学习的知识，待后期学习后再来补充。</li>
</ul>
<p>本文完。</p>
<p>文中的完整代码和素材可以在公众号后台回复「<strong>虎嗅 </strong>」 或者在下面的链接中获取：</p>
<p><a href="https://github.com/makcyun/eastmoney_spider" target="_blank" rel="noopener">https://github.com/makcyun/eastmoney_spider</a></p>
<hr>
<p>推荐阅读：</p>
<p><a href="https://www.makcyun.top/web_scraping_withpython4.html">做 PPT 没灵感？澎湃网 1500 期信息图送给你</a></p>
<p><a href="https://www.makcyun.top/web_scraping_withpython7.html">听说你想创业找投资？国内创业公司的信息都在这里了</a> </p>
<p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p>
<center>欢迎扫一扫识别关注我的公众号</center>
      
    </div>
	
	
    
    
    
	

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>你一打赏，我就写得更来劲了</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="高级农民工 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python爬虫/" rel="tag"><i class="fa fa-tag"></i> Python爬虫</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div style="color: rgba(0, 0, 0, 0.75); font-size:13px; letter-spacing:3px">(&gt;你觉得这篇文章怎么样？&lt;)</div>
            <div id="wpac-rating"></div>
          </div>
        

        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/weekly_sharing3.html" rel="next" title="安卓最好用的电子书阅读器">
                安卓最好用的电子书阅读器 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="http://pbscl931v.bkt.clouddn.com/18-8-3/40685653.jpg"
                alt="高级农民工" />
            
              <p class="site-author-name" itemprop="name">高级农民工</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/makcyun" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:johnny824lee@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          
        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-分析背景"><span class="nav-text">1. 分析背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-为什么选择虎嗅"><span class="nav-text">1.1. 为什么选择虎嗅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-分析内容"><span class="nav-text">1.2. 分析内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-分析工具"><span class="nav-text">1.3. 分析工具</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-数据抓取"><span class="nav-text">2. 数据抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-目标网站分析"><span class="nav-text">2.1. 目标网站分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-pyspider-介绍"><span class="nav-text">2.2. pyspider 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-抓取数据"><span class="nav-text">2.3. 抓取数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-数据清洗处理"><span class="nav-text">3. 数据清洗处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-描述性数据分析"><span class="nav-text">4. 描述性数据分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-总体情况"><span class="nav-text">4.1. 总体情况</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-不同时期文章发布的数量变化"><span class="nav-text">4.2. 不同时期文章发布的数量变化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-文章收藏量-TOP-10"><span class="nav-text">4.3. 文章收藏量 TOP 10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-历年文章收藏量-TOP3"><span class="nav-text">4.4. 历年文章收藏量 TOP3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-最高产作者-TOP20"><span class="nav-text">4.4.1. 最高产作者 TOP20</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-平均文章收藏量最多作者-TOP-10"><span class="nav-text">4.4.2. 平均文章收藏量最多作者 TOP 10</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-文章评论数最多-TOP10"><span class="nav-text">4.5. 文章评论数最多 TOP10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-文章标题长度"><span class="nav-text">4.6. 文章标题长度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-文本分析"><span class="nav-text">4.7. 文本分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-小结"><span class="nav-text">5. 小结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        

<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="heart">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">高级农民工</span>

  
</div>



  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_uv">访客数:<span id="busuanzi_value_site_uv"></span>人次</span>
  <span class="post-meta-divider">|</span>
  <span id="busuanzi_container_site_pv">总访问量:<span id="busuanzi_value_site_pv"></span>次</span>
  <span class="post-meta-divider">|</span>



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">全站共71.4k字</span>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("yps9pUuWWGeNG1MwVI2tVGys-gzGzoHsz", "SNn7AhhHPrCndytXWSTwD5A2");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 12522,
    el: 'wpac-rating',
    color: 'E0943E'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  

  

  

  
</body>
</html>
