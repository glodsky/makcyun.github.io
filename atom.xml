<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>高级农民工</title>
  
  <subtitle>Beginner&#39;s Mind</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.makcyun.top/"/>
  <updated>2018-10-28T13:14:03.676Z</updated>
  <id>https://www.makcyun.top/</id>
  
  <author>
    <name>高级农民工</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>每周分享第 2 期：一掏出手机，就暴露了程序猿身份</title>
    <link href="https://www.makcyun.top/weekly_sharing2.html"/>
    <id>https://www.makcyun.top/weekly_sharing2.html</id>
    <published>2018-10-26T08:16:24.000Z</published>
    <updated>2018-10-28T13:14:03.676Z</updated>
    
    <content type="html"><![CDATA[<p>推荐一款神级桌面 App：Aris 终端桌面。</p><a id="more"></a>  <p>这是「每周分享」的第 2 期。</p><p>先简单介绍下这个栏目。顾名思义，就是会在每个周末分享一篇文章。内容涵盖范围会比较广，主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p>好，下面开始进入正题。</p><p><img src="http://pc1lljdwb.bkt.clouddn.com/3.gif" alt=""></p><p>自从我的手机桌面变成这个样子以后，每当在电梯里掏出手机时，总隐隐约约能感觉到有异样的眼神。我猜他们心里在想：「这人要么是个程序猿，要么就是个装 X 犯。」</p><p>谁还没点极客精神，是吧？</p><p>这一期，想向你推荐这款我用了几个月并且爱不释手的 App：「<strong>Aris 终端桌面</strong>」，有 <strong>3</strong> 个原因。</p><p><code>首先，它能用来装 X</code>。</p><p>提供了多种桌面主题，且高度可自定义化。只要你想，就可以打造出独一无二的桌面来。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/99042177.jpg" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/60690017.jpg" alt=""></p><p>其次，它体积只有 4 M大，且非常省电。光这两个特点就足以秒杀众多其他桌面。</p><p>最后，也是最重要的，它可以改变我们玩手机的方式。`</p><p>如果说一般的手机桌面是 Windows 的话，这个 App 桌面就是 Linux，一切操作都靠命令来完成，快捷方便。<br>它有很多实用的功能 ，下面一一道来。</p><h3 id="1-打开-App"><a href="#1-打开-App" class="headerlink" title="1. 打开 App"></a>1. 打开 App</h3><p>它最实用的功能就是通常输入不超过 <strong>3 个字母</strong> 就能够快速打开手机上的 App。不用像普通桌面需要 <strong>切换或者下拉屏幕</strong> 挨个去找那么复杂。<br>比如打开 「微信」、「支付宝」分别只需要输入字母  <strong>W</strong> 和 <strong>ZF</strong> 就能打开，<code>1 秒钟都要不到</code>。</p><p>对于平常不太常用的功能比如「设置」、「闹钟」这些，仍然只需要输入几个字母，很快就能打开，不用再到处去找了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/33781571.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="2-打开微信「扫一扫」"><a href="#2-打开微信「扫一扫」" class="headerlink" title="2. 打开微信「扫一扫」"></a>2. 打开微信「扫一扫」</h3><p>我们每天几乎都要打开微信「扫一扫」付款或者添加好友。常规操作需要打开微信右上角的「+ 号」，再点击扫一扫。而使用它，只需要输入「wes」就能调用 wescan 命令直接打开「扫一扫」，比常规操作快多了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-24/51970718.jpg" alt=""></p><h3 id="3-打电话"><a href="#3-打电话" class="headerlink" title="3. 打电话"></a>3. 打电话</h3><p>打电话也很方便，像搜索 App 一样，可以直接输入联系人拼音字母，然后拨号就行了。省去进到电话 App 里面去拨打的步骤。</p><p>上面只是常规操作，下面介绍些<code>「骚」</code>操作。</p><h3 id="4-打开手电筒-WIFI"><a href="#4-打开手电筒-WIFI" class="headerlink" title="4. 打开手电筒 / WIFI"></a>4. 打开手电筒 / WIFI</h3><p>当需要打开手电筒时，那么只要输入 「to」就能打开手电筒的命令「Torch」，用完要关上就再输入一次「to」就可以；当走到一个可以连 WIFI的地方，只需要输入 WI，就能打开  WIFI命令。当然，其他很多工具都可以打开。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/34263305.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="5-查天气"><a href="#5-查天气" class="headerlink" title="5. 查天气"></a>5. 查天气</h3><p>如果你想查一下某个城市的天气，比如「北京」那么只需要输入：beijing weather。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/56479602.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="6-查股价"><a href="#6-查股价" class="headerlink" title="6. 查股价"></a>6. 查股价</h3><p>如果你炒股，还可以查询股票股价，比如查询下「万科」今天的股价，只需要输入：sz.000002 stock。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/30371194.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="7-翻译"><a href="#7-翻译" class="headerlink" title="7. 翻译"></a>7. 翻译</h3><p>还可以翻译单词，比如遇到一个不熟悉的词： charming，那么输入：charming translating ，就会返回该词的翻译：迷人的。</p><h3 id="8-发送-apk-到其他软件"><a href="#8-发送-apk-到其他软件" class="headerlink" title="8. 发送 apk 到其他软件"></a>8. 发送 apk 到其他软件</h3><p>这是个非常实用的功能，可以把手机上的 App  直接打包成 Apk 文件发给别人。比如我手机上有个好用的 WPS Office，然后想通过 QQ 发送给朋友，<br>则可以使用下面的命令：<br> wpsoffice apk qq 就能发送了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-23/76995835.jpg" alt=""></p><p>还有很多好用的功能，这里就不再往下罗列了，可以看官方文档进一步了解这款 App 是怎么使用的：</p><p><a href="https://7doger.gitbooks.io/aris/" target="_blank" rel="noopener">https://7doger.gitbooks.io/aris/</a></p><p>到这里，如果你想下载下来把玩一下，可以在网上搜索下载或者在后台回复<code>「aris」</code>得到这款 App。</p><p>最后，知乎上有一篇讲 <a href="https://www.zhihu.com/question/20311673/answer/30516935" target="_blank" rel="noopener">「极客精神」</a>的文章我觉得写地很好，分享给你。</p><blockquote><p>所谓的“极客精神”，描述起来就这样简单——“好奇之心与改变之力”。你不需要一定是个程序员或者是产品经理，也不一定变成一副科技宅的模样，如果你对世界充满好奇心和探索精神，并愿意自己去创造哪怕一些改变——这，其实就够了。</p></blockquote><p>本文完。</p><p><strong>推荐阅读：</strong></p><p><a href="https://www.makcyun.top/fuli01.html">每周分享第1期：关于 PDF 处理软件，你需要的都在这里了</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;推荐一款神级桌面 App：Aris 终端桌面。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="佳软" scheme="https://www.makcyun.top/tags/%E4%BD%B3%E8%BD%AF/"/>
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
      <category term="APP" scheme="https://www.makcyun.top/tags/APP/"/>
    
  </entry>
  
  <entry>
    <title>安卓最强大最好用的阅读器</title>
    <link href="https://www.makcyun.top/weekly_sharing3.html"/>
    <id>https://www.makcyun.top/weekly_sharing3.html</id>
    <published>2018-10-26T08:16:24.000Z</published>
    <updated>2018-11-02T07:13:52.234Z</updated>
    
    <content type="html"><![CDATA[<p>推荐一款最好用的手机阅读 App。</p><a id="more"></a>  <p>这是每周分享的第 3 期。</p><p>先简单介绍下这个栏目，顾名思义，就是会在每个周末分享一篇文章。内容主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p>好，下面开始进入正题。</p><p>我们大多数人每天当中花时间花的最多的事情，应该就是看手机了，聊天    玩游戏、看视频、或者阅读学习等。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/75427908.jpg" alt=""></p><p>我呢，手机阅读的时间相对多一些。自从学习 Python 以来，手机上下载了很多的 PDF 电子书，然后在 WPS 中阅读。起初一直觉得很方便，各个文件之间的切换也非常顺畅。不过，用久了之后发现有几个不方便的地方：</p><ul><li><p>目录跳转麻烦</p><p>有时看书不是从头看到尾的，而是跳跃地阅读部分章节。需要先定位到所在目录页数，再滑动到相应位置，还要看其他章节时，则需重复同样的操作。这样一来，需要花费较多的时间在目录和内容的切换上，真正阅读的时间减少了。</p></li><li><p>不支持其他电子书格式</p><p>除了 PDF 格式以外，WPS 几乎不支持 txt、epub、mobi 等格式。</p></li><li><p>阅读功能单一</p><p>缺少诸如：蓝光过滤、字体背景设置、滚动翻页等功能。</p></li><li><p>缺少自定义性功能</p><p>WPS 走的是简洁路线，提供的自定义功能很少，用来查看文件还可以，但是阅读就显得力不从心了。</p></li></ul><p>发现了这些痛点之后，我便开始寻找替代品，前后试用了多款 App。最近，终于找到一款能够解决以上所有问题的阅读器。这款堪称 「安卓最好用的阅读器」，名叫：「<strong>静读天下</strong>」。</p><p>很多非常棒的 App 都具备一个特质是：提供 <strong>高度可自定义</strong> 的使用功能以满足不同用户的喜好与需求。而 <strong>静读天下</strong> 就是其中的佼佼者之一。用了它之后，阅读体验得到了大大提升，<br>下面一一道来它的各种好。</p><h2 id="1-看书"><a href="#1-看书" class="headerlink" title="1. 看书"></a>1. 看书</h2><h3 id="1-1-目录快速跳转定位"><a href="#1-1-目录快速跳转定位" class="headerlink" title="1.1. 目录快速跳转定位"></a>1.1. 目录快速跳转定位</h3><p>它最实用的一个功能就是「<strong>能够自动识别电子书的目录</strong>」。单击屏幕即可呼出章节目录，然后点击想查看的章节内容即可快速跳转到相应位置。看完以后如果想看其他章节内容，只需要重复操作一次即可。这样，就缩短了查找内容的时间，从而可以更加专注于阅读。</p><p><img src="http://pc1lljdwb.bkt.clouddn.com/%E9%9D%99%E8%AF%BB%E5%A4%A9%E4%B8%8B.gif" alt=""></p><h3 id="1-2-便捷的护眼功能"><a href="#1-2-便捷的护眼功能" class="headerlink" title="1.2. 便捷的护眼功能"></a>1.2. 便捷的护眼功能</h3><p>每天看书的时间是在变化的，白天和晚上的光线会有所不同。因而护眼阅读就显得很有必要。<br>虽然手机上有调节亮度的功能，但功能不全且需要额外操作。所幸，该 App 里只需通过触屏手势就能实现 「快速调节亮度」 和 「蓝光过滤 」功能。另外，还能够自动切换白天/晚上模式。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/26133427.jpg" alt=""></p><h3 id="1-3-字体、配色、背景设置"><a href="#1-3-字体、配色、背景设置" class="headerlink" title="1.3. 字体、配色、背景设置"></a>1.3. 字体、配色、背景设置</h3><p>它支持的电子书格式除了 PDF 以外，还支持 txt、epub、mobi 等多种格式。这些格式支持设置字体、配色、背景等功能。<br>比如，看惯了无衬线字体，想体验下有棱角的衬线字体，那么可以下载自己喜欢的字体然后使用。字体的大小、颜色、行距、留白都可以凭自己喜好设置，甚至还可以竖版阅读。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/52757050.jpg" alt=""></p><p>除此之外，背景也是可以随意更换的，内置十几种背景，不够还可以下载自己自己喜欢的，可以说提供了无限选择。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/28095706.jpg" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/71065070.jpg" alt=""></p><h3 id="1-4-添加批注"><a href="#1-4-添加批注" class="headerlink" title="1.4. 添加批注"></a>1.4. 添加批注</h3><p>有时，会边阅读边做些批注，比如添加：高亮、下划线、删除线、标注等。它提供了丰富的批注功能，批注好后可以在书签页面里看到，方便下次快速查找到。</p><h3 id="1-5-书籍分类"><a href="#1-5-书籍分类" class="headerlink" title="1.5. 书籍分类"></a>1.5. 书籍分类</h3><p>如果手机里有很多且类别不同的电子书，有了这个书籍分类功能会更方便。比如，我将电子书分为 Pyhon、数学、机器学习等类别，想看哪一类就筛选即可。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/11726230.jpg" alt=""></p><h3 id="1-6-添加书签"><a href="#1-6-添加书签" class="headerlink" title="1.6. 添加书签"></a>1.6. 添加书签</h3><p>当需要暂停阅读时，可以添加书签，下次再看时直接点击即可快速回到上次的阅读位置。</p><h3 id="1-7-数百种自定义组合功能"><a href="#1-7-数百种自定义组合功能" class="headerlink" title="1.7. 数百种自定义组合功能"></a>1.7. 数百种自定义组合功能</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/73620836.jpg" alt=""></p><p>可以看到，上面所实现的能都是通过设定功能所对应的触屏手势完成的。该 App 可以实现的功能包括：翻页、搜索、书签、配色、导航等 15 种事件。具体功能对应的操作包括：屏幕点击、滑动手势、手机物理按键等 21 种操作。</p><p>恩，也就是说，如果你喜欢追求极端个性化操作的话，App 为你提供了 300 多种方案，足以满足你对快捷键的癖好。</p><h2 id="2-下书"><a href="#2-下书" class="headerlink" title="2. 下书"></a>2. 下书</h2><p>要阅读，那就得有电子书。这里推荐两个非常棒的电子书下载 App：「搜书大师」 和 「小寻书」，利用它们基本都能找到你所需的书。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-11-2/61777342.jpg" alt=""></p><h2 id="3-导入"><a href="#3-导入" class="headerlink" title="3. 导入"></a>3. 导入</h2><p>下载好以后，定位到电子书所在位置，然后导入即可。另外，还可以设置过滤来选择导入的文件类型。</p><p>OK，还有很多好用的功能，这里就不再往下罗列了，感兴趣的话可以到官网进一步了解：</p><p><a href="https://www.moondownload.com/chinese.html" target="_blank" rel="noopener">https://www.moondownload.com/chinese.html</a></p><p>如果你想体验下这 <strong>3</strong> 款 App，可以在网上搜索下载或者在公众号后台回复<code>「阅读器」</code>得到。</p><p>本文完。</p><hr><p><strong>推荐阅读：</strong></p><p><a href="https://www.makcyun.top/fuli01.html">每周分享第1期：关于 PDF 处理软件，你需要的都在这里了</a></p><p><a href="https://www.makcyun.top/weekly_sharing2.html">每周分享第 2 期：一掏出手机，就暴露了程序猿身份</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;推荐一款最好用的手机阅读 App。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="佳软" scheme="https://www.makcyun.top/tags/%E4%BD%B3%E8%BD%AF/"/>
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
      <category term="APP" scheme="https://www.makcyun.top/tags/APP/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(8)：模拟登录方法汇总</title>
    <link href="https://www.makcyun.top/web_scraping_withpython8.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython8.html</id>
    <published>2018-10-20T08:16:24.000Z</published>
    <updated>2018-10-29T08:24:43.382Z</updated>
    
    <content type="html"><![CDATA[<p>对于很多要先登录的网站来说，模拟登录往往是爬虫的第一道坎。本文介绍 POST 请求登录、获取 Cookies 登录、Seleium 模拟登录三种方法。</p><a id="more"></a>  <p><strong>摘要：</strong> 在进行爬虫时，除了常见的不用登录就能爬取的网站，还有一类需要先登录的网站。比如豆瓣、知乎，以及上一篇文章中的桔子网。这一类网站又可以分为：只需输入帐号密码、除了帐号密码还需输入或点击验证码等类型。本文以只需输入账号密码就能登录的桔子网为例，介绍模拟登录常用的 3 种方法。</p><ul><li>POST 请求方法：需要在后台获取登录的 URL并填写请求体参数，然后 POST 请求登录，相对麻烦；</li><li>添加 Cookies 方法：先登录将获取到的 Cookies 加入 Headers 中，最后用 GET 方法请求登录，这种最为方便；</li><li>Selenium 模拟登录：代替手工操作，自动完成账号和密码的输入，简单但速度比较慢。</li></ul><p>下面，我们用代码分别实现上述 3 种方法。</p><h2 id="1-目标网页"><a href="#1-目标网页" class="headerlink" title="1. 目标网页"></a>1. 目标网页</h2><p>这是我们要获取内容的网页：</p><p><a href="http://radar.itjuzi.com/investevent" target="_blank" rel="noopener">http://radar.itjuzi.com/investevent</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-19/68214534.jpg" alt=""></p><p>这个网页需要先登录才能看到数据信息，登录界面如下：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-17/98135777.jpg" alt=""></p><p>可以看到，只需要输入账号和密码就可以登录，不用输验证码，比较简单。下面我们利用一个测试账号和密码，来实现模拟登录。</p><h2 id="2-POST-提交请求登录"><a href="#2-POST-提交请求登录" class="headerlink" title="2. POST 提交请求登录"></a>2. POST 提交请求登录</h2><p>首先，我们要找到 POST 请求的 URL。</p><p>有两种方法，第一种是在网页 devtools 查看请求，第二种是在 Fiddler 软件中查看。</p><p>先说第一种方法。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-29/47863090.jpg" alt=""></p><p>在登录界面输入账号密码，并打开开发者工具，清空所有请求，接着点击登录按钮，这时便会看到有大量请求产生。哪一个才是 POST 请求的 URL呢？这个需要一点经验，因为是登录，所以可以尝试点击带有 「login」字眼的请求。这里我们点击第四个请求，在右侧 Headers 中可以看到请求的 URL，请求方式是 POST类型，说明 URL 找对了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-29/84079751.jpg" alt=""></p><p>接着，我们下拉到 Form Data，这里有几个参数，包括 identify 和 password，这两个参数正是我们登录时需要输入的账号和密码，也就是 POST 请求需要携带的参数。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-29/10155339.jpg" alt=""></p><p>参数构造非常简单，接下来只需要利用 Requests.post 方法请求登录网站，然后就可以爬取内容了。</p><p>下面，我们尝试用 Fiddler 获取 POST 请求。</p><p>如果你对 Fiddler 还不太熟悉或者没有电脑上没有安装，可以先了解和安装一下。</p><blockquote><p>Fiddler 是位于客户端和服务器端的 HTTP 代理，也是目前最常用的 HTTP 抓包工具之一 。 它能够记录客户端和服务器之间的所有 HTTP 请求，可以针对特定的 HTTP 请求，分析请求数据、设置断点、调试 web 应用、修改请求的数据，甚至可以修改服务器返回的数据，功能非常强大，是 web 调试的利器。</p></blockquote><p>Fiddler 下载地址：</p><p><a href="https://www.telerik.com/download/fiddler" target="_blank" rel="noopener">https://www.telerik.com/download/fiddler</a></p><p>使用教程：</p><p><a href="https://zhuanlan.zhihu.com/p/37374178" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37374178</a></p><p><a href="http://www.hangge.com/blog/cache/detail_1697.html" target="_blank" rel="noopener">http://www.hangge.com/blog/cache/detail_1697.html</a></p><p>下面，我们就通过 Fiddler 截取登录请求。</p><p>当点击登录时，官场 Fiddler 页面，左侧可以看到抓取了大量请求。通过观察，第15个请求的 URL中含有「login」字段，很有可能是登录的 POST 请求。我们点击该请求，回到右侧，分别点击「inspectors」、「Headers」，可以看到就是 POST 请求，该 URL 和上面的方法获取的 URL 是一致的。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-29/35336276.jpg" alt=""></p><p>接着，切换到右侧的 Webforms 选项，可以看到 Body 请求体。也和上面方法中得到的一致。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-29/71870548.jpg" alt=""></p><p>获取到 URL 和请求体参数之后，下面就可以开始用 Requests.post 方法模拟登录了。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'identity'</span>:<span class="string">'irw27812@awsoo.com'</span>,   </span><br><span class="line">    <span class="string">'password'</span>:<span class="string">'test2018'</span>,</span><br><span class="line">&#125;</span><br><span class="line">url =<span class="string">'https://www.itjuzi.com/user/login?redirect=&amp;flag=&amp;radar_coupon='</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">session.post(url,headers = headers,data = data)</span><br><span class="line"><span class="comment"># 登录后，我们需要获取另一个网页中的内容</span></span><br><span class="line">response = session.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>,headers = headers)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>使用 session.post 方法提交登录请求，然后用 session.get 方法请求目标网页，并输出 HTML代码。可以看到，成功获取到了网页内容。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-19/19273353.jpg" alt=""></p><p>下面，介绍第 2 种方法。</p><h2 id="3-获取-Cookies，直接请求登录"><a href="#3-获取-Cookies，直接请求登录" class="headerlink" title="3. 获取 Cookies，直接请求登录"></a>3. 获取 Cookies，直接请求登录</h2><p>上面一种方法，我们需要去后台获取 POST 请求链接和参数，比较麻烦。下面，我们可以尝试先登录，获取 Cookie，然后将该 Cookie 添加到 Headers 中去，然后用 GET 方法请求即可，过程简单很多。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://www.itjuzi.com/user/login?redirect=&amp;flag=&amp;radar_coupon='</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">response = session.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>, headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>可以看到，添加了 Cookie 后就不用再 POST 请求了，直接 GET 请求目标网页即可。可以看到，也能成功获取到网页内容。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-19/5798761.jpg" alt=""></p><p>下面介绍第 3 种方法。</p><h2 id="4-Selenium-模拟登录"><a href="#4-Selenium-模拟登录" class="headerlink" title="4. Selenium 模拟登录"></a>4. Selenium 模拟登录</h2><p>这个方法很直接，利用 Selenium 代替手动方法去自动输入账号密码然后登录就行了。</p><p>关于 Selenium 的使用，在之前的一篇文章中有详细介绍，如果你不熟悉可以回顾一下：</p><p><a href="https://www.makcyun.top/web_scraping_withpython5.html">https://www.makcyun.top/web_scraping_withpython5.html</a></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.maximize_window()  <span class="comment"># 最大化窗口</span></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>) <span class="comment"># 等待加载10s</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">'https://www.itjuzi.com/user/login'</span>)</span><br><span class="line">    input = wait.until(EC.presence_of_element_located(</span><br><span class="line">        (By.XPATH, <span class="string">'//*[@id="create_account_email"]'</span>)))</span><br><span class="line">    input.send_keys(<span class="string">'irw27812@awsoo.com'</span>)</span><br><span class="line">    input = wait.until(EC.presence_of_element_located(</span><br><span class="line">        (By.XPATH, <span class="string">'//*[@id="create_account_password"]'</span>)))</span><br><span class="line">    input.send_keys(<span class="string">'test2018'</span>)</span><br><span class="line">    submit = wait.until(EC.element_to_be_clickable(</span><br><span class="line">        (By.XPATH, <span class="string">'//*[@id="login_btn"]'</span>)))</span><br><span class="line">    submit.click() <span class="comment"># 点击登录按钮</span></span><br><span class="line">    get_page_index()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_index</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(browser.page_source)  <span class="comment"># 输出网页源码</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(str(e))</span><br><span class="line">login()</span><br></pre></td></tr></table></figure><p>这里，我们在网页中首先定位了账号节点位置：<code>&#39;//*[@id=&quot;create_account_email&quot;]&#39;</code>，然后用 input.send_keys 方法输入账号，同理，定位了密码框位置并输入了密码。接着定位 <strong>登录</strong> 按钮的位置：<code>//*[@id=&quot;login_btn&quot;]</code>，然后用 submit.click() 方法实现点击登录按钮操作，从而完成登录。可以看到，也能成功获取到网页内容。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-19/8322177.jpg" alt=""></p><p>以上就是模拟需登录网站的几种方法。当登录进去后，就可以开始爬取所需内容了。</p><h2 id="5-总结："><a href="#5-总结：" class="headerlink" title="5. 总结："></a>5. 总结：</h2><ul><li>本文分别实现了模拟登录的 3 种操作方法，建议优先选择第 2 种，即先获取 Cookies 再 Get 请求直接登录的方法。</li><li>本文模拟登录的网站，仅需输入账号密码，不需要获取相关加密参数，比如 Authenticity_token ，同时也无需输入验证码，所以方法比较简单。但是还有很多网站模拟登录时，需要处理加密参数、验证码输入等问题。后续将会介绍。</li></ul><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython5.html">Python爬虫(5)：Selenium 爬取东方财富网股票财务报表</a></p><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于很多要先登录的网站来说，模拟登录往往是爬虫的第一道坎。本文介绍 POST 请求登录、获取 Cookies 登录、Seleium 模拟登录三种方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="模拟登录" scheme="https://www.makcyun.top/tags/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(7)：IT桔子网，国内创业公司的信息都在这里了</title>
    <link href="https://www.makcyun.top/web_scraping_withpython7.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython7.html</id>
    <published>2018-10-17T08:16:24.000Z</published>
    <updated>2018-10-23T12:58:52.421Z</updated>
    
    <content type="html"><![CDATA[<p>以 IT 桔子网为例，介绍需登录网站的爬取方法。爬取该网站数据库中的信息：创业公司投融资情况、投资机构信息、独角兽公司。</p><a id="more"></a>  <p><strong>摘要：</strong> 之前爬的网站都是不需要登录就可以爬取的，但还有很多网站的内容需要先登录才能爬，比如 IT 桔子、豆瓣、知乎等。这时采用之前的方法就不行了，需要先登录再去爬。本文以 IT 桔子网站为例，介绍模拟登录方法，然后爬取该网站数据库中的数据信息，并保存到 MongoDB 数据库中。</p><h2 id="1-网站介绍"><a href="#1-网站介绍" class="headerlink" title="1. 网站介绍"></a>1. 网站介绍</h2><p>网址：<a href="https://www.itjuzi.com/" target="_blank" rel="noopener">https://www.itjuzi.com/</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-17/90303578.jpg" alt=""></p><p>对于创投圈的人来说，国内的<code>IT桔子</code>和国外的 <code>Crunchbase</code>应该算是必备网站。今天，我们要说的就是IT桔子，这个网站提供了很多有价值的信息。</p><p>信息的价值体现在哪里呢，举个简单的例子。你运营了一个不错的公众号，有着 10 万粉丝群，这时候你想找投资继续做大，但你不知道该到哪里去找投资。这时候你偶然发现了IT桔子，在网站上你看到了同领域的大 V 号信息，他们得到了好几家公司上千万的投资。你看着心生羡慕，也跃跃欲试。于是，你经过综合对比分析，对自己公众号估值 200 万，然后就去找投资大 V 号的那几家公司洽谈。由于目的性明确，准备也充分，你很快就得到了融资。</p><p>这个例子中，IT 桔子提供了创业公司（运营公众号也是创业）融资金额和投资机构等相关宝贵的信息。当然，这只是非常小的一点价值，该网站数据库提供了自 2000 年以来的海量数据，包括：超过11 万家的创业公司、6 万多条投融资信息、7 千多家投资机构以及其他数据，可以说是非常全了。这些数据的背后蕴含着大量有价值的信息。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-20/32908700.jpg" alt=""></p><p>接下来，本文尝试爬取该网站数据库的信息，之后做一些数据分析，尝试找到一些有意思的信息。</p><p><strong>主要内容：</strong></p><ul><li>模拟登录</li><li>分析 Ajax 然后抓取</li><li>存储到 MongoDB 数据库</li><li>导出 csv</li><li>数据分析(后期)</li></ul><h2 id="2-模拟登录"><a href="#2-模拟登录" class="headerlink" title="2. 模拟登录"></a>2. 模拟登录</h2><h3 id="2-1-Session-和-Cookies"><a href="#2-1-Session-和-Cookies" class="headerlink" title="2.1. Session 和 Cookies"></a>2.1. Session 和 Cookies</h3><p>观察这个网站，是需要先登录才能看到数据信息的，但是好在不用输验证码。我们需要利用账号和密码，然后实现模拟登录。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-17/98135777.jpg" alt=""></p><p>模拟登录的方法有好几种，比如 Post 直接提交账号、先登录获取 Cookies 再直接请求、Selenium 模拟登录等。其中：</p><ul><li><p>Post 方法需要在后台获取登录的 url，填写表单参数然后再请求，比较麻烦；</p></li><li><p>直接复制 Cookies 的方法就是先登录账号，复制出 Cookies 并添加到 Headers 中，再用 <strong>requests.get</strong> 方法提交请求，这种方法最为方便；</p></li><li><p>Selenium 模拟登录方法是直接输入账号、密码，也比较方便，但速度会有点慢。</p></li></ul><p>之后，会单独介绍几种方法的具体实现的步骤。这里，我们就先采用第二种方法。</p><p>首先，需要先了解两个知识点：Session 和 Cookies。</p><blockquote><p>Session 在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。所以我们可以理解为 Cookies 里面保存了登录的凭证，有了它我们只需要在下次请求携带 Cookies 发送 Request 而不必重新输入用户名、密码等信息重新登录了。<br>因此在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 Cookies 放在 Request Headers 里面直接请求。</p></blockquote><p>更多知识，可以参考崔庆才大神的文章：</p><p><a href="https://germey.gitbooks.io/python3webspider/content/2.4-Session%E5%92%8CCookies.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/2.4-Session%E5%92%8CCookies.html</a></p><p>在了解 Cookies 知识后，我们就可以进入正题了。</p><h3 id="2-2-Requests-请求登录"><a href="#2-2-Requests-请求登录" class="headerlink" title="2.2. Requests 请求登录"></a>2.2. Requests 请求登录</h3><p>首先，利用已有的账号和密码，登录进网站主页，然后右键-检查，打开第一个 <code>www.itjuzi.com</code> 请求：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/84996738.jpg" alt=""></p><p>向下拉到 Requests Headers 选项，可以看到有很多字段信息，这些信息之后我们都要添加到 Get 请求中去。</p><p>定位到下方的 Cookie 字段，可以看到有很多 Cookie 值和名称，这是在登录后自动产生的。我们将整个 Cookies 复制 Request Headers 里，然后请求网页就可以顺利登陆然后爬取。如果不加 Cookies，那么就卡在登录界面，也就无法进行后面的爬取，所以 Cookies 很重要，需要把它放到 Request Headers 里去。</p><p>下面，我们按照 json 格式开始构造 Request Headers。这里推荐一个好用的网站，可以帮我们自动构造 Request Headers：<a href="https://curl.trillworks.com/" target="_blank" rel="noopener">https://curl.trillworks.com/</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/58555393.jpg" alt=""></p><p>使用方法也很简单，右键复制<code>cURL链接</code>到这个网页中。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-19/77493357.jpg" alt=""></p><p>将 cURL 复制到左边选框，默认选择语言为 Python，然后右侧就会自动构造后 requests 请求，包括 headers，复制下来直接可以用。登录好以后，我们就转到投融资速递网页中（url：<a href="http://radar.itjuzi.com/investevent" target="_blank" rel="noopener">http://radar.itjuzi.com/investevent</a>），然后就可以获取该页面网页内容了。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">    <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">    <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>,</span><br><span class="line">    <span class="string">'DNT'</span>: <span class="string">'1'</span>,</span><br><span class="line">    <span class="string">'Referer'</span>: <span class="string">'http://radar.itjuzi.com/investevent'</span>,</span><br><span class="line">    <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">    <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7'</span>,</span><br><span class="line">    <span class="string">'If-None-Match'</span>: <span class="string">'W/^\\^5bc7df15-19cdc^\\^'</span>,</span><br><span class="line">    <span class="string">'If-Modified-Since'</span>: <span class="string">'Thu, 18 Oct 2018 01:17:09 GMT'</span>,</span><br><span class="line">    <span class="comment"># 主页cookie</span></span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://radar.itjuzi.com/investevent'</span>   <span class="comment"># 投融资信息</span></span><br><span class="line">s = requests.Session()</span><br><span class="line">response = s.get(url,headers = headers)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/21995265.jpg" alt=""></p><p>可以看到，添加 Cookie 后，我们请求投融资信息网页就成功了。这里如果不加 Cookie 的结果就什么也得不到：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/26869412.jpg" alt=""></p><p>好，这样就算成功登录了。但是整个 headers 请求头的参数太多，是否一定需要带这么多参数呢？ 这里就去尝试看哪些参数是请求一定要的，哪些则是不用的，不用的可以去掉以精简代码。经过尝试，仅需要下面三个参数就能请求成功。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">    <span class="comment"># 主页cookie</span></span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'复制你的cookie'</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>Tips：爬取失效的时候需要重新注册帐号，然后生成新的 Cookie</p><p>如果你没那么多邮箱账号，那么推荐一个可生成随机账号的免费邮箱，用来接收注册激活链接：</p><p><a href="https://10minutemail.net/" target="_blank" rel="noopener">https://10minutemail.net/</a></p></blockquote><h2 id="3-网页爬取分析"><a href="#3-网页爬取分析" class="headerlink" title="3. 网页爬取分析"></a>3. 网页爬取分析</h2><p>在成功登录以后，我们就可以通过分析网页结构来采取相应的爬取方法。这里，我们将爬取投融资速递、创业公司、投资机构和千里马等几个子板块的数据。首先，以投融资速递信息为例。</p><p>网址：<a href="http://radar.itjuzi.com/investevent" target="_blank" rel="noopener">http://radar.itjuzi.com/investevent</a></p><h3 id="3-1-分析网页"><a href="#3-1-分析网页" class="headerlink" title="3.1. 分析网页"></a>3.1. 分析网页</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/54988425.jpg" alt=""></p><p>可以看到，投融资事件信息网页中的数据是表格形式。经尝试点击翻页，发现url不变，可以初步判定网页数据采用 Ajax 形式呈现。切换到后台，点击翻页可以发现出现有规律的 <code>info?locatiop 开头的请求</code>，页数随 page 参数而发生规律的变化。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/23929665.jpg" alt=""></p><p>点击请求查分别查看 Response 和 Preview，发现表格数据是 json 格式，这就太好了。因为 json 格式的数据非常整齐也很容易抓取。上一篇文章，我们正好解决了 json 格式数据的处理方法，如果不太熟悉可以回顾一下：</p><p><a href="https://www.makcyun.top/web_scraping_withpython6.html">https://www.makcyun.top/web_scraping_withpython6.html</a></p><p>接着，我们就可以通过构造 url 参数，然后用 Get 请求就可以获取网页中的表格数据，最后再加个循环，就可以爬取多页数据了。</p><h3 id="3-2-构造-url"><a href="#3-2-构造-url" class="headerlink" title="3.2. 构造 url"></a>3.2. 构造 url</h3><p>下面，我们来构造一下 url，切换到 Headers 选项卡，拉到最底部可以看到 url 需带的请求参数。这里有三项，很好理解。location：in，表示国内数据； orderby：def，表示默认排序；page：1，表示第一页。所以，只需要改变 page 参数就可以查看其他页的结果，非常简单。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/23739456.jpg" alt=""></p><p>这里，如果我们对表格进行筛选，比如行业选择教育、时间选择 2018 年，那么相应的请求参数也会增加。通过构造参数就可以爬取指定的数据，这样就不用全部爬下来了，也对网站友好点。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/96770843.jpg" alt=""></p><h3 id="3-3-爬取数据"><a href="#3-3-爬取数据" class="headerlink" title="3.3. 爬取数据"></a>3.3. 爬取数据</h3><p>到这儿，我们就可以直接开始爬了。可以使用函数，也可以用定义类（Class）的方法。考虑到，Python 是一种面向对象的编程，类（Class）是面向对象最重要的概念之一，运用类的思想编程非常重要。所以，这里我们尝试采用类的方法来实现爬虫。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ITjuzi</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: ua.random,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">            <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.url = <span class="string">'http://radar.itjuzi.com/investevent/info?'</span>    <span class="comment"># investevent</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(self, page)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        1 获取投融资事件数据</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;                <span class="comment"># invsestevent</span></span><br><span class="line">            <span class="string">'location'</span>: <span class="string">'in'</span>,</span><br><span class="line">            <span class="string">'orderby'</span>: <span class="string">'def'</span>,</span><br><span class="line">            <span class="string">'page'</span>: page,</span><br><span class="line">            <span class="string">'date'</span>: <span class="number">2018</span>  <span class="comment"># 年份</span></span><br><span class="line">        &#125;</span><br><span class="line">        response = self.session.get(</span><br><span class="line">            self.url, params=params, headers=self.headers).json()</span><br><span class="line">        print(response)</span><br><span class="line">        <span class="comment"># self.save_to_mongo(response)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = itjuzi()</span><br><span class="line">    spider.get_table(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如果你之前一直是用 Def 函数的写法，而没有接触过 Class 类的写法，可能会看地比较别扭，我之前就是这样的，搞不懂<code>为什么要有 self</code>、<code>为什么用__init__</code>。这种思维的转变可以通过看教程和别人写的实际案例去揣摩。这里，我先略过，之后会单独介绍。</p><p>简单解释一下上面代码的意思。首先定义了一个类（class），类名是 <strong>ITjuzi</strong>，类名通常是大写开头的单词。后面的 <strong>(object)</strong>  表示该类是从哪个类继承下来的，这个可以暂时不用管，填上就可以。然后定义了一个特殊的<code>__init__</code>方法，<code>__init__</code>方法的第一个参数永远是 <code>self</code>，之后是其他想要设置的属性参数。在这个方法里可以绑定些确定而且必须的属性，比如 headers、url 等。</p><p>在 headers 里面，User-Agent 没有使用固定的 UA，而是借助 <code>fake_useragent包</code> 生成随机的 UA：<code>ua.random</code>。因为，这个网站做了一定的反爬措施，这样可以起到一定的反爬效果，后续我们会再说到。接着，定义了一个 get_table() 函数，这个函数和普通函数没有什么区别，除了第一个参数永远是实例变量 <code>self</code>。在 session.get（）方法中传入 url、请求参数和 headers，请求网页并指定获取的数据类型为 json 格式，然后就可以顺利输出 2018 年投融资信息的第 1 页数据：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/606314.jpg" alt=""></p><h3 id="3-4-存储到-MongoDB"><a href="#3-4-存储到-MongoDB" class="headerlink" title="3.4. 存储到 MongoDB"></a>3.4. 存储到 MongoDB</h3><p>数据爬取下来了，那么我们放到哪里呢？可以选择存储到 csv 中，但 json 数据中存在多层嵌套，csv 不能够直观展现。这里，我们可以尝试之前没有用过的 MongoDB 数据库，当作练习。另外，数据存储到该数据库中，后期也可以导出 csv，一举两得。</p><p>关于 MongoDB 的安装与基本使用，可以参考下面这两篇教程，之后我也会单独再写一下：</p><blockquote><p>安装</p><p><a href="https://germey.gitbooks.io/python3webspider/content/1.4.2-MongoDB%E7%9A%84%E5%AE%89%E8%A3%85.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/1.4.2-MongoDB%E7%9A%84%E5%AE%89%E8%A3%85.html</a></p><p>使用</p><p><a href="https://germey.gitbooks.io/python3webspider/content/5.3.1-MongoDB%E5%AD%98%E5%82%A8.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/5.3.1-MongoDB%E5%AD%98%E5%82%A8.html</a></p><p>可视化工具可采用 Robo 3T (之前叫 RoboMongo)</p><p><a href="https://www.mongodb.com/" target="_blank" rel="noopener">https://www.mongodb.com/</a></p></blockquote><p>下面我们就将上面返回的 json 数据，存储到 MongoDB 中去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># mongodb数据库初始化</span></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line"><span class="comment"># 指定数据库</span></span><br><span class="line">db = client.ITjuzi</span><br><span class="line"><span class="comment"># 指定集合，类似于mysql中的表</span></span><br><span class="line">mongo_collection1 = db.itjuzi_investevent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = response[<span class="string">'data'</span>][<span class="string">'rows'</span>]  <span class="comment"># dict可以连续选取字典层内的内容</span></span><br><span class="line">            df = pd.DataFrame(data)</span><br><span class="line">            table = json.loads(df.T.to_json()).values()</span><br><span class="line">            <span class="keyword">if</span> mongo_collection1.insert_many(table):  <span class="comment"># investment</span></span><br><span class="line">                print(<span class="string">'存储到mongodb成功'</span>)</span><br><span class="line">                sleep = np.random.randint(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">            time.sleep(sleep)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            print(<span class="string">'存储到mongodb失败'</span>)</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">spider_itjuzi</span><span class="params">(self, start_page, end_page)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">            print(<span class="string">'下载第%s页:'</span> % (page))</span><br><span class="line">            self.get_table(page)</span><br><span class="line">        print(<span class="string">'下载完成'</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = ITjuzi()</span><br><span class="line">    spider.spider_itjuzi(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>这里，安装好 MongoingDB 数据库、Robo 3T 和 pymongo 库后，我们就可以开始使用了。</p><p>首先，对数据库进行初始化，然后指定（如果没有则生成）数据将要存放的数据库和集合名称。接着，定义了<strong>save_to_mongo </strong>函数。由于表格里面的数据存储在键为 <strong>rows</strong> 的 value 值中，可使用 <code>response[&#39;data&#39;][&#39;rows&#39;]</code> 来获取到 json 里面的嵌套数据，然后转为 DataFrame。DataFrame 存储 MongoDB 参考了 <a href="https://stackoverflow.com/questions/20167194/insert-a-pandas-dataframe-into-mongodb-using-pymongo" target="_blank" rel="noopener">stackoverflow</a> 上面的一个回答：<strong>json.loads(df.T.to_json()).values()</strong>。</p><p>然后，使用 <strong>mongo_collection1.insert_many(table)</strong> 方法将数据插入到 mongo_collection1，也就是 itjuzi_investevent 集合中。爬取完一页数据后，设置随机延时 3-6 s，避免爬取太频繁，这也能起到一定的反爬作用。</p><p>最后，我们定义一个分页循环爬取函数 <strong>spider_itjuzi</strong>，利用 for 循环设置爬取起始页数就可以了，爬取结果如下：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/32245012.jpg" alt=""></p><p>打开 Robo 3T，可以看到数据成功存储到 MongoDB 中了：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-18/48807142.jpg" alt=""></p><p>好，以上，我们就基本上完成了 2018 年投融资信息数据表的爬取，如果你想爬其他年份或者更多页的数据，更改相应的参数即可。</p><h3 id="3-5-导出到-csv"><a href="#3-5-导出到-csv" class="headerlink" title="3.5. 导出到 csv"></a>3.5. 导出到 csv</h3><p>数据存好后，如果还不太熟悉 MongoDB 的对数据的操作，那么我们可以将数据导出为 csv，在 excel 中操作。MongoDB不能直接导出 csv，但操作起来也不麻烦，利用<code>mongoexport</code>命令，几行代码就可以输出 csv。</p><p><code>mongoexport</code>导出 csv 的方法：</p><p><a href="https://docs.mongodb.com/manual/reference/program/mongoexport/#mongoexport-fields-example" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/program/mongoexport/#mongoexport-fields-example</a></p><p>首先，运行 cmd，切换路径到 MongoDB 安装文件夹中的 bin 目录下，我这里是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd C:\Program Files\MongoDB\Server\<span class="number">4.0</span>\bin</span><br></pre></td></tr></table></figure><p>接着，在桌面新建一个txt文件，命名为<code>fields</code>，在里面输入我们需要输出的表格列名，如下所示：</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fv3SDvCH-YELedOEbTOWjyIIbzNW" alt=""></p><p>然后，利用<code>mongoexport</code>命令，按照：表格所在的数据库、集合、输出格式、导出列名文件位置和输出文件名的格式，编写好命令并运行就可以导出了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongoexport --db ITjuzi --collection itjuzi_investevent --type=csv --fieldFile C:\Users\sony\Desktop\fields.txt --out C:\Users\sony\Desktop\investevent.csv</span><br></pre></td></tr></table></figure><p>cmd 命令：</p><p><img src="http://pbscl931v.bkt.clouddn.com/FhWiQ9majkIgqMWDKAnAr-dij5xw" alt=""></p><p>导出 csv 结果如下：<br><img src="http://pbscl931v.bkt.clouddn.com/FhoU_6rggZJ0foJcRnfayTGxS0u4" alt=""></p><p><strong>Tips：直接用 excel 打开可能会是乱码，需先用 Notepad++ 转换为 UTF-8 编码，然后 excel 再打开就正常了。</strong></p><p>以上，我们就完成了整个数据表的爬取。</p><h3 id="3-6-完整代码"><a href="#3-6-完整代码" class="headerlink" title="3.6. 完整代码"></a>3.6. 完整代码</h3><p>下面，可以再尝试爬取创业公司、投资机构和千里马的数据。他们的数据结构形式是一样的，只需要更换相应的参数就可以了，感兴趣的话可以尝试下。将上面的代码再稍微整理一下，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="keyword">import</span> socket  <span class="comment"># 断线重试</span></span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="comment"># 随机ua</span></span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># mongodb数据库初始化</span></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line"><span class="comment"># 获得数据库</span></span><br><span class="line">db = client.ITjuzi</span><br><span class="line"><span class="comment"># 获得集合</span></span><br><span class="line">mongo_collection1 = db.itjuzi_investevent</span><br><span class="line">mongo_collection2 = db.itjuzi_company</span><br><span class="line">mongo_collection3 = db.itjuzi_investment</span><br><span class="line">mongo_collection4 = db.itjuzi_horse</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">itjuzi</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: ua.random,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">            <span class="comment"># 主页cookie</span></span><br><span class="line">            <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.url = <span class="string">'http://radar.itjuzi.com/investevent/info?'</span>    <span class="comment"># investevent</span></span><br><span class="line">        <span class="comment"># self.url = 'http://radar.itjuzi.com/company/infonew?'       # company</span></span><br><span class="line">        <span class="comment"># self.url = 'http://radar.itjuzi.com/investment/info?'       # investment</span></span><br><span class="line">        <span class="comment"># self.url = 'https://www.itjuzi.com/horse'               # horse</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(self, page)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        1 获取投融资事件网页内容</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;                <span class="comment"># 1 invsestevent</span></span><br><span class="line">        <span class="string">'location'</span>: <span class="string">'in'</span>,</span><br><span class="line">        <span class="string">'orderby'</span>: <span class="string">'def'</span>,</span><br><span class="line">        <span class="string">'page'</span>: page,</span><br><span class="line">        <span class="string">'date'</span>:<span class="number">2018</span>  <span class="comment">#年份  </span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # #</span></span><br><span class="line">        <span class="comment"># params = &#123;                  # 2 company</span></span><br><span class="line">        <span class="comment">#     'page': page,</span></span><br><span class="line">        <span class="comment">#     # 'scope[]': 1,  # 行业 1教育</span></span><br><span class="line">        <span class="comment">#     'orderby': 'pv',</span></span><br><span class="line">        <span class="comment">#     'born_year[]': 2018,  # 只能单年，不能多年筛选，会保留最后一个</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">        <span class="comment"># # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # #</span></span><br><span class="line">        <span class="comment"># params = &#123;                  # 3 investment</span></span><br><span class="line">        <span class="comment"># 'orderby': 'num',</span></span><br><span class="line">        <span class="comment"># 'page': page</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">        <span class="comment"># # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # #</span></span><br><span class="line">        <span class="comment"># params = &#123;                  # 4 horse</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">        <span class="comment"># 可能会遇到请求失败，则设置3次重新请求</span></span><br><span class="line">        retrytimes = <span class="number">3</span></span><br><span class="line">        <span class="keyword">while</span> retrytimes:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                response = self.session.get(</span><br><span class="line">                    self.url, params=params, headers=self.headers,timeout = (<span class="number">5</span>,<span class="number">20</span>)).json()</span><br><span class="line">                self.save_to_mongo(response)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> socket.timeout:</span><br><span class="line">                print(<span class="string">'下载第&#123;&#125;页，第&#123;&#125;次网页请求超时'</span> .format(page,retrytimes))</span><br><span class="line">                retrytimes -=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = response[<span class="string">'data'</span>][<span class="string">'rows'</span>]  <span class="comment"># dict可以连续选取字典层内的内容</span></span><br><span class="line">            <span class="comment"># data =response  # 爬取千里马时需替换为此data</span></span><br><span class="line">            df = pd.DataFrame(data)</span><br><span class="line">            table = json.loads(df.T.to_json()).values()</span><br><span class="line">            <span class="keyword">if</span> mongo_collection1.insert_many(table):      <span class="comment"># investevent</span></span><br><span class="line">            <span class="comment"># if mongo_collection2.insert_many(table):    # company</span></span><br><span class="line">            <span class="comment"># if mongo_collection3.insert_many(table):    # investment</span></span><br><span class="line">            <span class="comment"># if mongo_collection4.insert_many(table):    # horse </span></span><br><span class="line">                print(<span class="string">'存储到mongodb成功'</span>)</span><br><span class="line">                sleep = np.random.randint(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">                time.sleep(sleep)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            print(<span class="string">'存储到mongodb失败'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_itjuzi</span><span class="params">(self, start_page, end_page)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">            print(<span class="string">'下载第%s页:'</span> % (page))</span><br><span class="line">            self.get_table(page)</span><br><span class="line">        print(<span class="string">'下载完成'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = itjuzi()</span><br><span class="line">    spider.spider_itjuzi(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>源代码也可以在公众号后台回复：「<strong>it桔子</strong>」，或者在下面的链接中获取：</p><p><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><p>后续文章，我们将对这些数据进行分析，尝试从中找出一些有意思的发现。</p><h2 id="4-总结："><a href="#4-总结：" class="headerlink" title="4. 总结："></a>4. 总结：</h2><ul><li>本文以 IT 桔子网为例，介绍了需登录网站的爬取方法。即：先模拟登录再爬取数据信息。但是还有一些网站登录时需要输入验证码，这让爬取难度又增加，后期会再进行介绍。</li><li>IT 桔子相比之前的爬虫网站，反爬措施高了很多。本文通过设置随机延时、随机 UserAgent，可一定程度上增加爬虫的稳定性。但是仍然会受到反爬措施的限制，后期可尝试通过设置 IP 代理池进一步提升爬虫效率。</li><li>上面的爬虫程序在爬取过程容易中断，接着再进行爬取即可。但是手动修改非常不方便，也容易造成数据重复爬取或者漏爬。所以，为了更完整地爬取，需增加断点续传的功能。</li></ul><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎扫一扫关注我的微信公众号</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以 IT 桔子网为例，介绍需登录网站的爬取方法。爬取该网站数据库中的信息：创业公司投融资情况、投资机构信息、独角兽公司。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="模拟登录" scheme="https://www.makcyun.top/tags/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/"/>
    
      <category term="MongoDB" scheme="https://www.makcyun.top/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(6)：50 行代码爬取东方财富网上市公司 10 年近百万行财务报表数据</title>
    <link href="https://www.makcyun.top/web_scraping_withpython6.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython6.html</id>
    <published>2018-10-12T08:16:24.000Z</published>
    <updated>2018-10-21T06:29:59.805Z</updated>
    
    <content type="html"><![CDATA[<p>通过分析网址 JavaScript 请求，以比 Selenium 快 100 倍的方法，快速爬取东方财富网各上市公司历年的财务报表数据。</p><a id="more"></a>  <p><strong>摘要：</strong> 上一篇文章，我们用Selenium成功爬取了东方财富网的财务报表数据，但是速度非常慢，爬取 70 页需要好几十分钟。为了加快速度，本文分析网页JavaScript请求，找到数据接口然后快速爬取财务报表数据。</p><h2 id="1-JavaScript请求分析"><a href="#1-JavaScript请求分析" class="headerlink" title="1. JavaScript请求分析"></a>1. JavaScript请求分析</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-4/90719379.jpg" alt=""></p><p>上一篇文章，我们简单分了东方财富网财务报表网页后台的js请求，文章回顾：（<a href="https://www.makcyun.top/web_scraping_withpython5.html">https://www.makcyun.top/web_scraping_withpython5.html</a>）</p><p>接下来，我们深入分析。首先，点击报表底部的下一页，然后观察左侧<strong>Name</strong>列，看会弹出什么新的请求来：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-5/136165.jpg" alt=""></p><p>可以看到，当不断点击下一页时，会相应弹出以<strong>get？type</strong>开头的请求。点击右边Headers选项卡，可以看到请求的URL，网址非常长，先不管它，后续我们会分析各项参数。接着，点击右侧的Preview和Response，可以看到里面有很多整齐的数据，尝试猜测这可能是财务报表中的数据，经过和表格进行对比，发现这正是我们所需的数据，太好了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fpsj517KOZ63Z-9vrzIYjgLyfS80" alt=""></p><p>然后将URL复制到新链接中打开看看，可以看到表格中的数据完美地显示出来了。竟然不用添加Headers、UA去请求就能获取到，看来东方财富网很大方啊。</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fjn2ZLvaUCdam1U-2RDk_N17zGOy" alt=""></p><p>到这里，爬取思路已经很清晰了。首先，用Request请求该URL，将获取到的数据进行正则匹配，将数据转变为json格式，然后写入本地文件，最后再加一个分页循环爬取就OK了。这比之前的Selenium要简单很多，而且速度应该会快很多倍。下面我们就先来尝试爬一页数据看看。</p><h2 id="2-爬取单页"><a href="#2-爬取单页" class="headerlink" title="2. 爬取单页"></a>2. 爬取单页</h2><h3 id="2-1-抓取分析"><a href="#2-1-抓取分析" class="headerlink" title="2.1. 抓取分析"></a>2.1. 抓取分析</h3><p>这里仍然以<code>2018年中报的利润表</code>为例，抓取该网页的第一页表格数据，网页url为：<a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a></p><p>表格第一页的js请求的url为：<a href="http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?type=CWBB_LRB&amp;token=70f12f2f4f091e459a279469fe49eca5&amp;st=noticedate&amp;sr=-1&amp;p=2&amp;ps=50&amp;js=var%20spmVUpAF={pages:(tp" target="_blank" rel="noopener">http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?type=CWBB_LRB&amp;token=70f12f2f4f091e459a279469fe49eca5&amp;st=noticedate&amp;sr=-1&amp;p=2&amp;ps=50&amp;js=var%20spmVUpAF={pages:(tp),data:%20(x)}&amp;filter=(reportdate=^2018-06-30^)&amp;rt=51312886</a>,data:%20(x)}&amp;filter=(reportdate=^2018-06-30^)&amp;rt=51312886)</p><p>下面，我们通过分析该url，来抓取表格内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">()</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'type'</span>: <span class="string">'CWBB_LRB'</span>,  <span class="comment"># 表格类型,LRB为利润表缩写，必须</span></span><br><span class="line">        <span class="string">'token'</span>: <span class="string">'70f12f2f4f091e459a279469fe49eca5'</span>,  <span class="comment"># 访问令牌，必须</span></span><br><span class="line">        <span class="string">'st'</span>: <span class="string">'noticedate'</span>,  <span class="comment"># 公告日期</span></span><br><span class="line">        <span class="string">'sr'</span>: <span class="number">-1</span>,  <span class="comment"># 保持-1不用改动即可</span></span><br><span class="line">        <span class="string">'p'</span>: <span class="number">1</span>,  <span class="comment"># 表格页数</span></span><br><span class="line">        <span class="string">'ps'</span>: <span class="number">50</span>,  <span class="comment"># 每页显示多少条信息</span></span><br><span class="line">        <span class="string">'js'</span>: <span class="string">'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;'</span>,  <span class="comment"># js函数，必须</span></span><br><span class="line">        <span class="string">'filter'</span>: <span class="string">'(reportdate=^2018-06-30^)'</span>,  <span class="comment"># 筛选条件</span></span><br><span class="line">        <span class="comment"># 'rt': 51294261  可不用</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?'</span></span><br><span class="line">    response = requests.get(url, params=params).text</span><br><span class="line">    print(response)</span><br><span class="line">get_table()</span><br></pre></td></tr></table></figure><p>这里我们定义了一个get_table()方法，来输出抓取的第一页表格内容。params为url请求中所包含的参数。</p><p>这里对重要参数进行简单说明：<strong>type</strong>为7个表格的类型说明，将type拆成两部分：’CWBB_‘ 和’LRB’，资产负债表等后3个表是以’CWBB_’ 开头，业绩报表至预约披露时间表等前4个表是以’YJBB20_‘开头的；’LRB’为利润表的首字母缩写，同理业绩报表则为’YJBB’。所以，如果要爬取不同的表格，就需要更改type参数。’filter’为表格筛选参数，这里筛选出年中报的数据。不同的表格筛选条件会不一样，所以当type类型更改的时候，也要相应修改filter类型。</p><p>params参数设置好之后，将url和params参数一起传进requests.get()方法中，这样就构造好了请求连接。几行代码就可以成功获取网页第一页的表格数据了：</p><p><img src="http://pbscl931v.bkt.clouddn.com/FiX9K5Z5Y_Hue7c16dZaXZ_IpQ5O" alt=""></p><p>可以看到，表格信息存储在LFtlXDqn变量中，pages表示表格有72页。data为表格数据，是一个由多个字典构成的列表，每个字典是表格的一行数据。我们可以通过正则表达式分别提取出pages和data数据。</p><h3 id="2-2-正则表达式提取表格"><a href="#2-2-正则表达式提取表格" class="headerlink" title="2.2. 正则表达式提取表格"></a>2.2. 正则表达式提取表格</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定页数</span></span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line">pat = re.compile(<span class="string">'var.*?&#123;pages:(\d+),data:.*?'</span>)</span><br><span class="line">page_all = re.search(pat, response)</span><br><span class="line">print(page_all.group(<span class="number">1</span>))</span><br><span class="line">结果：</span><br><span class="line"><span class="number">72</span></span><br></pre></td></tr></table></figure><p>这里用<code>\d+</code>匹配页数中的数值，然后用re.search()方法提取出来。group(1)表示输出第一个结果，这里就是()中的页数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取出list，可以使用json.dumps和json.loads</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">pattern = re.compile(<span class="string">'var.*?data: (.*)&#125;'</span>, re.S)</span><br><span class="line">items = re.search(pattern, response)</span><br><span class="line">data = items.group(<span class="number">1</span>)</span><br><span class="line">print(data)</span><br><span class="line">print(type(data))</span><br><span class="line">结果如下：</span><br><span class="line">[&#123;<span class="string">'scode'</span>: <span class="string">'600478'</span>, <span class="string">'hycode'</span>: <span class="string">'016040'</span>, <span class="string">'companycode'</span>: <span class="string">'10001305'</span>, <span class="string">'sname'</span>: <span class="string">'科力远'</span>, <span class="string">'publishname'</span>: <span class="string">'材料行业'</span>...</span><br><span class="line"><span class="string">'sjltz'</span>: <span class="number">10.466665</span>, <span class="string">'kcfjcxsyjlr'</span>: <span class="number">46691230.93</span>, <span class="string">'sjlktz'</span>: <span class="number">10.4666649042</span>, <span class="string">'eutime'</span>: <span class="string">'2018/9/6 20:18:42'</span>, <span class="string">'yyzc'</span>: <span class="number">14238766.31</span>&#125;]</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br></pre></td></tr></table></figure><p>这里在匹配表格数据用了<code>(.*)</code>表示贪婪匹配，因为data中有很多个字典，每个字典都是以’}’结尾，所以我们利用贪婪匹配到最后一个’}’，这样才能获取data所有数据。多数情况下，我们可能会用到(.*?)，这表示非贪婪匹配，意味着之多匹配一个’}’，这样的话，我们只能匹配到第一行数据，显然是不对的。</p><h3 id="2-3-json-loads-输出表格"><a href="#2-3-json-loads-输出表格" class="headerlink" title="2.3. json.loads()输出表格"></a>2.3. json.loads()输出表格</h3><p>这里提取出来的list是str字符型的，我们需要转换为list列表类型。为什么要转换为list类型呢，因为无法用操作list的方法去操作str，比如list切片。转换为list后，我们可以对list进行切片，比如data[0]可以获取第一个{}中的数据，也就是表格第一行，这样方便后续构造循环从而逐行输出表格数据。这里采用json.loads()方法将str转换为list。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = json.loads(data)</span><br><span class="line"><span class="comment"># print(data) 和上面的一样</span></span><br><span class="line">print(type(data))</span><br><span class="line">print(data[<span class="number">0</span>])</span><br><span class="line">结果如下：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line">&#123;'scode': '600478', 'hycode': '016040', 'companycode': '10001305', 'sname': '科力远', 'publishname': '材料行业', 'reporttimetypecode': '002', 'combinetypecode': '001', 'dataajusttype': '2', 'mkt': 'shzb', 'noticedate': '2018-10-13T00:00:00', 'reportdate': '2018-06-30T00:00:00', 'parentnetprofit': -46515200.15, 'totaloperatereve': 683459458.22, 'totaloperateexp': 824933386.17, 'totaloperateexp_tb': -0.0597570689015973, 'operateexp': 601335611.67, 'operateexp_tb': -0.105421872593886, 'saleexp': 27004422.05, 'manageexp': 141680603.83, 'financeexp': 33258589.95, 'operateprofit': -94535963.65, 'sumprofit': -92632216.61, 'incometax': -8809471.54, 'operatereve': '-', 'intnreve': '-', 'intnreve_tb': '-', 'commnreve': '-', 'commnreve_tb': '-', 'operatetax': 7777267.21, 'operatemanageexp': '-', 'commreve_commexp': '-', 'intreve_intexp': '-', 'premiumearned': '-', 'premiumearned_tb': '-', 'investincome': '-', 'surrenderpremium': '-', 'indemnityexp': '-', 'tystz': -0.092852, 'yltz': 0.178351, 'sjltz': 0.399524, 'kcfjcxsyjlr': -58082725.17, 'sjlktz': 0.2475682609, 'eutime': '2018/10/12 21:01:36', 'yyzc': 601335611.67&#125;</span><br></pre></td></tr></table></figure><p>接下来我们就将表格内容输入到csv文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入csv文件</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        w = csv.writer(f)</span><br><span class="line">        w.writerow(d.values())</span><br></pre></td></tr></table></figure><p>通过for循环，依次取出表格中的每一行字典数据{}，然后用with…open的方法写入’eastmoney.csv’文件中。</p><p><strong>tips</strong>：’a’表示可重复写入；encoding=’utf_8_sig’ 能保持csv文件的汉字不会乱码；newline为空能避免每行数据中产生空行。</p><p>这样，第一页50行的表格数据就成功输出到csv文件中去了：</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fn4lk33DpaNp2SyAK2HB5_IaKClv" alt=""></p><p>这里，我们还可以在输出表格之前添加上表头：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加列标题</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_header</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = list(data[<span class="number">0</span>].keys())</span><br><span class="line">        print(headers)  </span><br><span class="line">        print(len(headers)) <span class="comment"># 输出list长度，也就是有多少列</span></span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow(headers)</span><br></pre></td></tr></table></figure><p>这里，data[0]表示list的一个字典中的数据，data[0].keys()表示获取字典中的key键值，也就是列标题。外面再加一个list序列化（结果如下），然后将该list输出到’eastmoney.csv’中作为表格的列标题即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'scode'</span>, <span class="string">'hycode'</span>, <span class="string">'companycode'</span>, <span class="string">'sname'</span>, <span class="string">'publishname'</span>, <span class="string">'reporttimetypecode'</span>, <span class="string">'combinetypecode'</span>, <span class="string">'dataajusttype'</span>, <span class="string">'mkt'</span>, <span class="string">'noticedate'</span>, <span class="string">'reportdate'</span>, <span class="string">'parentnetprofit'</span>, <span class="string">'totaloperatereve'</span>, <span class="string">'totaloperateexp'</span>, <span class="string">'totaloperateexp_tb'</span>, <span class="string">'operateexp'</span>, <span class="string">'operateexp_tb'</span>, <span class="string">'saleexp'</span>, <span class="string">'manageexp'</span>, <span class="string">'financeexp'</span>, <span class="string">'operateprofit'</span>, <span class="string">'sumprofit'</span>, <span class="string">'incometax'</span>, <span class="string">'operatereve'</span>, <span class="string">'intnreve'</span>, <span class="string">'intnreve_tb'</span>, <span class="string">'commnreve'</span>, <span class="string">'commnreve_tb'</span>, <span class="string">'operatetax'</span>, <span class="string">'operatemanageexp'</span>, <span class="string">'commreve_commexp'</span>, <span class="string">'intreve_intexp'</span>, <span class="string">'premiumearned'</span>, <span class="string">'premiumearned_tb'</span>, <span class="string">'investincome'</span>, <span class="string">'surrenderpremium'</span>, <span class="string">'indemnityexp'</span>, <span class="string">'tystz'</span>, <span class="string">'yltz'</span>, <span class="string">'sjltz'</span>, <span class="string">'kcfjcxsyjlr'</span>, <span class="string">'sjlktz'</span>, <span class="string">'eutime'</span>, <span class="string">'yyzc'</span>]</span><br><span class="line"><span class="number">44</span> <span class="comment"># 一共有44个字段，也就是说表格有44列。</span></span><br></pre></td></tr></table></figure><p><img src="http://pbscl931v.bkt.clouddn.com/FktoPi6Qd0WRm8soOZNY6L5LByHf" alt=""></p><p>以上，就完成了单页表格的爬取和下载到本地的过程。</p><h2 id="3-多页表格爬取"><a href="#3-多页表格爬取" class="headerlink" title="3. 多页表格爬取"></a>3. 多页表格爬取</h2><p>将上述代码整理为相应的函数，再添加for循环，仅50行代码就可以爬取72页的利润报表数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(page)</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'type'</span>: <span class="string">'CWBB_LRB'</span>,  <span class="comment"># 表格类型,LRB为利润表缩写，必须</span></span><br><span class="line">        <span class="string">'token'</span>: <span class="string">'70f12f2f4f091e459a279469fe49eca5'</span>,  <span class="comment"># 访问令牌，必须</span></span><br><span class="line">        <span class="string">'st'</span>: <span class="string">'noticedate'</span>,  <span class="comment"># 公告日期</span></span><br><span class="line">        <span class="string">'sr'</span>: <span class="number">-1</span>,  <span class="comment"># 保持-1不用改动即可</span></span><br><span class="line">        <span class="string">'p'</span>: page,  <span class="comment"># 表格页数</span></span><br><span class="line">        <span class="string">'ps'</span>: <span class="number">50</span>,  <span class="comment"># 每页显示多少条信息</span></span><br><span class="line">        <span class="string">'js'</span>: <span class="string">'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;'</span>,  <span class="comment"># js函数，必须</span></span><br><span class="line">        <span class="string">'filter'</span>: <span class="string">'(reportdate=^2018-06-30^)'</span>,  <span class="comment"># 筛选条件，如果不选则默认下载全部时期的数据</span></span><br><span class="line">        <span class="comment"># 'rt': 51294261  可不用</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?'</span></span><br><span class="line">    response = requests.get(url, params=params).text</span><br><span class="line">  <span class="comment"># 确定页数</span></span><br><span class="line">    pat = re.compile(<span class="string">'var.*?&#123;pages:(\d+),data:.*?'</span>)</span><br><span class="line">    page_all = re.search(pat, response)  <span class="comment"># 总页数</span></span><br><span class="line">    pattern = re.compile(<span class="string">'var.*?data: (.*)&#125;'</span>, re.S)</span><br><span class="line">    items = re.search(pattern, response)</span><br><span class="line">    data = items.group(<span class="number">1</span>)</span><br><span class="line">    data = json.loads(data)</span><br><span class="line">    print(<span class="string">'\n正在下载第 %s 页表格'</span> % page)</span><br><span class="line">    <span class="keyword">return</span> page_all,data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_header</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = list(data[<span class="number">0</span>].keys())</span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow(headers)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_table</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">            w = csv.writer(f)</span><br><span class="line">            w.writerow(d.values())</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    data = get_table(page)</span><br><span class="line">    write_table(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start_time = time.time()  <span class="comment"># 下载开始时间</span></span><br><span class="line">    <span class="comment"># 写入表头</span></span><br><span class="line">    write_header(get_table(<span class="number">1</span>))</span><br><span class="line">    page_all = get_table(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    page_all = int(page_all.group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, page_all):</span><br><span class="line">        main(page)</span><br><span class="line">    end_time = time.time() - start_time  <span class="comment"># 结束时间</span></span><br><span class="line">    print(<span class="string">'下载用时: &#123;:.1f&#125; s'</span> .format(end_time))</span><br></pre></td></tr></table></figure><p>整个下载只用了20多秒，而之前用selenium花了几十分钟，这效率提升了足有100倍！</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fs-EixHunxcirWSlKPZhF_A1Ariu" alt=""></p><p>这里，如果我们想下载全部时期（从2007年-2018年）利润报表数据，也很简单。只要将<code>type</code>中的<code>filter</code>参数注释掉，意味着也就是不筛选日期，那么就可以下载全部时期的数据。这里当我们取消注释filter列，将会发现总页数page_all会从2018年中报的72页增加到2528页，全部下载完成后，表格有超过12万行的数据。基于这些数据，可以尝试从中进行一些有价值的数据分析。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FuP6aIzr6A7c52EgxAvGiptpoeCX" alt=""></p><h2 id="4-通用代码构造"><a href="#4-通用代码构造" class="headerlink" title="4. 通用代码构造"></a>4. 通用代码构造</h2><p>以上代码实现了2018年中报利润报表的爬取，但如果不想局限于该报表，还想爬取其他报表或者其他任意时期的数据，那么就需要手动地去修改代码中相应的字段，很不方便。所以上面的代码可以说是简短但不够强大。</p><p>为了能够灵活实现爬取任意类别和任意时期的报表数据，需要对代码再进行一些加工，就可以构造出通用强大的爬虫程序了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">e.g: http://data.eastmoney.com/bbsj/201806/lrb.html</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置文件保存在D盘eastmoney文件夹下</span></span><br><span class="line">file_path = <span class="string">'D:\\eastmoney'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">    os.mkdir(file_path)</span><br><span class="line">os.chdir(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 设置表格爬取时期、类别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_table</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line">    print(<span class="string">'\t\t\t\t东方财富网报表下载'</span>)</span><br><span class="line">    print(<span class="string">'作者：高级农民工  2018.10.10'</span>)</span><br><span class="line">    print(<span class="string">'--------------'</span>)</span><br><span class="line">    year = int(float(input(<span class="string">'请输入要查询的年份(四位数2007-2018)：\n'</span>)))</span><br><span class="line">    <span class="comment"># int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10</span></span><br><span class="line">    <span class="keyword">while</span> (year &lt; <span class="number">2007</span> <span class="keyword">or</span> year &gt; <span class="number">2018</span>):</span><br><span class="line">        year = int(float(input(<span class="string">'年份数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    quarter = int(float(input(<span class="string">'请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：\n'</span>)))</span><br><span class="line">    <span class="keyword">while</span> (quarter &lt; <span class="number">1</span> <span class="keyword">or</span> quarter &gt; <span class="number">4</span>):</span><br><span class="line">        quarter = int(float(input(<span class="string">'季度数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充，</span></span><br><span class="line">    <span class="comment"># http://www.runoob.com/python/att-string-format.html</span></span><br><span class="line">    quarter = <span class="string">'&#123;:02d&#125;'</span>.format(quarter * <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># quarter = '%02d' %(int(month)*3)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定季度所对应的最后一天是30还是31号</span></span><br><span class="line">    <span class="keyword">if</span> (quarter == <span class="string">'06'</span>) <span class="keyword">or</span> (quarter == <span class="string">'09'</span>):</span><br><span class="line">        day = <span class="number">30</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        day = <span class="number">31</span></span><br><span class="line">    date = <span class="string">'&#123;&#125;-&#123;&#125;-&#123;&#125;'</span> .format(year, quarter, day)</span><br><span class="line">    <span class="comment"># print('date:', date)  # 测试日期 ok</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2 设置财务报表种类</span></span><br><span class="line">    tables = int(</span><br><span class="line">        input(<span class="string">'请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): \n'</span>))</span><br><span class="line"></span><br><span class="line">    dict_tables = &#123;<span class="number">1</span>: <span class="string">'业绩报表'</span>, <span class="number">2</span>: <span class="string">'业绩快报表'</span>, <span class="number">3</span>: <span class="string">'业绩预告表'</span>,</span><br><span class="line">                   <span class="number">4</span>: <span class="string">'预约披露时间表'</span>, <span class="number">5</span>: <span class="string">'资产负债表'</span>, <span class="number">6</span>: <span class="string">'利润表'</span>, <span class="number">7</span>: <span class="string">'现金流量表'</span>&#125;</span><br><span class="line"></span><br><span class="line">    dict = &#123;<span class="number">1</span>: <span class="string">'YJBB'</span>, <span class="number">2</span>: <span class="string">'YJKB'</span>, <span class="number">3</span>: <span class="string">'YJYG'</span>,</span><br><span class="line">            <span class="number">4</span>: <span class="string">'YYPL'</span>, <span class="number">5</span>: <span class="string">'ZCFZB'</span>, <span class="number">6</span>: <span class="string">'LRB'</span>, <span class="number">7</span>: <span class="string">'XJLLB'</span>&#125;</span><br><span class="line">    category = dict[tables]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># js请求参数里的type，第1-4个表的前缀是'YJBB20_'，后3个表是'CWBB_'</span></span><br><span class="line">    <span class="comment"># 设置set_table()中的type、st、sr、filter参数</span></span><br><span class="line">    <span class="keyword">if</span> tables == <span class="number">1</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'latestnoticedate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter =  <span class="string">"(securitytypecode in ('058001001','058001002'))(reportdate=^%s^)"</span> %(date)</span><br><span class="line">    <span class="keyword">elif</span> tables == <span class="number">2</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'ldate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter = <span class="string">"(securitytypecode in ('058001001','058001002'))(rdate=^%s^)"</span> %(date)</span><br><span class="line">    <span class="keyword">elif</span> tables == <span class="number">3</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'ndate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter=<span class="string">" (IsLatest='T')(enddate=^2018-06-30^)"</span></span><br><span class="line">    <span class="keyword">elif</span> tables == <span class="number">4</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'frdate'</span></span><br><span class="line">        sr = <span class="number">1</span></span><br><span class="line">        filter =  <span class="string">"(securitytypecode ='058001001')(reportdate=^%s^)"</span> %(date)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        category_type = <span class="string">'CWBB_'</span></span><br><span class="line">        st = <span class="string">'noticedate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter = <span class="string">'(reportdate=^%s^)'</span> % (date)</span><br><span class="line"></span><br><span class="line">    category_type = category_type + category</span><br><span class="line">    <span class="comment"># print(category_type)</span></span><br><span class="line">    <span class="comment"># 设置set_table()中的filter参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'date'</span>:date,</span><br><span class="line">    <span class="string">'category'</span>:dict_tables[tables],</span><br><span class="line">    <span class="string">'category_type'</span>:category_type,</span><br><span class="line">    <span class="string">'st'</span>:st,</span><br><span class="line">    <span class="string">'sr'</span>:sr,</span><br><span class="line">    <span class="string">'filter'</span>:filter</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 设置表格爬取起始页数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">page_choose</span><span class="params">(page_all)</span>:</span></span><br><span class="line">    <span class="comment"># 选择爬取页数范围</span></span><br><span class="line">    start_page = int(input(<span class="string">'请输入下载起始页数：\n'</span>))</span><br><span class="line">    nums = input(<span class="string">'请输入要下载的页数，（若需下载全部则按回车）：\n'</span>)</span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断输入的是数值还是回车空格</span></span><br><span class="line">    <span class="keyword">if</span> nums.isdigit():</span><br><span class="line">        end_page = start_page + int(nums)</span><br><span class="line">    <span class="keyword">elif</span> nums == <span class="string">''</span>:</span><br><span class="line">        end_page = int(page_all.group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'页数输入错误'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回所需的起始页数，供后续程序调用</span></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'start_page'</span>: start_page,</span><br><span class="line">        <span class="string">'end_page'</span>: end_page</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 表格正式爬取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(date, category_type,st,sr,filter,page)</span>:</span></span><br><span class="line">    <span class="comment"># 参数设置</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="comment"># 'type': 'CWBB_LRB',</span></span><br><span class="line">        <span class="string">'type'</span>: category_type,  <span class="comment"># 表格类型</span></span><br><span class="line">        <span class="string">'token'</span>: <span class="string">'70f12f2f4f091e459a279469fe49eca5'</span>,</span><br><span class="line">        <span class="string">'st'</span>: st,</span><br><span class="line">        <span class="string">'sr'</span>: sr,</span><br><span class="line">        <span class="string">'p'</span>: page,</span><br><span class="line">        <span class="string">'ps'</span>: <span class="number">50</span>,  <span class="comment"># 每页显示多少条信息</span></span><br><span class="line">        <span class="string">'js'</span>: <span class="string">'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;'</span>,</span><br><span class="line">        <span class="string">'filter'</span>: filter,</span><br><span class="line">        <span class="comment"># 'rt': 51294261  可不用</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?'</span></span><br><span class="line">    response = requests.get(url, params=params).text</span><br><span class="line">    <span class="comment"># 确定页数</span></span><br><span class="line">    pat = re.compile(<span class="string">'var.*?&#123;pages:(\d+),data:.*?'</span>)</span><br><span class="line">    page_all = re.search(pat, response)</span><br><span class="line">    <span class="comment"># print(page_all.group(1))  # ok</span></span><br><span class="line">    <span class="comment"># 提取出list，可以使用json.dumps和json.loads</span></span><br><span class="line">    pattern = re.compile(<span class="string">'var.*?data: (.*)&#125;'</span>, re.S)</span><br><span class="line">    items = re.search(pattern, response)</span><br><span class="line">    <span class="comment"># 等价于</span></span><br><span class="line">    <span class="comment"># items = re.findall(pattern,response)</span></span><br><span class="line">    <span class="comment"># print(items[0])</span></span><br><span class="line">    data = items.group(<span class="number">1</span>)</span><br><span class="line">    data = json.loads(data)</span><br><span class="line">    <span class="keyword">return</span> page_all, data,page</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 写入表头</span></span><br><span class="line"><span class="comment"># 方法1 借助csv包，最常用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_header</span><span class="params">(data,category)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'&#123;&#125;.csv'</span> .format(category), <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = list(data[<span class="number">0</span>].keys())</span><br><span class="line">        <span class="comment"># print(headers)  # 测试 ok</span></span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow(headers)</span><br><span class="line"><span class="comment"># 5 写入表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_table</span><span class="params">(data,page,category)</span>:</span></span><br><span class="line">    print(<span class="string">'\n正在下载第 %s 页表格'</span> % page)</span><br><span class="line">    <span class="comment"># 写入文件方法1</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'&#123;&#125;.csv'</span> .format(category), <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">            w = csv.writer(f)</span><br><span class="line">            w.writerow(d.values())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(date, category_type,st,sr,filter,page)</span>:</span></span><br><span class="line">    func = get_table(date, category_type,st,sr,filter,page)</span><br><span class="line">    data = func[<span class="number">1</span>]</span><br><span class="line">    page = func[<span class="number">2</span>]</span><br><span class="line">    write_table(data,page,category)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 获取总页数，确定起始爬取页数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> set_table():</span><br><span class="line">        date = i.get(<span class="string">'date'</span>)</span><br><span class="line">        category = i.get(<span class="string">'category'</span>)</span><br><span class="line">        category_type = i.get(<span class="string">'category_type'</span>)</span><br><span class="line">        st = i.get(<span class="string">'st'</span>)</span><br><span class="line">        sr = i.get(<span class="string">'sr'</span>)</span><br><span class="line">        filter = i.get(<span class="string">'filter'</span>)</span><br><span class="line">    constant = get_table(date,category_type,st,sr,filter, <span class="number">1</span>)</span><br><span class="line">    page_all = constant[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> page_choose(page_all):</span><br><span class="line">        start_page = i.get(<span class="string">'start_page'</span>)</span><br><span class="line">        end_page = i.get(<span class="string">'end_page'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先写入表头</span></span><br><span class="line">    write_header(constant[<span class="number">1</span>],category)</span><br><span class="line">    start_time = time.time()  <span class="comment"># 下载开始时间</span></span><br><span class="line">    <span class="comment"># 爬取表格主程序</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">        main(date,category_type,st,sr,filter, page)</span><br><span class="line">    end_time = time.time() - start_time  <span class="comment"># 结束时间</span></span><br><span class="line">    print(<span class="string">'下载完成'</span>)</span><br><span class="line">    print(<span class="string">'下载用时: &#123;:.1f&#125; s'</span> .format(end_time))</span><br></pre></td></tr></table></figure><p>以爬取2018年中业绩报表为例，感受一下比selenium快得多的爬取效果（视频链接）：</p><p><a href="https://v.qq.com/x/page/a0519bfxajc.html" target="_blank" rel="noopener">https://v.qq.com/x/page/a0519bfxajc.html</a></p><p>利用上面的程序，我们可以下载任意时期和任意报表的数据。这里，我下载完成了2018年中报所有7个报表的数据。</p><p>文中代码和素材资源可以在下面的链接中获取：</p><p><a href="https://github.com/makcyun/eastmoney_spider" target="_blank" rel="noopener">https://github.com/makcyun/eastmoney_spider</a></p><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过分析网址 JavaScript 请求，以比 Selenium 快 100 倍的方法，快速爬取东方财富网各上市公司历年的财务报表数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(5)：Selenium 爬取东方财富网股票财务报表</title>
    <link href="https://www.makcyun.top/web_scraping_withpython5.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython5.html</id>
    <published>2018-10-02T09:18:57.000Z</published>
    <updated>2018-10-18T08:39:10.437Z</updated>
    
    <content type="html"><![CDATA[<p>利用Selenium爬取东方财富网各上市公司历年的财务报表数据。</p><a id="more"></a>  <p><strong>摘要：</strong> 现在很多网页都采取JavaScript进行动态渲染，其中包括Ajax技术。上一篇文章通过分析Ajax接口数据，顺利爬取了澎湃新闻网动态网页中的图片。但有的网页虽然也Ajax技术，但接口参数可能是加密的无法直接获得，比如淘宝；有的动态网页也采用JavaScript，但不是Ajax技术，比如Echarts官网。所以，当遇到这两类网页时，上一篇文章中的方法便不再奏效，需要新的采取新的方法，这其中包括干脆、直接、好用的的Selenium大法。东方财富网的财务报表网页也是通过JavaScript动态加载的，本文利用Selenium方法爬取该网站上市公司的财务报表数据。</p><p>[TOC]</p><h2 id="1-实战背景"><a href="#1-实战背景" class="headerlink" title="1. 实战背景"></a>1. 实战背景</h2><p>很多网站都提供上市公司的公告、财务报表等金融投资信息和数据，比如：腾讯财经、网易财经、新浪财经、东方财富网等。这之中，发现东方财富网的数据非常齐全。</p><p><code>东方财富网</code>有一个数据中心：<a href="http://data.eastmoney.com/center/" target="_blank" rel="noopener">http://data.eastmoney.com/center/</a>，该数据中心提供包括特色数据、研究报告、年报季报等在内的大量数据（见下图）。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-4/3257803.jpg" alt=""></p><p>以年报季报类别为例，我们点开该分类查看一下2018年中报（见下图），可以看到该分类下又包括：业绩报表、业绩快报、利润表等7个报表的数据。以业绩报表为例，报表包含全部3000多只股票的业绩报表数据，一共有70多页。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-4/90719379.jpg" alt=""></p><p>假如，我们想获取所有股票2018年中的业绩报表数据，然后对该数据进行一些分析。采取手动复制的方法，70多页可以勉强完成。但如果想获取任意一年、任意季度、任意报表的数据，要再通过手动复制的方法，工作量会非常地大。举个例子，假设要获取10年间（40个季度）、所有7个报表的数据，那么手动复制的工作量大约将是：40×7×70（每个报表大约70页），差不多要重复性地复制2万次！！！可以说是人工不可能完成的任务。所以，本文的目标就是利用Selenium自动化技术，爬取年报季报类别下，任意一年（网站有数据至今）、任意财务报表数据。我们所需要做的，仅是简单输入几个字符，其他就全部交给电脑，然后过一会儿打开excel，就可以看到所需数据”静静地躺在那里”，是不是挺酷的？</p><p>好，下面我们就开始实操一下。首先，需要分析要爬取的网页对象。</p><h2 id="2-网页分析"><a href="#2-网页分析" class="headerlink" title="2. 网页分析"></a>2. 网页分析</h2><p>之前，我们已经爬过表格型的数据，所以对表格数据的结构应该不会太陌生，如果忘了，可以再看一下这篇文章：<a href="https://www.makcyun.top/web_scraping_withpython2.html">https://www.makcyun.top/web_scraping_withpython2.html</a></p><p>我们这里以上面的2018年中报的业绩报表为例，查看一下表格的形式。</p><p>网址url：<a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a>，<code>bbsj</code>代表年报季报，<code>201803</code>代表<code>2018年一季报</code>，类似地，201806表示年中报；<code>lrb</code>是<code>利润表</code>的首字母缩写，同理，<code>yjbb</code>表示<code>业绩报表</code>。可以看出，该网址格式很简单，便于构造url。</p><p>接着，我们点击<code>下一页</code>按钮，可以看到表格更新后，url没有发生改变，可以判定是采用了Javscript。那么，我们首先判断是不是采用了Ajax加载的。方法也很简单，右键检查或按F12，切换到network并选择下面的XHR，再按F5刷新。可以看到只有一个Ajax请求，点击下一页也并没有生成新的Ajax请求，可以判断该网页结构不是常见的那种点击下一页或者下拉会源源不断出现的Ajax请求类型，那么便无法构造url来实现分页爬取。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-4/90923672.jpg" alt=""></p><p>XHR选项里没有找到我们需要的请求，接下来试试看能不能再JS里找到表格的数据请求。将选项选为JS，再次F5刷新，可以看到出现了很多JS请求，然后我们点击几次下一页，会发现弹出新的请求来，然后右边为响应的请求信息。url链接非常长，看上去很复杂。好，这里我们先在这里打住不往下了。</p><p>可以看到，通过分析后台元素来爬取该动态网页的方法，相对比较复杂。那么有没有干脆、直截了当地就能够抓取表格内容的方法呢？有的，就是本文接下来要介绍的Selenium大法。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-5/136165.jpg" alt=""></p><h2 id="3-Selenium知识"><a href="#3-Selenium知识" class="headerlink" title="3. Selenium知识"></a>3. Selenium知识</h2><p>Selenium 是什么？一句话，自动化测试工具。它是为了测试而出生的，但在近几年火热的爬虫领域中，它摇身一变，变成了爬虫的利器。直白点说， <strong>Seleninm能控制浏览器, 像人一样”上网”</strong>。比如，可以实现网页自动翻页、登录网站、发送邮件、下载图片/音乐/视频等等。举个例子，写几行python代码就可以用Selenium实现登录<a href="https://www.itjuzi.com/" target="_blank" rel="noopener">IT桔子</a>，然后浏览网页的功能。</p><p><img src="http://pc1lljdwb.bkt.clouddn.com/selenium.gif" alt=""></p><p>怎么样，仅用几行代码就能实现自动上网操作，是不是挺神奇的？当然，这仅仅是Selenium最简单的功能，还有很多更加丰富的操作，可以参考以下几篇教程：</p><p>参考网站：</p><blockquote><p>Selenium官网： <a href="https://selenium-python.readthedocs.io/" target="_blank" rel="noopener">https://selenium-python.readthedocs.io/</a></p><p>SeleniumPython文档（英文版）：<a href="http://selenium-python.readthedocs.org/index.html" target="_blank" rel="noopener">http://selenium-python.readthedocs.org/index.html</a></p><p>SeleniumPython文档（中文版）：<a href="https://selenium-python-zh.readthedocs.io/en/latest/faq.html" target="_blank" rel="noopener">https://selenium-python-zh.readthedocs.io/en/latest/faq.html</a></p><p>Selenium 基本操作：<a href="https://www.yukunweb.com/2017/7/python-spider-Selenium-PhantomJS-basic/" target="_blank" rel="noopener">https://www.yukunweb.com/2017/7/python-spider-Selenium-PhantomJS-basic/</a></p><p>Selenium爬取淘宝信息实战：<a href="https://cuiqingcai.com/2852.html" target="_blank" rel="noopener">https://cuiqingcai.com/2852.html</a></p></blockquote><p>只需要记住重要的一点就是：<code>Selenium能做到&quot;可见即可爬&quot;</code>。也就是说网页上你能看到的东西，Selenium基本上都能爬取下来。包括上面我们提到的东方财富网的财务报表数据，它也能够做到，而且非常简单直接，不用去后台查看用了什么JavaScript技术或者Ajax参数。下面我们就实际来操练下吧。</p><h2 id="4-编码实现"><a href="#4-编码实现" class="headerlink" title="4. 编码实现"></a>4. 编码实现</h2><h3 id="4-1-思路"><a href="#4-1-思路" class="headerlink" title="4.1. 思路"></a>4.1. 思路</h3><ul><li>安装配置好Selenium运行的相关环境，浏览器可以用Chrome、Firefox、PhantomJS等，我用的是Chrome；</li><li>东方财富网的财务报表数据不用登录可直接获得，Selenium更加方便爬取；</li><li>先以单个网页中的财务报表为例，表格数据结构简单，可先直接定位到整个表格，然后一次性获取所有td节点对应的表格单元内容；</li><li>接着循环分页爬取所有上市公司的数据，并保存为csv文件。</li><li>重新构造灵活的url，实现可以爬取任意时期、任意一张财务报表的数据。</li></ul><p>根据上述思路，下面就用代码一步步来实现。</p><h3 id="4-2-爬取单页表格"><a href="#4-2-爬取单页表格" class="headerlink" title="4.2. 爬取单页表格"></a>4.2. 爬取单页表格</h3><p>我们先以<code>2018年中报的利润表</code>为例，抓取该网页的第一页表格数据，网页url：<a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-5/446556.jpg" alt=""></p><p>快速定位到表格所在的节点：<strong>id = dt_1</strong>，然后可以用Selenium进行抓取了，方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="comment"># 当测试好能够顺利爬取后，为加快爬取速度可设置无头模式，即不弹出浏览器</span></span><br><span class="line"><span class="comment"># 添加无头headlesss 1使用chrome headless,2使用PhantomJS</span></span><br><span class="line"><span class="comment"># 使用 PhantomJS 会警告高不建议使用phantomjs，建议chrome headless</span></span><br><span class="line"><span class="comment"># chrome_options = webdriver.ChromeOptions()</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument('--headless')</span></span><br><span class="line"><span class="comment"># browser = webdriver.Chrome(chrome_options=chrome_options)</span></span><br><span class="line"><span class="comment"># browser = webdriver.PhantomJS()</span></span><br><span class="line"><span class="comment"># browser.maximize_window()  # 最大化窗口,可以选择设置</span></span><br><span class="line"></span><br><span class="line">browser.get(<span class="string">'http://data.eastmoney.com/bbsj/201806/lrb.html'</span>)</span><br><span class="line">element = browser.find_element_by_css_selector(<span class="string">'#dt_1'</span>)  <span class="comment"># 定位表格，element是WebElement类型</span></span><br><span class="line"><span class="comment"># 提取表格内容td</span></span><br><span class="line">td_content = element.find_elements_by_tag_name(<span class="string">"td"</span>) <span class="comment"># 进一步定位到表格内容所在的td节点</span></span><br><span class="line">lst = []  <span class="comment"># 存储为list</span></span><br><span class="line"><span class="keyword">for</span> td <span class="keyword">in</span> td_content:</span><br><span class="line">    lst.append(td.text)</span><br><span class="line">print(lst) <span class="comment"># 输出表格内容</span></span><br></pre></td></tr></table></figure><p>这里，使用Chrome浏览器构造一个Webdriver对象，赋值给变量browser，browser调用get()方法请求想要抓取的网页。接着使用<code>find_element_by_css_selector</code>方法查找表格所在的节点：<strong>‘#dt_1’</strong>。</p><p>这里推荐一款小巧、快速定位css/xpath的Chrome插件：<strong>SelectorGadget</strong>，使用这个插件就不用再去源代码中手动定位节点那么麻烦了。</p><p>插件地址：<a href="https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb" target="_blank" rel="noopener">https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb</a></p><p>紧接着再向下定位到<strong>td</strong>节点，因为网页中有很多个td节点，所以要用<strong>find_elements</strong>方法。然后，遍历数据节点存储到list中。打印查看一下结果：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-5/53703391.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list形式:</span></span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'002161'</span>, <span class="string">'远望谷'</span>, ...<span class="string">'-7960万'</span>, <span class="string">'09-29'</span>,</span><br><span class="line"> <span class="string">'2'</span>,<span class="string">'002316'</span>, <span class="string">'亚联发展'</span>, ...<span class="string">'1.79亿'</span>, <span class="string">'09-29'</span>, </span><br><span class="line"> <span class="string">'3'</span>,...</span><br><span class="line"> <span class="string">'50'</span>, <span class="string">'002683'</span>, <span class="string">'宏大爆破'</span>,...<span class="string">'1.37亿'</span>, <span class="string">'09-01'</span>]</span><br></pre></td></tr></table></figure><p>是不是很方便，几行代码就能抓取下来这一页表格，除了速度有点慢。</p><p>为了便于后续存储，我们将list转换为DataFrame。首先需要把这一个大的list分割为多行多列的子list，实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 确定表格列数</span></span><br><span class="line">col = len(element.find_elements_by_css_selector(<span class="string">'tr:nth-child(1) td'</span>))</span><br><span class="line"><span class="comment"># 通过定位一行td的数量，可获得表格的列数，然后将list拆分为对应列数的子list</span></span><br><span class="line">lst = [lst[i:i + col] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(lst), col)]</span><br><span class="line"><span class="comment"># 原网页中打开"详细"链接可以查看更详细的数据，这里我们把url提取出来，方便后期查看</span></span><br><span class="line">lst_link = []</span><br><span class="line">links = element.find_elements_by_css_selector(<span class="string">'#dt_1 a.red'</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">    url = link.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">    lst_link.append(url)</span><br><span class="line">lst_link = pd.Series(lst_link)</span><br><span class="line"><span class="comment"># list转为dataframe</span></span><br><span class="line">df_table = pd.DataFrame(lst)</span><br><span class="line"><span class="comment"># 添加url列</span></span><br><span class="line">df_table[<span class="string">'url'</span>] = lst_link</span><br><span class="line">print(df_table.head())  <span class="comment"># 查看DataFrame</span></span><br></pre></td></tr></table></figure><p>这里，要将list分割为子list，只需要确定表格有多少列即可，然后将每相隔这么多数量的值划分为一个子list。如果我们数一下该表的列数，可以发现一共有16列。但是这里不能使用这个数字，因为除了利润表，其他报表的列数并不是16，所以当后期爬取其他表格可能就会报错。这里仍然通过f<strong>ind_elements_by_css_selector</strong>方法，定位首行td节点的数量，便可获得表格的列数，然后将list拆分为对应列数的子list。同时，原网页中打开”详细”列的链接可以查看更详细的数据，这里我们把url提取出来，并增加一列到DataFrame中，方便后期查看。打印查看一下输出结果：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-5/20285338.jpg" alt=""></p><p>可以看到，表格所有的数据我们都抓取到了，下面只需要进行分页循环爬取就行了。</p><p>这里，没有抓取表头是因为表头有合并单元格，处理起来就非常麻烦。建议表格抓取下来后，在excel中复制表头进去就行了。如果，实在想要用代码完成，可以参考这篇文章：<a href="https://blog.csdn.net/weixin_39461443/article/details/75456962" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39461443/article/details/75456962</a></p><h3 id="4-3-分页爬取"><a href="#4-3-分页爬取" class="headerlink" title="4.3. 分页爬取"></a>4.3. 分页爬取</h3><p>上面完成了单页表格的爬取，下面我们来实现分页爬取。</p><p>首先，我们先实现Selenium模拟翻页跳转操作，成功后再爬取每页的表格内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.maximize_window()  <span class="comment"># 最大化窗口,可以选择设置</span></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        browser.get(<span class="string">'http://data.eastmoney.com/bbsj/201806/lrb.html'</span>)</span><br><span class="line">        print(<span class="string">'正在爬取第： %s 页'</span> % page)</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"dt_1"</span>)))</span><br><span class="line">        <span class="comment"># 判断是否是第1页，如果大于1就输入跳转，否则等待加载完成。</span></span><br><span class="line">        <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 确定页数输入框</span></span><br><span class="line">            input = wait.until(EC.presence_of_element_located(</span><br><span class="line">                (By.XPATH, <span class="string">'//*[@id="PageContgopage"]'</span>)))</span><br><span class="line">            input.click()</span><br><span class="line">            input.clear()</span><br><span class="line">            input.send_keys(page)</span><br><span class="line">            submit = wait.until(EC.element_to_be_clickable(</span><br><span class="line">                (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; a.btn_link'</span>)))</span><br><span class="line">            submit.click()</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 确认成功跳转到输入框中的指定页</span></span><br><span class="line">        wait.until(EC.text_to_be_present_in_element(</span><br><span class="line">            (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; span.at'</span>), str(page)))</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):  <span class="comment"># 测试翻4页</span></span><br><span class="line">        index_page(page)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>这里，我们先加载了相关包，使用WebDriverWait对象，设置最长10s的显式等待时间，以便网页加载出表格。判断表格是否加载出来，用到了<strong>EC.presence_of_element_located</strong>条件。表格加载出来后，设置一个页面判断，如果在第1页就等待页面加载完成，如果大于第1页就开始跳转。</p><p>要完成跳转操作，我们需要通过获取输入框input节点，然后用clear()方法清空输入框，再通过send_keys()方法填写相应的页码，接着通过submit.click()方法击下一页完成翻页跳转。</p><p>这里，我们测试一下前4页跳转效果，可以看到网页成功跳转了。下面就可以对每一页应用第一页爬取表格内容的方法，抓取每一页的表格，转为DataFrame然后存储到csv文件中去。</p><p><img src="" alt="http://pc1lljdwb.bkt.clouddn.com/selenium%E5%88%86%E9%A1%B5%E5%8E%8B%E7%BC%A9.gif"></p><h3 id="4-4-通用爬虫构造"><a href="#4-4-通用爬虫构造" class="headerlink" title="4.4. 通用爬虫构造"></a>4.4. 通用爬虫构造</h3><p>上面，我们完成了<strong>2018年中报利润表： <a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a></strong>，一个网页表格的爬取。但如果我们想爬取任意时期、任意一张报表的表格，比如2017年3季度的利润表、2016年全年的业绩报表、2015年1季度的现金流量表等等。上面的代码就行不通了，下面我们对代码进行一下改造，变成更通用的爬虫。从图中可以看到，东方财富网年报季报有7张表格，财务报表最早从2007年开始每季度一次。基于这两个维度，可重新构造url的形式，然后爬取表格数据。下面，我们用代码进行实现：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-6/3087007.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构url</span></span><br><span class="line"><span class="comment"># 1 设置财务报表获取时期</span></span><br><span class="line">year = int(float(input(<span class="string">'请输入要查询的年份(四位数2007-2018)：  '</span>)))</span><br><span class="line"><span class="comment"># int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会</span></span><br><span class="line"><span class="keyword">while</span> (year &lt; <span class="number">2007</span> <span class="keyword">or</span> year &gt; <span class="number">2018</span>):</span><br><span class="line">    year = int(float(input(<span class="string">'年份数值输入错误，请重新输入：'</span>)))</span><br><span class="line">quarter = int(float(input(<span class="string">'请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：  '</span>)))</span><br><span class="line"><span class="keyword">while</span> (quarter &lt; <span class="number">1</span> <span class="keyword">or</span> quarter &gt; <span class="number">4</span>):</span><br><span class="line">    quarter = int(float(input(<span class="string">'季度数值输入错误，请重新输入：  '</span>)))</span><br><span class="line"><span class="comment"># 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充</span></span><br><span class="line">quarter = <span class="string">'&#123;:02d&#125;'</span>.format(quarter * <span class="number">3</span>)</span><br><span class="line"><span class="comment"># quarter = '%02d' %(int(month)*3)</span></span><br><span class="line">date = <span class="string">'&#123;&#125;&#123;&#125;'</span> .format(year, quarter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 设置财务报表种类</span></span><br><span class="line">tables = int(</span><br><span class="line">    input(<span class="string">'请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表):  '</span>))</span><br><span class="line">dict_tables = &#123;<span class="number">1</span>: <span class="string">'业绩报表'</span>, <span class="number">2</span>: <span class="string">'业绩快报表'</span>, <span class="number">3</span>: <span class="string">'业绩预告表'</span>,</span><br><span class="line">               <span class="number">4</span>: <span class="string">'预约披露时间表'</span>, <span class="number">5</span>: <span class="string">'资产负债表'</span>, <span class="number">6</span>: <span class="string">'利润表'</span>, <span class="number">7</span>: <span class="string">'现金流量表'</span>&#125;</span><br><span class="line">dict = &#123;<span class="number">1</span>: <span class="string">'yjbb'</span>, <span class="number">2</span>: <span class="string">'yjkb/13'</span>, <span class="number">3</span>: <span class="string">'yjyg'</span>,</span><br><span class="line">        <span class="number">4</span>: <span class="string">'yysj'</span>, <span class="number">5</span>: <span class="string">'zcfz'</span>, <span class="number">6</span>: <span class="string">'lrb'</span>, <span class="number">7</span>: <span class="string">'xjll'</span>&#125;</span><br><span class="line">category = dict[tables]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 设置url</span></span><br><span class="line">url = <span class="string">'http://data.eastmoney.com/&#123;&#125;/&#123;&#125;/&#123;&#125;.html'</span> .format(<span class="string">'bbsj'</span>, date, category)</span><br><span class="line">print(url)  <span class="comment"># 测试输出的url</span></span><br></pre></td></tr></table></figure><p><img src="http://pc1lljdwb.bkt.clouddn.com/%E9%80%9A%E7%94%A8url.gif" alt=""></p><p>经过上面的设置，我们通过输入想要获得指定时期、制定财务报表类型的数值，就能返回相应的url链接。将该链接应用到前面的爬虫中，就可以爬取相应的报表内容了。</p><p>另外，除了从第一页开始爬取到最后一页的结果以外，我们还可以自定义设置想要爬取的页数。比如起始页数从第1页开始，然后爬取10页。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 选择爬取页数范围</span></span><br><span class="line">start_page = int(input(<span class="string">'请输入下载起始页数：\n'</span>))</span><br><span class="line">nums = input(<span class="string">'请输入要下载的页数，（若需下载全部则按回车）：\n'</span>)</span><br><span class="line"><span class="comment"># 确定网页中的最后一页</span></span><br><span class="line">browser.get(url)</span><br><span class="line"><span class="comment"># 确定最后一页页数不直接用数字而是采用定位，因为不同时间段的页码会不一样</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    page = browser.find_element_by_css_selector(<span class="string">'.next+ a'</span>)  <span class="comment"># next节点后面的a节点</span></span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    page = browser.find_element_by_css_selector(<span class="string">'.at+ a'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'没有找到该节点'</span>)</span><br><span class="line"><span class="comment"># 上面用try.except是因为绝大多数页码定位可用'.next+ a'，但是业绩快报表有的只有2页，无'.next+ a'节点</span></span><br><span class="line">end_page = int(page.text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> nums.isdigit():</span><br><span class="line">    end_page = start_page + int(nums)</span><br><span class="line"><span class="keyword">elif</span> nums == <span class="string">''</span>:</span><br><span class="line">    end_page = end_page</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'页数输入错误'</span>)</span><br><span class="line"><span class="comment"># 输入准备下载表格类型</span></span><br><span class="line">print(<span class="string">'准备下载:&#123;&#125;-&#123;&#125;'</span> .format(date, dict_tables[tables]))</span><br></pre></td></tr></table></figure><p>经过上面的设置，我们就可以实现自定义时期和财务报表类型的表格爬取了，将代码再稍微整理一下，可实现下面的爬虫效果：</p><p>视频截图：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-6/66845185.jpg" alt=""></p><p>视频地址：<a href="https://v.qq.com/x/page/y07335thsn2.html" target="_blank" rel="noopener">https://v.qq.com/x/page/y07335thsn2.html</a></p><p>背景中类似黑客帝国的代码雨效果，其实是动态网页效果。素材来源于下面这个网站，该网站还有很多酷炫的动态背景可以下载下来。</p><p><a href="http://wallpaper.upupoo.com/store/paperDetail-1783830052.htm" target="_blank" rel="noopener">http://wallpaper.upupoo.com/store/paperDetail-1783830052.htm</a></p><h3 id="4-5-完整代码"><a href="#4-5-完整代码" class="headerlink" title="4.5. 完整代码"></a>4.5. 完整代码</h3><p>整个爬虫的完整代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先chrome，后phantomjs</span></span><br><span class="line"><span class="comment"># browser = webdriver.Chrome()</span></span><br><span class="line"><span class="comment"># 添加无头headlesss</span></span><br><span class="line">chrome_options = webdriver.ChromeOptions()</span><br><span class="line">chrome_options.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># browser = webdriver.PhantomJS() # 会报警高提示不建议使用phantomjs，建议chrome添加无头</span></span><br><span class="line">browser.maximize_window()  <span class="comment"># 最大化窗口</span></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(<span class="string">'正在爬取第： %s 页'</span> % page)</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"dt_1"</span>)))</span><br><span class="line">        <span class="comment"># 判断是否是第1页，如果大于1就输入跳转，否则等待加载完成。</span></span><br><span class="line">        <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 确定页数输入框</span></span><br><span class="line">            input = wait.until(EC.presence_of_element_located(</span><br><span class="line">                (By.XPATH, <span class="string">'//*[@id="PageContgopage"]'</span>)))</span><br><span class="line">            input.click()</span><br><span class="line">            input.clear()</span><br><span class="line">            input.send_keys(page)</span><br><span class="line">            submit = wait.until(EC.element_to_be_clickable(</span><br><span class="line">                (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; a.btn_link'</span>)))</span><br><span class="line">            submit.click()</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 确认成功跳转到输入框中的指定页</span></span><br><span class="line">        wait.until(EC.text_to_be_present_in_element(</span><br><span class="line">            (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; span.at'</span>), str(page)))</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_table</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 提取表格第一种方法</span></span><br><span class="line">    <span class="comment"># element = wait.until(EC.presence_of_element_located((By.ID, "dt_1")))</span></span><br><span class="line">    <span class="comment"># 第二种方法</span></span><br><span class="line">    element = browser.find_element_by_css_selector(<span class="string">'#dt_1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取表格内容td</span></span><br><span class="line">    td_content = element.find_elements_by_tag_name(<span class="string">"td"</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> td <span class="keyword">in</span> td_content:</span><br><span class="line">        <span class="comment"># print(type(td.text)) # str</span></span><br><span class="line">        lst.append(td.text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定表格列数</span></span><br><span class="line">    col = len(element.find_elements_by_css_selector(<span class="string">'tr:nth-child(1) td'</span>))</span><br><span class="line">    <span class="comment"># 通过定位一行td的数量，可获得表格的列数，然后将list拆分为对应列数的子list</span></span><br><span class="line">    lst = [lst[i:i + col] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(lst), col)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 原网页中打开"详细"链接，可以查看更详细的数据，这里我们把url提取出来，方便后期查看</span></span><br><span class="line">    lst_link = []</span><br><span class="line">    links = element.find_elements_by_css_selector(<span class="string">'#dt_1 a.red'</span>)</span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">        url = link.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">        lst_link.append(url)</span><br><span class="line"></span><br><span class="line">    lst_link = pd.Series(lst_link)</span><br><span class="line">    <span class="comment"># list转为dataframe</span></span><br><span class="line">    df_table = pd.DataFrame(lst)</span><br><span class="line">    <span class="comment"># 添加url列</span></span><br><span class="line">    df_table[<span class="string">'url'</span>] = lst_link</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(df_table.head())</span></span><br><span class="line">    <span class="keyword">return</span> df_table</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file</span><span class="params">(df_table, category)</span>:</span></span><br><span class="line">    <span class="comment"># 设置文件保存在D盘eastmoney文件夹下</span></span><br><span class="line">    file_path = <span class="string">'D:\\eastmoney'</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">        os.mkdir(file_path)</span><br><span class="line">    os.chdir(file_path)</span><br><span class="line">    df_table.to_csv(<span class="string">'&#123;&#125;.csv'</span> .format(category), mode=<span class="string">'a'</span>,</span><br><span class="line">                    encoding=<span class="string">'utf_8_sig'</span>, index=<span class="number">0</span>, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置表格获取时间、类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_table</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line">    print(<span class="string">'\t\t\t\t东方财富网报表下载'</span>)</span><br><span class="line">    print(<span class="string">'作者：高级农民工  2018.10.6'</span>)</span><br><span class="line">    print(<span class="string">'--------------'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1 设置财务报表获取时期</span></span><br><span class="line">    year = int(float(input(<span class="string">'请输入要查询的年份(四位数2007-2018)：\n'</span>)))</span><br><span class="line">    <span class="comment"># int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10</span></span><br><span class="line">    <span class="keyword">while</span> (year &lt; <span class="number">2007</span> <span class="keyword">or</span> year &gt; <span class="number">2018</span>):</span><br><span class="line">        year = int(float(input(<span class="string">'年份数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    quarter = int(float(input(<span class="string">'请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：\n'</span>)))</span><br><span class="line">    <span class="keyword">while</span> (quarter &lt; <span class="number">1</span> <span class="keyword">or</span> quarter &gt; <span class="number">4</span>):</span><br><span class="line">        quarter = int(float(input(<span class="string">'季度数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充，</span></span><br><span class="line">    <span class="comment"># http://www.runoob.com/python/att-string-format.html</span></span><br><span class="line">    quarter = <span class="string">'&#123;:02d&#125;'</span>.format(quarter * <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># quarter = '%02d' %(int(month)*3)</span></span><br><span class="line">    date = <span class="string">'&#123;&#125;&#123;&#125;'</span> .format(year, quarter)</span><br><span class="line">    <span class="comment"># print(date) 测试日期 ok</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2 设置财务报表种类</span></span><br><span class="line">    tables = int(</span><br><span class="line">        input(<span class="string">'请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): \n'</span>))</span><br><span class="line"></span><br><span class="line">    dict_tables = &#123;<span class="number">1</span>: <span class="string">'业绩报表'</span>, <span class="number">2</span>: <span class="string">'业绩快报表'</span>, <span class="number">3</span>: <span class="string">'业绩预告表'</span>,</span><br><span class="line">                   <span class="number">4</span>: <span class="string">'预约披露时间表'</span>, <span class="number">5</span>: <span class="string">'资产负债表'</span>, <span class="number">6</span>: <span class="string">'利润表'</span>, <span class="number">7</span>: <span class="string">'现金流量表'</span>&#125;</span><br><span class="line">    dict = &#123;<span class="number">1</span>: <span class="string">'yjbb'</span>, <span class="number">2</span>: <span class="string">'yjkb/13'</span>, <span class="number">3</span>: <span class="string">'yjyg'</span>,</span><br><span class="line">            <span class="number">4</span>: <span class="string">'yysj'</span>, <span class="number">5</span>: <span class="string">'zcfz'</span>, <span class="number">6</span>: <span class="string">'lrb'</span>, <span class="number">7</span>: <span class="string">'xjll'</span>&#125;</span><br><span class="line">    category = dict[tables]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3 设置url</span></span><br><span class="line">    <span class="comment"># url = 'http://data.eastmoney.com/bbsj/201803/lrb.html' eg.</span></span><br><span class="line">    url = <span class="string">'http://data.eastmoney.com/&#123;&#125;/&#123;&#125;/&#123;&#125;.html'</span> .format(</span><br><span class="line">        <span class="string">'bbsj'</span>, date, category)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 4 选择爬取页数范围</span></span><br><span class="line">    start_page = int(input(<span class="string">'请输入下载起始页数：\n'</span>))</span><br><span class="line">    nums = input(<span class="string">'请输入要下载的页数，（若需下载全部则按回车）：\n'</span>)</span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定网页中的最后一页</span></span><br><span class="line">    browser.get(url)</span><br><span class="line">    <span class="comment"># 确定最后一页页数不直接用数字而是采用定位，因为不同时间段的页码会不一样</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        page = browser.find_element_by_css_selector(<span class="string">'.next+ a'</span>)  <span class="comment"># next节点后面的a节点</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        page = browser.find_element_by_css_selector(<span class="string">'.at+ a'</span>)</span><br><span class="line">    <span class="comment"># else:</span></span><br><span class="line">    <span class="comment">#     print('没有找到该节点')</span></span><br><span class="line">    <span class="comment"># 上面用try.except是因为绝大多数页码定位可用'.next+ a'，但是业绩快报表有的只有2页，无'.next+ a'节点</span></span><br><span class="line">    end_page = int(page.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> nums.isdigit():</span><br><span class="line">        end_page = start_page + int(nums)</span><br><span class="line">    <span class="keyword">elif</span> nums == <span class="string">''</span>:</span><br><span class="line">        end_page = end_page</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'页数输入错误'</span>)</span><br><span class="line">    <span class="comment"># 输入准备下载表格类型</span></span><br><span class="line">    print(<span class="string">'准备下载:&#123;&#125;-&#123;&#125;'</span> .format(date, dict_tables[tables]))</span><br><span class="line">    print(url)</span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>: url,</span><br><span class="line">        <span class="string">'category'</span>: dict_tables[tables],</span><br><span class="line">        <span class="string">'start_page'</span>: start_page,</span><br><span class="line">        <span class="string">'end_page'</span>: end_page</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(category, page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        index_page(page)</span><br><span class="line">        <span class="comment"># parse_table() #测试print</span></span><br><span class="line">        df_table = parse_table()</span><br><span class="line">        write_to_file(df_table, category)</span><br><span class="line">        print(<span class="string">'第 %s 页抓取完成'</span> % page)</span><br><span class="line">        print(<span class="string">'--------------'</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'网页爬取失败，请检查网页中表格内容是否存在'</span>)</span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> set_table():</span><br><span class="line">        <span class="comment"># url = i.get('url')</span></span><br><span class="line">        category = i.get(<span class="string">'category'</span>)</span><br><span class="line">        start_page = i.get(<span class="string">'start_page'</span>)</span><br><span class="line">        end_page = i.get(<span class="string">'end_page'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">        <span class="comment"># for page in range(44,pageall+1): # 如果下载中断，可以尝试手动更改网页继续下载</span></span><br><span class="line">        main(category, page)</span><br><span class="line">    print(<span class="string">'全部抓取完成'</span>)</span><br></pre></td></tr></table></figure><p>这里，我下载了所有上市公司的部分报表。</p><p>2018年中报业绩报表：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-6/90979397.jpg" alt=""></p><p>2017年报的利润表：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-10-6/66373311.jpg" alt=""></p><p>如果你想下载更多的报表，可以使用文中的代码，代码和素材资源可以在下面的链接中获取：</p><p><a href="https://github.com/makcyun/eastmoney_spider" target="_blank" rel="noopener">https://github.com/makcyun/eastmoney_spider</a></p><p>另外，爬虫还可以再完善一下，比如增加爬取上市公司的公告信息，设置可以爬任意一家（数家/行业）的公司数据而不用全部。</p><p>还有一个问题是，Selenium爬取的速度很慢而且很占用内存，建议尽量先尝试采用Requests请求的方法，抓不到的时候再考虑这个。文章开头在进行网页分析的时候，我们初步分析了表格JS的请求数据，是否能从该请求中找到我们需要的表格数据呢？ 后续文章，我们换一个思路再来尝试爬取一次。</p><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;利用Selenium爬取东方财富网各上市公司历年的财务报表数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Pyhton可视化(3): 中国66家环保股上市公司市值Top20强</title>
    <link href="https://www.makcyun.top/Python_visualization02.html"/>
    <id>https://www.makcyun.top/Python_visualization02.html</id>
    <published>2018-09-20T05:27:22.000Z</published>
    <updated>2018-10-12T00:07:41.759Z</updated>
    
    <content type="html"><![CDATA[<p>Tushare包提取中国环保股上市公司近10年市值排名，并结合D3.js做动态数据可视化表。</p><a id="more"></a>  <p><strong>摘要：</strong> 之前介绍过Tushare包和D3.js可视化动态表格，本文将二者进行结合，制作中国上市公司环境保护股Top20强，近十年的市值变化动态表。</p><p>制作近10年期间，环保板块市值最高的20只股票动态变化，需要获得各只股票在不同年份的市值。获取特定股票的市值可以利用<code>pro.daily_basic</code>接口获取到每日的市值，然后利用Resample函数获得年均市值。但获取环保板块所有几十只股票的数据，用手动输入股票代码就不是很方便，此时，可以利用该包另外一个接口<code>ts.get_stock_basics()接口</code>获取所有股票基本数据，该接口能够返回股票代码、行业类别等数据。两个接口合二为一就可以提取出所需的数据，下面开始详细实现步骤。</p><h2 id="1-提取所有股票代码"><a href="#1-提取所有股票代码" class="headerlink" title="1. 提取所有股票代码"></a>1. 提取所有股票代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="comment"># 获取所有股票列表</span></span><br><span class="line">data = ts.get_stock_basics()</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 返回数据如下，所有列值可以参考：http://tushare.org/fundamental.html</span></span><br><span class="line">         name industry area      pe  outstanding  totals  totalAssets  \</span><br><span class="line">code                                                                    </span><br><span class="line"><span class="number">002936</span>    N郑银       银行   河南    <span class="number">8.27</span>         <span class="number">6.00</span>   <span class="number">59.22</span>  <span class="number">44363604.00</span>   </span><br><span class="line"><span class="number">600856</span>   中天能源     供气供热   吉林   <span class="number">21.28</span>        <span class="number">13.43</span>   <span class="number">13.67</span>   <span class="number">1712831.63</span>   </span><br><span class="line"><span class="number">300021</span>   大禹节水     农业综合   甘肃   <span class="number">35.27</span>         <span class="number">6.48</span>    <span class="number">7.97</span>    <span class="number">359294.91</span>   </span><br><span class="line"><span class="number">603111</span>   康尼机电     运输设备   江苏    <span class="number">0.00</span>         <span class="number">7.38</span>    <span class="number">9.93</span>    <span class="number">734670.69</span>   </span><br><span class="line"><span class="number">000498</span>   山东路桥     建筑施工   山东   <span class="number">19.52</span>         <span class="number">4.41</span>   <span class="number">11.20</span>   <span class="number">1926262.38</span></span><br></pre></td></tr></table></figure><p>可以看到，index是股票代码，name股票名称，industry是行业分类。我们需要获取环保类（可以获取任意行业类别，也可以全部获取所有股票，为了后期数据提取量耗时短一些，所以选择提取环保类股票）的股票代码和股票名称，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data = data[data.industry ==<span class="string">'环境保护'</span>]</span><br><span class="line">print(data.head()) <span class="comment">#返回的环保股数据</span></span><br><span class="line">print(<span class="string">'环保股股票数量为'</span>：len(data.industry)) <span class="comment">#计算环保股股票数量</span></span><br><span class="line">结果如下：</span><br><span class="line">        name industry area     pe  outstanding  totals  totalAssets  \</span><br><span class="line">code                                                                  </span><br><span class="line"><span class="number">300056</span>   三维丝     环境保护   福建   <span class="number">0.00</span>         <span class="number">2.37</span>    <span class="number">3.85</span>    <span class="number">266673.63</span>   </span><br><span class="line"><span class="number">002549</span>  凯美特气     环境保护   湖南  <span class="number">34.44</span>         <span class="number">6.20</span>    <span class="number">6.24</span>    <span class="number">122630.13</span>   </span><br><span class="line"><span class="number">300422</span>   博世科     环境保护   广西  <span class="number">19.22</span>         <span class="number">2.64</span>    <span class="number">3.56</span>    <span class="number">509822.44</span>   </span><br><span class="line"><span class="number">601330</span>  绿色动力     环境保护   深圳  <span class="number">59.83</span>         <span class="number">1.16</span>   <span class="number">11.61</span>    <span class="number">784969.25</span>   </span><br><span class="line"><span class="number">000820</span>  神雾节能     环境保护   辽宁   <span class="number">0.00</span>         <span class="number">2.88</span>    <span class="number">6.37</span>    <span class="number">284674.34</span>   </span><br><span class="line">环保股股票数量为： <span class="number">66</span></span><br></pre></td></tr></table></figure><p>可以看到，环境保护股一共有66只，下面我们将用这66只股票的代码和名称，输入到<code>pro.daily_basic()接口</code>中，获取每只股票的每日数据，其中包括每日市值。时间期限从2009年1月1日至2018年9月10日，共10年的逐日数据。</p><h2 id="2-提股票每日市值"><a href="#2-提股票每日市值" class="headerlink" title="2. 提股票每日市值"></a>2. 提股票每日市值</h2><p>每日基本指标的数据接口：<a href="https://tushare.pro/document/2?doc_id=32" target="_blank" rel="noopener">https://tushare.pro/document/2?doc_id=32</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pro = ts.pro_api()</span><br><span class="line">pro.daily_basic(ts_code=<span class="string">''</span>, trade_date=<span class="string">''</span>,start_date = <span class="string">''</span>,end_date = <span class="string">''</span>)</span><br><span class="line"><span class="comment"># ts_code是股票代码，格式为000002.SZ,可以为一只股票，也可以是列表组成的多支股票</span></span><br><span class="line"><span class="comment"># 后面三个是交易日期，可以为固定日期，也可以为一个时期，格式'20180919'</span></span><br></pre></td></tr></table></figure><p>该接口股票代码的格式是<code>000002.SZ</code>，而上面股票代码格式是：<code>000002</code>，没有带后缀.SZ，由此需要添加上，然后就可获取每只股票近10年的逐日市值数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'code2'</span>] = data.index</span><br><span class="line"><span class="comment"># apply方法添加.SZ后缀</span></span><br><span class="line">data[<span class="string">'code2'</span>] = data[<span class="string">'code2'</span>].apply(<span class="keyword">lambda</span> i:i+<span class="string">'.SZ'</span>)</span><br><span class="line">data = data.set_index([<span class="string">'code2'</span>])</span><br><span class="line"><span class="comment"># 将code和name转为dict，因为我们只需要表格中的代码和名称列</span></span><br><span class="line">data = data[<span class="string">'name'</span>]</span><br><span class="line">data = data.to_dict()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(data) #测试返回的环保股字典数据 ok</span></span><br><span class="line">&#123;<span class="string">'300056.SZ'</span>: <span class="string">'三维丝'</span>, <span class="string">'002549.SZ'</span>: <span class="string">'凯美特气'</span>, <span class="string">'300422.SZ'</span>: <span class="string">'博世科'</span>, <span class="string">'601330.SZ'</span>: <span class="string">'绿色动力'</span>, <span class="string">'000820.SZ'</span>: <span class="string">'神雾节能'</span>, <span class="string">'300072.SZ'</span>: <span class="string">'三聚环保'</span>, <span class="string">'300055.SZ'</span>: <span class="string">'万邦达'</span>, <span class="string">'002717.SZ'</span>: <span class="string">'岭南股份'</span>, <span class="string">'300070.SZ'</span>: <span class="string">'碧水源'</span>, <span class="string">'000504.SZ'</span>: <span class="string">'南华生物'</span>, <span class="string">'300203.SZ'</span>: <span class="string">'聚光科技'</span>, <span class="string">'002672.SZ'</span>: <span class="string">'东江环保'</span>, <span class="string">'000967.SZ'</span>: <span class="string">'盈峰环境'</span>, <span class="string">'002322.SZ'</span>: <span class="string">'理工环科'</span>, <span class="string">'300272.SZ'</span>: <span class="string">'开能健康'</span>, <span class="string">'300495.SZ'</span>: <span class="string">'美尚生态'</span>, <span class="string">'603717.SZ'</span>: <span class="string">'天域生态'</span>, <span class="string">'300266.SZ'</span>: <span class="string">'兴源环境'</span>, <span class="string">'603126.SZ'</span>: <span class="string">'中材节能'</span>, <span class="string">'002200.SZ'</span>: <span class="string">'云投生态'</span>, <span class="string">'300385.SZ'</span>: <span class="string">'雪浪环境'</span>, <span class="string">'603200.SZ'</span>: <span class="string">'上海洗霸'</span>, <span class="string">'000826.SZ'</span>: <span class="string">'启迪桑德'</span>, <span class="string">'300262.SZ'</span>: <span class="string">'巴安水务'</span>, <span class="string">'002887.SZ'</span>: <span class="string">'绿茵生态'</span>, <span class="string">'603568.SZ'</span>: <span class="string">'伟明环保'</span>, <span class="string">'300631.SZ'</span>: <span class="string">'久吾高科'</span>, <span class="string">'002616.SZ'</span>: <span class="string">'长青集团'</span>, <span class="string">'300156.SZ'</span>: <span class="string">'神雾环保'</span>, <span class="string">'000920.SZ'</span>: <span class="string">'南方汇通'</span>, <span class="string">'600008.SZ'</span>: <span class="string">'首创股份'</span>, <span class="string">'601200.SZ'</span>: <span class="string">'上海环境'</span>, <span class="string">'603955.SZ'</span>: <span class="string">'大千生态'</span>, <span class="string">'603177.SZ'</span>: <span class="string">'德创环保'</span>, <span class="string">'600481.SZ'</span>: <span class="string">'双良节能'</span>, <span class="string">'300190.SZ'</span>: <span class="string">'维尔利'</span>, <span class="string">'603588.SZ'</span>: <span class="string">'高能环境'</span>, <span class="string">'002034.SZ'</span>: <span class="string">'旺能环境'</span>, <span class="string">'603817.SZ'</span>: <span class="string">'海峡环保'</span>, <span class="string">'002499.SZ'</span>: <span class="string">'科林环保'</span>, <span class="string">'603822.SZ'</span>: <span class="string">'嘉澳环保'</span>, <span class="string">'300664.SZ'</span>: <span class="string">'鹏鹞环保'</span>, <span class="string">'300332.SZ'</span>: <span class="string">'天壕环境'</span>, <span class="string">'600526.SZ'</span>: <span class="string">'菲达环保'</span>, <span class="string">'600874.SZ'</span>: <span class="string">'创业环保'</span>, <span class="string">'600292.SZ'</span>: <span class="string">'远达环保'</span>, <span class="string">'603903.SZ'</span>: <span class="string">'中持股份'</span>, <span class="string">'300172.SZ'</span>: <span class="string">'中电环保'</span>, <span class="string">'000544.SZ'</span>: <span class="string">'中原环保'</span>, <span class="string">'300692.SZ'</span>: <span class="string">'中环环保'</span>, <span class="string">'600388.SZ'</span>: <span class="string">'龙净环保'</span>, <span class="string">'300425.SZ'</span>: <span class="string">'环能科技'</span>, <span class="string">'300388.SZ'</span>: <span class="string">'国祯环保'</span>, <span class="string">'300362.SZ'</span>: <span class="string">'天翔环境'</span>, <span class="string">'300197.SZ'</span>: <span class="string">'铁汉生态'</span>, <span class="string">'300187.SZ'</span>: <span class="string">'永清环保'</span>, <span class="string">'300090.SZ'</span>: <span class="string">'盛运环保'</span>, <span class="string">'002573.SZ'</span>: <span class="string">'清新环境'</span>, <span class="string">'000035.SZ'</span>: <span class="string">'中国天楹'</span>, <span class="string">'603797.SZ'</span>: <span class="string">'联泰环保'</span>, <span class="string">'603603.SZ'</span>: <span class="string">'博天环境'</span>, <span class="string">'300137.SZ'</span>: <span class="string">'先河环保'</span>, <span class="string">'300355.SZ'</span>: <span class="string">'蒙草生态'</span>, <span class="string">'300152.SZ'</span>: <span class="string">'科融环境'</span>, <span class="string">'002658.SZ'</span>: <span class="string">'雪迪龙'</span>, <span class="string">'600217.SZ'</span>: <span class="string">'中再资环'</span>&#125;</span><br></pre></td></tr></table></figure><p>可以看到，很完整地显示了环保股的股票代码和名称，下面通过for循环即可获取每日数据。为了方便，将上式代码命名为一个函数get_code()，return data 为上面的dict。</p><h2 id="3-提取环保股公司数据"><a href="#3-提取环保股公司数据" class="headerlink" title="3. 提取环保股公司数据"></a>3. 提取环保股公司数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ts_codes = get_code()</span><br><span class="line">start = <span class="string">'20090101'</span></span><br><span class="line">end = <span class="string">'201809010'</span></span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> ts_codes.items():</span><br><span class="line">data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) <span class="comment"># 获取每只股票时间段数据</span></span><br><span class="line">    <span class="comment"># 添加代码列和名称列</span></span><br><span class="line">    <span class="comment"># 替换掉末尾的.SZ,regex设置为true才行</span></span><br><span class="line">    data[<span class="string">'code'</span>] = data[<span class="string">'ts_code'</span>].replace(<span class="string">'.SZ'</span>,<span class="string">''</span>,regex = <span class="keyword">True</span>)</span><br><span class="line">    data[<span class="string">'name'</span>] = value</span><br><span class="line">    <span class="comment"># 存储结果</span></span><br><span class="line">    data.to_csv(<span class="string">'environment.csv'</span>,mode=<span class="string">'a'</span>,encoding = <span class="string">'utf_8_sig'</span>,index = <span class="keyword">False</span>,header = <span class="number">0</span>)</span><br><span class="line">    print(<span class="string">'数据提取完毕'</span>)</span><br></pre></td></tr></table></figure><p>表格结果如下，66只股票10年一共产生了75933行数据。如果提前全部3000多家股票的数据，那么数据量会达到几百万行，量太大，所以这里仅提取了66支。其中，选中的列为每日市值（万元）。下面就可以根据日期、市值得到各只股票每年的市值均值，然后绘制股票动态表。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-20/15865243.jpg" alt=""></p><h2 id="4-绘制Top20强动态表"><a href="#4-绘制Top20强动态表" class="headerlink" title="4. 绘制Top20强动态表"></a>4. 绘制Top20强动态表</h2><p>首先读取上面的表格，获取DataFrame信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'environment.csv'</span>,encoding = <span class="string">'utf-8'</span>,converters = &#123;<span class="string">'code'</span>:str&#125;)</span><br><span class="line"><span class="comment"># converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示</span></span><br><span class="line">print(df.info())</span><br><span class="line"></span><br><span class="line">Data columns (total <span class="number">17</span> columns):</span><br><span class="line">ts_code          <span class="number">75932</span> non-null object</span><br><span class="line">trade_date       <span class="number">75932</span> non-null int64</span><br><span class="line">close            <span class="number">75932</span> non-null float64</span><br><span class="line">turnover_rate    <span class="number">75932</span> non-null float64</span><br><span class="line">volume_ratio     <span class="number">0</span> non-null float64</span><br><span class="line">pe               <span class="number">70861</span> non-null float64</span><br><span class="line">e_ttm            <span class="number">69254</span> non-null float64</span><br><span class="line">pb               <span class="number">73713</span> non-null float64</span><br><span class="line">ps               <span class="number">75932</span> non-null float64</span><br><span class="line">ps_ttm           <span class="number">75840</span> non-null float64</span><br><span class="line">total_share      <span class="number">75932</span> non-null float64</span><br><span class="line">float_share      <span class="number">75932</span> non-null float64</span><br><span class="line">free_share       <span class="number">75932</span> non-null float64</span><br><span class="line">total_mv         <span class="number">75932</span> non-null float64</span><br><span class="line">circ_mv          <span class="number">75932</span> non-null float64</span><br><span class="line">code             <span class="number">75932</span> non-null int64</span><br><span class="line">name             <span class="number">75932</span> non-null object</span><br><span class="line">dtypes: float64(<span class="number">13</span>), int64(<span class="number">2</span>), object(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>可以看到trade_date交易日期是整形，需将交易日期先转换为字符型再转换为datetime日期型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="comment"># trade_date是int型，需转为字符型</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = df[<span class="string">'trade_date'</span>].apply(str)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="comment"># df['trade_date'] = df['trade_date'].astype(str)</span></span><br><span class="line"><span class="comment"># 将object转为datatime</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = pd.to_datetime(df[<span class="string">'trade_date'</span>],format = <span class="string">'%Y%m%d'</span>,errors = <span class="string">'ignore'</span>) <span class="comment">#errors忽略无法转换的数据，不然会报错</span></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">Data columns (total <span class="number">17</span> columns):</span><br><span class="line">ts_code          <span class="number">75932</span> non-null object</span><br><span class="line">trade_date       <span class="number">75932</span> non-null datetime64[ns]</span><br><span class="line">close            <span class="number">75932</span> non-null float64</span><br><span class="line">turnover_rate    <span class="number">75932</span> non-null float64</span><br><span class="line">volume_ratio     <span class="number">0</span> non-null float64</span><br><span class="line">pe               <span class="number">70861</span> non-null float64</span><br><span class="line">e_ttm            <span class="number">69254</span> non-null float64</span><br><span class="line">pb               <span class="number">73713</span> non-null float64</span><br><span class="line">ps               <span class="number">75932</span> non-null float64</span><br><span class="line">ps_ttm           <span class="number">75840</span> non-null float64</span><br><span class="line">total_share      <span class="number">75932</span> non-null float64</span><br><span class="line">float_share      <span class="number">75932</span> non-null float64</span><br><span class="line">free_share       <span class="number">75932</span> non-null float64</span><br><span class="line">total_mv         <span class="number">75932</span> non-null float64</span><br><span class="line">circ_mv          <span class="number">75932</span> non-null float64</span><br><span class="line">code             <span class="number">75932</span> non-null int64</span><br><span class="line">name             <span class="number">75932</span> non-null object</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置总市值数字格式由万元变为亿元</span></span><br><span class="line">df[<span class="string">'total_mv'</span>] = (df[<span class="string">'total_mv'</span>]/<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># 保留四列,并将交易日期设为index</span></span><br><span class="line">df = df[[<span class="string">'ts_code'</span>,<span class="string">'trade_date'</span>,<span class="string">'total_mv'</span>,<span class="string">'name'</span>]]</span><br><span class="line">df = df.set_index(<span class="string">'trade_date'</span>)</span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 结果如下:</span></span><br><span class="line">               ts_code   total_mv  name</span><br><span class="line">trade_date                            </span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-30</span>  <span class="number">300090.</span>SZ  <span class="number">36.034715</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-29</span>  <span class="number">300090.</span>SZ  <span class="number">38.014644</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-28</span>  <span class="number">300090.</span>SZ  <span class="number">39.202602</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-27</span>  <span class="number">300090.</span>SZ  <span class="number">40.126569</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-24</span>  <span class="number">300090.</span>SZ  <span class="number">38.938611</span>  盛运环保</span><br></pre></td></tr></table></figure><p>接下来，求出每只股票每年的市值平均值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">求平均市值时需切片同一股票，这里股票名称切片赋值为value变量，也就是dict字典里<span class="number">66</span>只股票名称</span><br><span class="line">df = df[df.name == value]</span><br><span class="line"><span class="comment"># 不能用query方法,会报错 df = df.query('name == value')</span></span><br><span class="line"><span class="comment"># resampe按年统计数据</span></span><br><span class="line">df = df.resample(<span class="string">'AS'</span>).mean()  <span class="comment">#年平均市值</span></span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">              total_mv code</span><br><span class="line">trade_date                 </span><br><span class="line"><span class="number">2009</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">25.184678</span>  三维丝</span><br><span class="line"><span class="number">2010</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">50.672849</span>  三维丝</span><br><span class="line"><span class="number">2011</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">46.488004</span>  三维丝</span><br><span class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">39.214508</span>  三维丝</span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">59.110332</span>  三维丝</span><br><span class="line"><span class="comment"># 再用to_period按年显示市值数据</span></span><br><span class="line">df = df.to_period(<span class="string">'A'</span>)</span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 结果如下:</span></span><br><span class="line">                  total_mv code</span><br><span class="line">trade_date                 </span><br><span class="line"><span class="number">2009</span>         <span class="number">25.184678</span>  三维丝</span><br><span class="line"><span class="number">2010</span>         <span class="number">50.672849</span>  三维丝</span><br><span class="line"><span class="number">2011</span>         <span class="number">46.488004</span>  三维丝</span><br><span class="line"><span class="number">2012</span>         <span class="number">39.214508</span>  三维丝</span><br><span class="line"><span class="number">2013</span>         <span class="number">59.110332</span>  三维丝</span><br></pre></td></tr></table></figure><p>经过以上处理，基本就获得了想要的数据。为了能够满足D3.js模板表格条件，再做一点修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加code列</span></span><br><span class="line">df[<span class="string">'code'</span>] = value</span><br><span class="line"><span class="comment"># 重置index</span></span><br><span class="line">df = df.reset_index()</span><br><span class="line"><span class="comment"># 重命名为d3.js格式</span></span><br><span class="line"><span class="comment"># 增加一列空type</span></span><br><span class="line">df[<span class="string">'type'</span>] = <span class="string">''</span></span><br><span class="line">df = df[[<span class="string">'code'</span>,<span class="string">'type'</span>,<span class="string">'total_mv'</span>,<span class="string">'trade_date'</span>]]</span><br><span class="line">df.rename(columns = &#123;<span class="string">'code'</span>:<span class="string">'name'</span>,<span class="string">'total_mv'</span>:<span class="string">'value'</span>,<span class="string">'type'</span>:<span class="string">'type'</span>,<span class="string">'trade_date'</span>:<span class="string">'date'</span>&#125;)</span><br><span class="line"><span class="comment"># df.to_csv('parse_environment.csv',mode='a',encoding = 'utf_8_sig',index = False,float_format = '%.1f',header = 0)   # float_format = '%.1f' #设置输出浮点数格式为1位小数</span></span><br></pre></td></tr></table></figure><p>最终，生成parse_environment.csv文件如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">nametypevaluedate</span><br><span class="line">中国天楹<span class="number">8.8</span><span class="number">2009</span></span><br><span class="line">中国天楹<span class="number">9.8</span><span class="number">2010</span></span><br><span class="line">中国天楹<span class="number">15</span><span class="number">2011</span></span><br><span class="line">中国天楹<span class="number">18.8</span><span class="number">2012</span></span><br><span class="line">中国天楹<span class="number">22.5</span><span class="number">2013</span></span><br><span class="line">...</span><br><span class="line">东方园林<span class="number">177.1</span><span class="number">2014</span></span><br><span class="line">东方园林<span class="number">320.5</span><span class="number">2015</span></span><br><span class="line">东方园林<span class="number">288.4</span><span class="number">2016</span></span><br><span class="line">东方园林<span class="number">481</span><span class="number">2017</span></span><br><span class="line">东方园林<span class="number">461.5</span><span class="number">2018</span></span><br></pre></td></tr></table></figure><p>可绘制出动态可视化表格，见下面视频。可以看到前几年市值龙头由东方园林、碧桂园轮流坐庄，近两年三聚环保强势崛起，市值增长迅猛，跃居头名。</p><p><a href="https://www.bilibili.com/video/av32087716/" target="_blank" rel="noopener">https://www.bilibili.com/video/av32087716/</a></p><p>本文仅对比了环保股企业的市值变化，你还可以分析互联网股、金融股等100多种行业的企业市值对比。另外，Tushare包返回的参数还可以做更多其他分析。</p><p>文章完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">ts.set_token(<span class="string">'404ba015bd44c01cf09c8183dcd89bb9b25749057ff72b5f8671b9e6'</span>)</span><br><span class="line">pro = ts.pro_api()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_code</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 所有股票列表</span></span><br><span class="line">    data = ts.get_stock_basics()</span><br><span class="line">    <span class="comment"># data = data.query('industry == "环境保护"')</span></span><br><span class="line">    <span class="comment"># 或者</span></span><br><span class="line">    data = data[data.industry ==<span class="string">'环境保护'</span>]</span><br><span class="line">    <span class="comment"># 提取股票代码code并转化为list</span></span><br><span class="line">    data[<span class="string">'code2'</span>] = data.index</span><br><span class="line">    <span class="comment"># apply方法添加.SZ后缀</span></span><br><span class="line">    data[<span class="string">'code2'</span>] = data[<span class="string">'code2'</span>].apply(<span class="keyword">lambda</span> i:i+<span class="string">'.SZ'</span>)</span><br><span class="line">    data = data.set_index([<span class="string">'code2'</span>])</span><br><span class="line">    <span class="comment"># 将code和name转为dict</span></span><br><span class="line">    data = data[<span class="string">'name'</span>]</span><br><span class="line">    data = data.to_dict()</span><br><span class="line">    <span class="comment"># 增加东方园林</span></span><br><span class="line">    data[<span class="string">'002310.SZ'</span>] = <span class="string">'东方园林'</span></span><br><span class="line">    <span class="comment"># print(data) #测试返回的环保股dict ok</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stock</span><span class="params">(key,start,end,value)</span>:</span></span><br><span class="line">    data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) <span class="comment"># 获取每只股票时间段数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 替换掉末尾的.SZ,regex设置为true才行</span></span><br><span class="line">    data[<span class="string">'code'</span>] = data[<span class="string">'ts_code'</span>].replace(<span class="string">'.SZ'</span>,<span class="string">''</span>,regex = <span class="keyword">True</span>)</span><br><span class="line">    data[<span class="string">'name'</span>] = value</span><br><span class="line">    <span class="comment"># print(data)</span></span><br><span class="line">    data.to_csv(<span class="string">'environment.csv'</span>,mode=<span class="string">'a'</span>,encoding = <span class="string">'utf_8_sig'</span>,index = <span class="keyword">False</span>,header = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_code</span><span class="params">()</span>:</span></span><br><span class="line">    df = pd.read_csv(<span class="string">'environment.csv'</span>,encoding = <span class="string">'utf-8'</span>,converters = &#123;<span class="string">'code'</span>:str&#125;)</span><br><span class="line">    <span class="comment"># converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示</span></span><br><span class="line">    df.columns = [<span class="string">'ts_code'</span>,<span class="string">'trade_date'</span>,<span class="string">'close'</span>,<span class="string">'turnover_rate'</span>,<span class="string">'volume_ratio'</span>,<span class="string">'pe'</span>,<span class="string">'e_ttm'</span>,<span class="string">'pb'</span>,<span class="string">'ps'</span>,<span class="string">'ps_ttm'</span>,<span class="string">'total_share'</span>,<span class="string">'float_share'</span>,<span class="string">'free_share'</span>,<span class="string">'total_mv'</span>,<span class="string">'circ_mv'</span>, <span class="string">'code'</span>,<span class="string">'name'</span>]</span><br><span class="line">    <span class="comment"># trade_date是int型，需转为字符型</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = df[<span class="string">'trade_date'</span>].apply(str)</span><br><span class="line"><span class="comment"># 或者df['trade_date'] = df['trade_date'].astype(str)</span></span><br><span class="line"><span class="comment"># 将object转为datatime</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = pd.to_datetime(df[<span class="string">'trade_date'</span>],format = <span class="string">'%Y%m%d'</span>,errors = <span class="string">'ignore'</span>) <span class="comment">#errors忽略无法转换的数据，不然会报错</span></span><br><span class="line">    <span class="comment">## 设置总市值数字格式由万元变为亿元</span></span><br><span class="line">    df[<span class="string">'total_mv'</span>] = (df[<span class="string">'total_mv'</span>]/<span class="number">10000</span>)</span><br><span class="line">    <span class="comment"># 保留四列,并将交易日期设为index</span></span><br><span class="line">    df = df[[<span class="string">'ts_code'</span>,<span class="string">'trade_date'</span>,<span class="string">'total_mv'</span>,<span class="string">'name'</span>]]</span><br><span class="line">    df = df.set_index(<span class="string">'trade_date'</span>)</span><br><span class="line"></span><br><span class="line">    df = df[df.name == value]</span><br><span class="line">    <span class="comment"># # 不能用query方法</span></span><br><span class="line">    <span class="comment"># # df = df.query('name == ')</span></span><br><span class="line">    df = df.resample(<span class="string">'AS'</span>).mean()/<span class="number">10000</span>  <span class="comment">#年平均市值</span></span><br><span class="line">    df = df.to_period(<span class="string">'A'</span>)</span><br><span class="line">    <span class="comment"># # 增加code列</span></span><br><span class="line">    df[<span class="string">'code'</span>] = value</span><br><span class="line">    <span class="comment"># # 重置index</span></span><br><span class="line">    df = df.reset_index()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重命名为d3.js格式</span></span><br><span class="line">    <span class="comment"># 增加一列空type</span></span><br><span class="line">    df[<span class="string">'type'</span>] = <span class="string">''</span></span><br><span class="line">    df = df[[<span class="string">'code'</span>,<span class="string">'type'</span>,<span class="string">'total_mv'</span>,<span class="string">'trade_date'</span>]]</span><br><span class="line">    df.rename(columns = &#123;<span class="string">'code'</span>:<span class="string">'name'</span>,<span class="string">'total_mv'</span>:<span class="string">'value'</span>,<span class="string">'type'</span>:<span class="string">'type'</span>,<span class="string">'trade_date'</span>:<span class="string">'date'</span>&#125;)</span><br><span class="line">    df.to_csv(<span class="string">'parse_environment.csv'</span>,mode=<span class="string">'a'</span>,encoding = <span class="string">'utf_8_sig'</span>,index = <span class="keyword">False</span>,float_format = <span class="string">'%.1f'</span>,header = <span class="number">0</span>)</span><br><span class="line">    float_format = <span class="string">'%.1f'</span> <span class="comment">#设置输出浮点数格式</span></span><br><span class="line">    <span class="comment"># print(df)</span></span><br><span class="line">    <span class="comment"># print(df.info())</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># get_code()  #提取环保股dict</span></span><br><span class="line">    start = <span class="string">'20090101'</span></span><br><span class="line">    end = <span class="string">'201809010'</span></span><br><span class="line">    ts_codes = get_code()</span><br><span class="line">    <span class="comment"># dict_values转list</span></span><br><span class="line">    keys = list(ts_codes.keys())</span><br><span class="line">    values = list(ts_codes.values())</span><br><span class="line">    <span class="keyword">for</span> key,value <span class="keyword">in</span> ts_codes.items():</span><br><span class="line">        stock(key,start,end,value)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> values:</span><br><span class="line">        parse_code(value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>文件素材可以在这里获得：</p><p><a href="https://github.com/makcyun/web_scraping_with_python/tree/master/Tushare" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python/tree/master/Tushare</a></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tushare包提取中国环保股上市公司近10年市值排名，并结合D3.js做动态数据可视化表。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://www.makcyun.top/categories/Python/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>我的白血病妻子</title>
    <link href="https://www.makcyun.top/life01.html"/>
    <id>https://www.makcyun.top/life01.html</id>
    <published>2018-09-15T11:27:22.000Z</published>
    <updated>2018-09-21T01:41:38.458Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>世界骨髓捐献者日。</em></b><br><a id="more"></a>  </p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/81039212.jpg" alt=""></p><p>当夜晚来临，人们结束一天的忙碌，正赶着回家之际，我的一天才刚刚开始。守夜时间是晚七时至凌晨七时，我称之为”深夜医院”。你问我有没有人，我会说人还挺少的。</p><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>这是一篇关于一个年轻的家庭和白血病作斗争的生活随笔。<br><strong>人物介绍</strong>：我，苏克，27岁。妻子，”23”，25岁。儿子，馍馍，2岁半。<br><strong>时间</strong> ： 2018年5月至今<br><strong>地点</strong> ：东莞—广州—北京</p><h2 id="2-天使与恶魔"><a href="#2-天使与恶魔" class="headerlink" title="2. 天使与恶魔"></a>2. 天使与恶魔</h2><p><em>5月18日</em><br>23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”<br>我：”老婆，太棒了，我就说你可以的，你都找不到工作，那很多人更找不到了！”</p><p>23：”一直说想换工作，这次终于是跨出这一步了，很紧张啊！”<br>我：”不要紧张，你肯定能很快适应新工作的。”</p><p>23：”好吧，我答应 HR 6月1号去签合同，再上几天班就把年假休了，放松一下。”<br>我：”对啊，好好休息休息，我要是也有年假，就陪你出去玩了。”</p><p>23：”你在深圳，周末能有空回来就不错了…”<br>我：”以后，回东莞来就有时间了…”</p><p><em>一周后 5月25日 周五</em><br>23：”老公，我住院了。”<br>我：”啊？怎么回事？”</p><p>23：” 背痛了两天，痛地不行了，还发烧…”<br>我：”还是没有缓解啊，我刚下班，现在从梅林关坐大巴回来，到了马上去医院…”</p><p><em>一个多小时后…</em><br>我：”怎么样？医生怎么说?”<br>23：”医生看了血常规，说白细胞很高(29万多，正常是1万以内)，让我转到血液科来了，现在打消炎针退烧…”<br>我：”希望没大碍，早点退烧吧，这几天晚上我陪你。”</p><p><em>半夜…</em><br>23：”老公，我很热，出了好多汗，衣服全湿了…”<br>我：”我给你换，你额头不烫了，多出点汗烧就退下来了。”<br>23：”我背好疼…”<br>我：”忍受不了的痛么？我刚去找了医生，医生说忍一忍，坚强一点。”</p><p><em>3天后 周一</em><br>我：” 老婆，我得赶去深圳上班了，希望你这周能退烧。”<br>23：”我也希望，我这周五还要去公司签劳动合同呢。”</p><p><em>6月1日 周五</em><br>我：”老婆，今天儿童节，我在网上买了好多书给儿子，以后周末回来给他讲故事，争取做个会讲故事的好爸爸。”<br>23：”好，你快回来吧，我这几天反复高烧到40度，医生也找不到原因，让做骨穿还建议转院去广州”<br>我：”啊？太奇怪了，这次发烧怎么这么顽固，骨穿是做什么？我马上坐车回来…”</p><p><em>6月2日 转院到广州一家医院的血液科</em><br>当地医院的医生找不到病因，让我们转院到广州去。我一直不明白，发烧为什么要住到血液科病区。到了之后，医生给23抽了骨髓，说是查找病因。</p><p><em>一天后</em><br>医生单独跟我我和家里人说：”确诊病人患了急性淋巴细胞白血病，之前反复高烧和骨骼疼痛是这种病的常见临床表现。这种病得马上开始化疗，不然会有生命危险。” 听完，我一时没有反应过来。</p><p>后来了解到，如果一切顺利的话，治疗和康复时长大概要2年，治疗费用在100万左右；如果不顺利，生命可能只剩不到2年。</p><p>我突然明白了：原来这不是普通地发烧和背疼啊。不过，”白血病是什么病? 严重么？好治么？”</p><p>23那几天一直问：”检测结果有没有出来？到底是怎么回事？”</p><p>我说：”医生说需要十几天才能检测出来，别着急，这边的医生肯定会把你治好的。”</p><p>我偷偷百度了下，大致了解到白血病俗称”血癌”，是癌症的一种。后面一直在想，为什么23会得上这个病？<br>不管怎样，首先要做好长期住院治疗的准备，那么得尽快辞职了。</p><p><em>6月8日</em><br>我回到深圳，火速辞了职、退了租房。</p><p>晚上，关了灯，望着窗外灯火通明的深圳，看着这个刚刚来了3个月的城市。各种思绪开始涌上心头：23会很顺利地治好病么？未来的生活会是怎样？2年后进入社会我们还有竞争力么？很多刚毕业的人感叹生活艰难，一出来就要变成”房奴”。那我，如果不靠父母亲友，岂不是要变成”病奴”了？</p><p>想了好一会儿却什么没想出来，那么就顺其自然、虔诚祈祷吧。转而，开始回忆起这几年的走过的点点滴滴，想从曾面对过的种种困难中找到信心。</p><hr><p><br></p><h2 id="3-时光倒流"><a href="#3-时光倒流" class="headerlink" title="3. 时光倒流"></a>3. 时光倒流</h2><h3 id="3-1-2014年-浪漫邂逅"><a href="#3-1-2014年-浪漫邂逅" class="headerlink" title="3.1. 2014年-浪漫邂逅"></a>3.1. 2014年-浪漫邂逅</h3><p>2014年夏天，我本科毕业，从成都开启了筹划已久的70天毕业旅行。对于一个在新疆长大的内地人来说，22、3岁才第一次见到大海，那是一种难以言表的心情。整个旅途，我都尽可能地享受和体验每一处风土人情。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-14/12369483.jpg" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-13/50707742.jpg" alt=""></p><p>70天，不知不觉就快要结束，来到了最后一站：南亚岛国斯里兰卡。在一个青旅里我第一次见到了23。那种感觉用一个词形容就够了：”一见钟情”。聊了几次天之后，得知她在广州念大三，来斯里兰卡是到一个孤儿院做义工。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-14/40605269.jpg" alt=""></p><p>一周后，我到了她所在的城市”加勒”——一座濒临印度洋的小城镇。距离回国倒数的那几天里，我用一顿大盘鸡把她”收买”了。后来，我认为这是我人生中最重要的决定之一。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-13/47037291.jpg" alt=""></p><h3 id="3-2-2015年-约定终身"><a href="#3-2-2015年-约定终身" class="headerlink" title="3.2. 2015年-约定终身"></a>3.2. 2015年-约定终身</h3><p>回国后，我们开始异地，我在成都读研，她在广州读大四。这一年，我们的感情状况可以用”打飞的”来形容，她常来成都，我也常过去，还参加了她的毕业典礼。</p><p>最终，我们决定在8月，认识的一周年的那天领证结婚。婚礼仪式很简单，仅是请亲人朋友吃了一顿饭。</p><h3 id="3-3-2016年-馍馍出生"><a href="#3-3-2016年-馍馍出生" class="headerlink" title="3.3. 2016年-馍馍出生"></a>3.3. 2016年-馍馍出生</h3><p>2016年过年期间，感谢伟大的妈妈，把儿子馍馍带到了这个世界上。</p><p>坦诚地说，那种感觉是惊喜与压力并存。我们俩都没有什么经济来源，只能靠家里”救济”，我仅能每个月回来几天。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-14/48590797.jpg" alt=""></p><h3 id="3-4-2017年-毕业工作"><a href="#3-4-2017年-毕业工作" class="headerlink" title="3.4. 2017年-毕业工作"></a>3.4. 2017年-毕业工作</h3><p>就这样，异地状态维持了近3年，我终于毕业，可以回去抱娃和工作。曾经，八卦地转发过一篇文章给23说：”居然有人带娃参加毕业典礼”，没想到，说的也算是自己啊。</p><p>家也从西北搬到了东南，走过这一路，仅仅花了多数人二到三分之一的时间。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/54207175.jpg" alt=""></p><h3 id="3-5-2018年-再次异地"><a href="#3-5-2018年-再次异地" class="headerlink" title="3.5. 2018年-再次异地"></a>3.5. 2018年-再次异地</h3><p>在东莞工作了半年，2018年春节后，我决定还是去隔壁更大的深圳看看。<br>4月的某一天，我和23说：”要不你还是从政府出来吧，趁还年轻换个更有活力和挑战的工作环境。”<br>23：”是啊，为了馍馍在里面待了3年，我也想出来了。”<br>我：”那我们就开始找找看吧。”</p><p><em>一个月后的5月18日</em><br>23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”(回到了文章的开头）</p><p>窗外的霓虹灯渐渐地黯淡下来，发现已经半夜，赶紧睡吧，明天得赶去广州的医院，要开始新的人生节奏了。</p><hr><p><br></p><h2 id="4-难熬的化疗"><a href="#4-难熬的化疗" class="headerlink" title="4. 难熬的化疗"></a>4. 难熬的化疗</h2><h3 id="4-1-治疗过程"><a href="#4-1-治疗过程" class="headerlink" title="4.1. 治疗过程"></a>4.1. 治疗过程</h3><p>6月2日，23在广州医院住下来，仍然在发烧并伴随强烈的骨痛。前3天体温持续超过38度，半夜最高烧到了40.3度。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-14/13426921.jpg" alt=""></p><p>6月4日下午，瞒着23马上开始进行了化疗药物注射。说起化疗，我脑子里最先想到的是赵本山和范伟演过的一个小品，里面戏称化疗为”谈话治疗”。可实际中的化疗远比这难受地多。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/4399129.jpg" alt=""></p><p>23对药物反应特别大，不停呕吐、浑身乏力、也没有任何食欲，体重开始暴瘦。连续打了三天化疗针以后，总算压制住了体内的恶性白细胞，23不再发烧和骨痛，整个人才渐渐恢复精神，开始有胃口喝粥和下床活动一下。随着在病区看到和听到医生、护士、病友关于白血病的治疗和讲解，23慢慢接受了自己得病的事实。化疗的第14天，出现了掉头发现象后，她同意剪发，过肩的长发被剪成了齐耳短发。好在，她很看得开，说：”这样很方便，至少出汗不用那么难受了”。</p><p>没有料到，头发脱落的速度是那么地快。仅仅2天过后，头发稍微一抓就是一把，整个床上和地下到处都是，打扫卫生的阿姨说”快点剪了吧，打扫卫生不方便”。坚持了几天后，在出院的那一天，给23彻底剪光了所有的头发。她依然很释怀，还在朋友圈让大家推荐戴哪个假发好，我心里感到很意外。</p><p>在病房，23每天要经历的三件事就是输液、抽血和用药。 </p><p>大部分的时间都是在输液，各种各样的药物通过一袋袋的氯化钠和葡萄糖注射进23的血液里，有时是单手臂，有时双手都要打。同时，几乎每天要扎针抽血，各种颜色的抽血管，到后期手背已全部瘀青。至于服药，就像一日三餐一样稀松平常。就这样，每一天都仿佛在重复着昨天。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/31936040.jpg" alt=""></p><p>整个疗程22天下来，一共输入了114袋液体，平均每天5袋，重达23200 ml，大致和一个5岁的小孩体重差不多。一共抽了81管血液，平均每天4管。注射和服用各种各样的药物加起来，有60种之多。化疗两周后，身高168 cm 的23体重降到了79斤，搀扶她上厕所，摸到的都是骨骼。多数女生喜欢越瘦越好，瘦到这种程度可能就不喜欢了吧。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/84469243.jpg" alt=""></p><h3 id="4-2-23的感受"><a href="#4-2-23的感受" class="headerlink" title="4.2. 23的感受"></a>4.2. 23的感受</h3><p>尽管生理和心理都遭受着从未体验过的艰难考验，23的表现大大超出了我的心理预估。不难受的时候，她会发一些朋友圈，每个人看到都会情不自禁地被感动到。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/98531897.jpg" alt=""></p><p>要说，会让23难受的一件事，那就是见不到儿子。从他出生下来至今，从来没有这么多天看不到他。所以，每次和馍馍视频，她都特别开心。</p><p>有一天晚上，打电话回家，发现突然找不到馍馍，23急得要命。后来，终于在一个小剧场找到了他，原来和小伙伴在一起。23想哭又有点生气，训了他几句。之后，我妈说原来儿子是想跟着小伙伴的妈妈去进剧场看节目，但他没有票就赖在那里不想走。他在那里哭着说：”我妈妈在就好了，妈妈在就会给我买票。”  </p><p>听完23就哭了，我也鼻头一酸，2岁半的他竟然会讲出这样的话。</p><p><em>6月23日</em><br>住了3周院后，23终于能踏出病区，呼吸到外面的空气。回家的路上，她一直看着车窗外，没有说一句话。回到家后第一件事就是抱馍馍，馍馍居然很自然地接受了光头的妈妈。回到房间之后，23突然大哭了一场，吓了我一跳，急忙问怎么了，她说：”活着真好。”</p><h3 id="4-3-陪伴"><a href="#4-3-陪伴" class="headerlink" title="4.3. 陪伴"></a>4.3. 陪伴</h3><p>我一直都是一个大老粗，不怎么会照顾人，更不用说病人。好在，家里人给予了非常强力的支持。在23身边，做的最多的仅仅是陪伴。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/17625322.jpg" alt=""></p><hr><p><br></p><h2 id="5-关于白血病"><a href="#5-关于白血病" class="headerlink" title="5. 关于白血病"></a>5. 关于白血病</h2><h3 id="5-1-什么是白血病"><a href="#5-1-什么是白血病" class="headerlink" title="5.1. 什么是白血病"></a>5.1. 什么是白血病</h3><p>在此之前，我的医学常识差到连感冒、发烧该吃什么药都不太清楚。白血病？我甚至有百度过：是不是血液是白色的病？(最好不要去百度)</p><p>在医院住久了之后，慢慢地开始对这个病有所了解。</p><blockquote><p><strong>白血病</strong>，是一种骨髓中的白细胞大量、异常增生的癌症。这些白细胞会破坏骨髓的造血功能，转而取代正常细胞。初期会导致病人出现发烧、贫血、感染等症状，具体发病原因至今没有完全弄清楚。</p></blockquote><p>虽然是癌症，但受益于科技发展，目前白血病是可以根治的。5年的生存率总体在50%以上，儿童高于成年人，且不同类型的白血病存活率也会不一样。根据细胞类型和发病程度，可将白血病分为四种主要类型：</p><table><thead><tr><th style="text-align:center">细胞类型</th><th style="text-align:center">急性</th><th style="text-align:center">慢性</th></tr></thead><tbody><tr><td style="text-align:center">淋巴细胞白血病</td><td style="text-align:center">急性淋巴细胞白血病</td><td style="text-align:center">慢性淋巴细胞白血病</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">（ALL）</td><td style="text-align:center">（CLL ）</td></tr><tr><td style="text-align:center">髓细胞白血病</td><td style="text-align:center">急性髓性白血病</td><td style="text-align:center">慢性粒细胞白血病</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">（AML ）</td><td style="text-align:center">（CML）</td></tr></tbody></table><p>儿童中，四分之三患上的是ALL；成人中，AML  和CLL 最为常见，《我不是药神》里面说的是CML。23患上的是ALL，并且属于其中的高危型，存活率为50%。ALL接受造血干细胞移植且5年内没有复发的话，之后终身基本上就不会再复发，也就是会和正常人一样。</p><p>普通个体患这种病的概率是十万分之1-1.5，所以很多人对这个病比较陌生，觉得离自己很遥远。其实，不用觉得陌生和遥远。像宋庆龄、肯德基创始人、居里夫人等这些名人就是罹患这种病而逝世的。根据中国红十字会统计，<a href="https://new.crcf.org.cn/html/2012-07/1733.html" target="_blank" rel="noopener">2012年中国内地有400万患者，并且每年新增4万人。</a></p><h3 id="5-2-治疗方法"><a href="#5-2-治疗方法" class="headerlink" title="5.2. 治疗方法"></a>5.2. 治疗方法</h3><p>白血病的治疗通常是采用：化疗、放疗、干细胞移植中的一种或者组合。以23所属的ALL高危类型来说，治疗主要分为三个阶段：化疗缓解、造血干细胞移植和排异控制和恢复。</p><h4 id="5-2-1-化疗"><a href="#5-2-1-化疗" class="headerlink" title="5.2.1. 化疗"></a>5.2.1. 化疗</h4><p>刚患病时，骨髓中会产生超出常人数十倍的白细胞，这么多的白细胞需要采用高强度的化疗进行治疗。最终将骨髓中的白血病原始细胞减少至&lt;5％，并从血液中清除肿瘤细胞。这个阶段的治疗通常需要3-6个疗程，然后为下一个阶段做准备。</p><h4 id="5-2-2-造血干细胞捐献移植"><a href="#5-2-2-造血干细胞捐献移植" class="headerlink" title="5.2.2. 造血干细胞捐献移植"></a>5.2.2. 造血干细胞捐献移植</h4><p>干细胞具有自我更新、繁殖并能分化成不同种类的成熟细胞的能力。造血干细胞是一群未分化的血液细胞，可以制造运输氧气的红血球、帮助凝血的血小板、抵抗感染的白血球等。所以，造血干细胞可用来治疗许多包括白血病在内的血液疾病、肿瘤等。它存在于骨髓、婴儿脐带血、以及成人周边血液中。</p><p>不少人会认为捐献造血干细胞就是抽取骨髓，而抽取骨髓会有很大风险甚至会残疾。所以常出现志愿者反悔的情形，给出的理由常常是”父母不同意”、”儿女不同意”，甚至是”丈母娘不同意”、”领导不同意”。其实，骨髓捐献造血干细胞已经是很久以前的方法了，现在90%以上的捐献者采用的捐献周边血干细胞。其实，这就是一个加强版的抽血过程，血液中左手臂抽出经过一个血液分离机，提取出干细胞，然后再经过右手臂回输到自己身体中。整个过程持续半天，最终抽取出200-300ml的血液就算完成捐献了。</p><p>对于病人来说，造血干细胞移植，就是利用供者的干细胞的造血能力全部换掉自身体内的血液，从而彻底清除癌变的白细胞，然后重新造血。因此，病人的血型在移植完后会变成供体的血型。</p><p>供体的选择，通常是按照：兄弟姐妹、直系亲属、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序去筛选寻找。</p><p>判断供体和不合适，其实和常人输液一样，如果你是A型血，那你需要同样是A型血的人，B型血的人就爱莫能助。只不过移植不是用血型来判断，而是根据HLA（人类白细胞抗原）。</p><blockquote><p>HLA（人类白细胞抗原）是编码人类的主要组织相容性复合体的基因，负责调节人体免疫系统。简单地说，HLA是人体生物学的”身份证”，由父母遗传，能识别”自己”和”非己”，会通过免疫反应排除”非己”。所以，HLA在造血干细胞移植的成败中起着重要的作用。造血干细胞移植就需要供体和受体HLA配型相同，如果不相同就会发生致命的排斥反应。</p></blockquote><p>由于不同人种、不同种族、不同个体的HLA差别很大，所以要采用一种方法来确定HLA。判断两个人的HLA是否相同，只需要抽8ml的血，做HLA高分辨检测化验，然后将HLA上5组基因的数字进行对比就能判定。</p><p>简单地举个例子，供体1的10组数字和受体全部对应，称为”全相合”，这是最理想的，移植效果也最好；供体2有5组数字和受体对应，称为”半相合”，如果是直系亲属，那么可以移植，如果是非血缘供提，那么无法移植；供体3一组数字都对不上，称为”零相合”，完全不可用。类似地还会有9组数字对应的情况，称为9个点匹配，以此类推。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/47625856.jpg" alt=""></p><p>尽管判断HLA是否相同很简单，但是供体的选择范围其实很有限。按照兄弟姐妹、父母子女、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序来解释一下。</p><ul><li><p>兄弟姐妹</p><p>根据染色体的基本常识，正常情况下，子女的染色体一半来自父亲、一半来自母亲。可以知道，兄弟姐妹有25%的概率是全相合、50%的概率是半相合、剩下25%则是零相合。可以说，只要有兄弟姐妹，供体基本就找到了，但很多人没有兄弟姐妹，尤其90后。</p></li><li><p>父母子女</p><p>没有兄弟姐妹的人，可以将父母子、女身作为潜在的供体对象。绝大多数情况下，父母和子女之间是半相合，这是最常见的。目前，国内半相合移植技术发展地很好，很多患者才得以生存下来。但是也有条件限制，比如在健康的条件下，父母年龄最好不要超过60岁，子女体重要达到一定要求才行。</p></li><li><p>堂（表）兄弟姐妹</p><p>在堂（表）兄弟姐妹中，这个概率其实就比较低了，但如果前两类人群中无法找到，也只能去试一下。</p></li><li><p>骨髓库志愿者</p><p>上面三类人群都无法找到，那么只能寄希望于骨髓库中的志愿者了。没有血缘关系的人，HLA能匹配上的概率非常低，只有数十万到上百万分之一！但依然得抱着希望去尝试，因为这是最后一根救命稻草。</p></li></ul><h4 id="5-2-3-控制排异"><a href="#5-2-3-控制排异" class="headerlink" title="5.2.3. 控制排异"></a>5.2.3. 控制排异</h4><p>移植完成后，供体的干细胞进入病人身体，自身细胞会视为外来敌人然后会发生排斥。可以比喻成战争，视战役的惨烈程度分为两种情况：第一种小规模或者和平停战，那么移植算成功，慢慢恢复就能康复起来；第二种是拼到了两败俱伤的境地，就会引起严重的排异反应，这就会致命。控制排异和恢复的过程需要持续数年。</p><h3 id="5-3-治疗费用"><a href="#5-3-治疗费用" class="headerlink" title="5.3. 治疗费用"></a>5.3. 治疗费用</h3><p>根据治疗方案就可以大致计算白血病的治疗费用。23的治疗费用主要包括：化疗费用+移植费用+排异控制费用+服药费用。简单做了一下费用预估：</p><ul><li>化疗费用15-40万</li><li>移植费用30-50万</li><li>排异控制费用：可能几万，也可能几百万</li><li>口服药（几万到数十万）。</li></ul><p>拿口服药举个例子，服用进口药，60片共3g要8500元。目前，千足金（99.99%）的价格是265元/g，这种药的单价是其10倍不止。一瓶够服用一个月，服用时间按年计算。</p><p>所以，白血病可能50万能治好，也可能花了500万，仍然留不住病人。</p><h2 id="6-希望之光"><a href="#6-希望之光" class="headerlink" title="6. 希望之光"></a>6. 希望之光</h2><p>很多亲朋好友在得知我们的情况后，非常关心并且给予了很多的帮助，让我们不要有压力。</p><p>我觉得，压力大不大最好是去对比一下。一对比，很容易就能得出答案。3个多月以来，住了3家医院，接触和听说过不少病友的故事。比如：有怀身孕6个月的妻子照顾患病的丈夫、这辈子几乎无法再当妈妈的未婚女生、撇下2岁多孩子消失的父亲等等。</p><p>相比之下，我们的压力不算大，唯一的压力来自于供体的寻找。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-15/14687057.jpg" alt=""></p><p><em>9月13日</em><br>  医生告诉了一个久违的好消息：在台湾骨髓库找到了一个全相合且有意愿的供者。全家听到后都特别的开心，祈祷这位同胞体检能够通过，最终顺利捐出造血干细胞。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-16/47265044.jpg" alt=""></p><p><em>2015年，世界骨髓捐献者协会宣布：每年9月的第3个周六，为”世界骨髓捐献者日”。</em></p><p>截至2018年9月，全球骨髓库有<a href="https://worldmarrowdonorday.org/" target="_blank" rel="noopener">3250万志愿者</a>，占全世界75亿人的千分之四左右。中国内地排在美国、德国和巴西之后，位列世界第四，志愿者的人数为250万，仅占总人口比例的0.18%，比其他国家（地区）低一个数量级。250万这个数字，还只是登记在库内的初查人数，实际上真正愿意做志愿者的人，会比这低很多。</p><p><strong>从2001年成立到现在，17年只捐献了7600例，意味着最多有7600个患者得到了救治，每年只有平均450人。</strong>第一大骨髓库美国，30年来也不过，只有8万人得到了救治。<strong>可以说，不仅国内，全世界骨髓库都很需要志愿者的加入。</strong></p><table><thead><tr><th style="text-align:center">骨髓库</th><th style="text-align:center">成立时间（年）</th><th style="text-align:center">库容志愿者人数（万）</th><th style="text-align:center">人口（亿）</th><th style="text-align:center">占人口比例</th><th style="text-align:center">捐献造血干细胞例数</th></tr></thead><tbody><tr><td style="text-align:center">美国</td><td style="text-align:center">1988</td><td style="text-align:center">900</td><td style="text-align:center">3.3</td><td style="text-align:center">2.73%</td><td style="text-align:center"><a href="https://bethematch.org/workarea/downloadasset.aspx?id=1968" target="_blank" rel="noopener">80,000</a></td></tr><tr><td style="text-align:center">德国</td><td style="text-align:center">1993</td><td style="text-align:center">400</td><td style="text-align:center">0.83</td><td style="text-align:center">9.37%</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">巴西</td><td style="text-align:center">-</td><td style="text-align:center">500</td><td style="text-align:center">2.12</td><td style="text-align:center">2.36%</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">中国</td><td style="text-align:center">2001</td><td style="text-align:center"><a href="http://www.cmdp.org.cn/" target="_blank" rel="noopener">250</a></td><td style="text-align:center">13.9</td><td style="text-align:center"><strong>0.18%</strong></td><td style="text-align:center">7,600</td></tr><tr><td style="text-align:center">台湾</td><td style="text-align:center">1993</td><td style="text-align:center"><a href="http://btcscc.tzuchi.com.tw/index.php?option=com_content&amp;view=article&amp;id=1146%3A2014-01-02-06-00-06&amp;catid=185%3A2013-09-24-06-23-25&amp;Itemid=558&amp;lang=tw" target="_blank" rel="noopener">43</a></td><td style="text-align:center">0.24</td><td style="text-align:center">1.79%</td><td style="text-align:center">5,100</td></tr></tbody></table><p><em>参考来源：<a href="https://www.wmda.info/wp-content/uploads/2018/06/20180531-GTR-SearchMatch-2017.pdf" target="_blank" rel="noopener">WMDA global trend report 2017</a>、  <a href="http://lnmdp.org.admin.yuntai.net/readnews.aspx?n_id=1817&amp;type=4" target="_blank" rel="noopener">http://lnmdp.org.admin.yuntai.net/readnews.aspx?n_id=1817&amp;type=4</a></em></p><p>假如，大陆志愿者的人口比例，能够达到台湾的比例，<strong>那么，这一数字将会从250万变成2500万！</strong></p><p>成为志愿者的了解渠道：</p><p><a href="http://www.cmdp.org.cn/show/1020166.html" target="_blank" rel="noopener">http://www.cmdp.org.cn/show/1020166.html</a></p><p>本文完。</p><p>链接：</p><blockquote><p>世界骨髓捐献者协会WMDA：<a href="https://www.wmda.info/" target="_blank" rel="noopener">https://www.wmda.info/</a></p><p>世界骨髓捐献者日网站WMDD：<a href="https://worldmarrowdonorday.org/" target="_blank" rel="noopener">https://worldmarrowdonorday.org/</a></p><p>中华骨髓库CMDP：<a href="http://www.cmdp.org.cn/" target="_blank" rel="noopener">http://www.cmdp.org.cn/</a></p><p>台湾慈濟骨髓幹細胞中心BTCSCC：<a href="http://btcscc.tzuchi.com.tw/" target="_blank" rel="noopener">http://btcscc.tzuchi.com.tw/</a></p><p>香港骨髓庫：<a href="https://www5.ha.org.hk/rcbts/hkarticle.asp?bid=56&amp;MenuID=6#.W52iwfl4nDc" target="_blank" rel="noopener">https://www5.ha.org.hk/rcbts/hkarticle.asp?bid=56&amp;MenuID=6#.W52iwfl4nDc</a></p><p>美国骨髓库NMDP：<a href="https://bethematch.org/" target="_blank" rel="noopener">https://bethematch.org/</a></p><p>欧洲骨髓库EMDIS：<a href="www.emdis.net/">www.emdis.net/</a></p><p>德国骨髓库ZKRD：<a href="https://www.zkrd.de/de/" target="_blank" rel="noopener">https://www.zkrd.de/de/</a></p><p>日本骨髓库JMDP： <a href="http://www.jmdp.or.jp/" target="_blank" rel="noopener">http://www.jmdp.or.jp/</a></p><hr><p>白血病介绍：<a href="https://en.m.wikipedia.org/wiki/Leukemia" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Leukemia</a></p><p>急性淋巴细胞白血病介绍： <a href="https://en.m.wikipedia.org/wiki/Acute_lymphoblastic_leukemia" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Acute_lymphoblastic_leukemia</a></p><p>HLA介绍：<a href="https://en.m.wikipedia.org/wiki/Human_leukocyte_antigen" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Human_leukocyte_antigen</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;世界骨髓捐献者日。&lt;/em&gt;&lt;/b&gt;&lt;br&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="白血病" scheme="https://www.makcyun.top/tags/%E7%99%BD%E8%A1%80%E7%97%85/"/>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Python数据处理分析(1)：日期型数据处理</title>
    <link href="https://www.makcyun.top/python_data_analysis01.html"/>
    <id>https://www.makcyun.top/python_data_analysis01.html</id>
    <published>2018-09-11T01:36:41.000Z</published>
    <updated>2018-09-24T09:33:46.374Z</updated>
    
    <content type="html"><![CDATA[<p>以调用大名鼎鼎的Tushare包案例，介绍日期数据的处理。</p><a id="more"></a>  <p><strong>摘要：</strong> Python数据处理分析中，日期型数据的处理是相对复杂且非常重要的一环。本文以调用Tushare包获得股票的各种信息数据为案例，介绍日期数据的处理。</p><p>之前的<a href="https://www.makcyun.top/web_scraping_withpython2.html">一篇文章</a>用爬虫实现了上市公司信息的抓取。但还有更简单的方法，就是调用Tushare包，可以很便捷地拿到干净的各种股市数据。</p><p>强烈推荐一下这款由国内团队开发的包，Github上目前Star数 6000+。Tushare是一个开源免费、强大的python金融财经数据接口包。调用该包返回的数据格式基本是Pandas DataFrame类型，非常便于后续处理分析。包的数据来源于新浪财经、腾讯财经、上交所和深交所，比较齐全，质量也很可靠。</p><blockquote><p>参考：<br><a href="https://tushare.pro/document/2" target="_blank" rel="noopener">https://tushare.pro/document/2</a><br><a href="https://github.com/waditu/Tushare" target="_blank" rel="noopener">https://github.com/waditu/Tushare</a></p></blockquote><p>下面我们就来简单体检一下这款包的便利，然后利用它返回的数据处理其中的日期型数据。</p><h2 id="1-获取数据"><a href="#1-获取数据" class="headerlink" title="1. 获取数据"></a>1. 获取数据</h2><p>接口使用前提：首先在官网注册成功后获得token，然后通过下面命令下载Tushare包，然后在程序中调用就可以使用了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip instasll tushare</span><br></pre></td></tr></table></figure></p><p>可以获得的信息接口非常多，包括：行情数据、基础数据、财务数据板块等。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-11/86932586.jpg" alt="">  </p><p>下面就简单使用下部分接口。首先，获取国内股票列表数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">ts.set_token(<span class="string">'你的token'</span>)</span><br><span class="line">pro = ts.pro_api()</span><br><span class="line">data = pro.stock_basic(exchange_id=<span class="string">''</span>, is_hs=<span class="string">''</span>, fields=<span class="string">'symbol,name,is_hs,list_date,list_status'</span>)</span><br><span class="line">print(data)</span><br><span class="line"><span class="comment"># ''表示获取全部</span></span><br></pre></td></tr></table></figure></p><p><code>exchange_id</code>表示股票代码，可以获取特定股票的基础信息，为空则获取全部;<code>is_hs</code>表示是否沪深港通，为空表示提取所有股市；<code>fields</code>表示想要提取的信息列表。</p><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">        ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line"><span class="number">0</span>     <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L   <span class="number">19910403</span>     S</span><br><span class="line"><span class="number">1</span>     <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L   <span class="number">19910129</span>     S</span><br><span class="line"><span class="number">2</span>     <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L   <span class="number">19910114</span>     N</span><br><span class="line"><span class="number">3</span>     <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L   <span class="number">19901210</span>     N</span><br><span class="line"><span class="number">4</span>     <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L   <span class="number">19920427</span>     S</span><br><span class="line"><span class="number">5</span>     <span class="number">000007.</span>SZ  <span class="number">000007</span>   全新好           L   <span class="number">19920413</span>     N</span><br><span class="line"><span class="number">6</span>     <span class="number">000008.</span>SZ  <span class="number">000008</span>  神州高铁           L   <span class="number">19920507</span>     S</span><br><span class="line"><span class="number">7</span>     <span class="number">000009.</span>SZ  <span class="number">000009</span>  中国宝安           L   <span class="number">19910625</span>     S</span><br><span class="line"><span class="number">8</span>     <span class="number">000010.</span>SZ  <span class="number">000010</span>  美丽生态           L   <span class="number">19951027</span>     N</span><br><span class="line"><span class="number">9</span>     <span class="number">000011.</span>SZ  <span class="number">000011</span>  深物业A           L   <span class="number">19920330</span>     S</span><br><span class="line"><span class="number">10</span>    <span class="number">000012.</span>SZ  <span class="number">000012</span>   南玻A           L   <span class="number">19920228</span>     S</span><br><span class="line">···</span><br><span class="line"><span class="number">3532</span>  <span class="number">603987.</span>SH  <span class="number">603987</span>   康德莱           L   <span class="number">20161121</span>     N</span><br><span class="line"><span class="number">3533</span>  <span class="number">603988.</span>SH  <span class="number">603988</span>  中电电机           L   <span class="number">20141104</span>     N</span><br><span class="line"><span class="number">3534</span>  <span class="number">603989.</span>SH  <span class="number">603989</span>  艾华集团           L   <span class="number">20150515</span>     H</span><br><span class="line"><span class="number">3535</span>  <span class="number">603990.</span>SH  <span class="number">603990</span>  麦迪科技           L   <span class="number">20161208</span>     N</span><br><span class="line"><span class="number">3536</span>  <span class="number">603991.</span>SH  <span class="number">603991</span>  至正股份           L   <span class="number">20170308</span>     N</span><br><span class="line"><span class="number">3537</span>  <span class="number">603993.</span>SH  <span class="number">603993</span>  洛阳钼业           L   <span class="number">20121009</span>     H</span><br><span class="line"><span class="number">3538</span>  <span class="number">603996.</span>SH  <span class="number">603996</span>  中新科技           L   <span class="number">20151222</span>     N</span><br><span class="line"><span class="number">3539</span>  <span class="number">603997.</span>SH  <span class="number">603997</span>  继峰股份           L   <span class="number">20150302</span>     H</span><br><span class="line"><span class="number">3540</span>  <span class="number">603998.</span>SH  <span class="number">603998</span>  方盛制药           L   <span class="number">20141205</span>     N</span><br><span class="line"><span class="number">3541</span>  <span class="number">603999.</span>SH  <span class="number">603999</span>  读者传媒           L   <span class="number">20151210</span>     N</span><br></pre></td></tr></table></figure><p>很轻松地就能获得3542家上市公司的基本情况。下面就将这个数据作为日期型处理的基础数据。</p><h2 id="2-日期型数据处理"><a href="#2-日期型数据处理" class="headerlink" title="2. 日期型数据处理"></a>2. 日期型数据处理</h2><p>查看一下数据结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">RangeIndex: <span class="number">3542</span> entries, <span class="number">0</span> to <span class="number">3541</span></span><br><span class="line">Data columns (total <span class="number">6</span> columns):</span><br><span class="line">ts_code        <span class="number">3542</span> non-null object</span><br><span class="line">symbol         <span class="number">3542</span> non-null object</span><br><span class="line">name           <span class="number">3542</span> non-null object</span><br><span class="line">list_status    <span class="number">3542</span> non-null object</span><br><span class="line">list_date      <span class="number">3542</span> non-null object</span><br><span class="line">is_hs          <span class="number">3542</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>所有列都是object字符型。这里想对日期做数据分析，比如可以统计一下历年上市公司数量。需更改日期型数据字符型为日期型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'list_date'</span>] = pd.to_datetime(data[<span class="string">'list_date'</span>])</span><br></pre></td></tr></table></figure></p><p>pd.to_datetime将’list_date’列格式改为datetime格式，再来看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">RangeIndex: <span class="number">3542</span> entries, <span class="number">0</span> to <span class="number">3541</span></span><br><span class="line">Data columns (total <span class="number">6</span> columns):</span><br><span class="line">ts_code        <span class="number">3542</span> non-null object</span><br><span class="line">symbol         <span class="number">3542</span> non-null object</span><br><span class="line">name           <span class="number">3542</span> non-null object</span><br><span class="line">list_status    <span class="number">3542</span> non-null object</span><br><span class="line">list_date      <span class="number">3542</span> non-null datetime64[ns]</span><br><span class="line">is_hs          <span class="number">3542</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">6</span>)</span><br></pre></td></tr></table></figure><h3 id="2-1-按日期切片筛选数据"><a href="#2-1-按日期切片筛选数据" class="headerlink" title="2.1. 按日期切片筛选数据"></a>2.1. 按日期切片筛选数据</h3><p>有时候我们需要按年、季度、月、日这样的日期格式来筛选提取相应的数据。</p><h4 id="2-1-1-按年度"><a href="#2-1-1-按年度" class="headerlink" title="2.1.1. 按年度"></a>2.1.1. 按年度</h4><ul><li>获取单一年份数据，比如2017年</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data = data.set_index(data[<span class="string">'list_date'</span>])</span><br><span class="line">data = data[<span class="string">'2017'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>  <span class="number">001965.</span>SZ  <span class="number">001965</span>  招商公路           L <span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>     S</span><br><span class="line"><span class="number">2017</span><span class="number">-03</span><span class="number">-24</span>  <span class="number">002774.</span>SZ  <span class="number">002774</span>  快意电梯           L <span class="number">2017</span><span class="number">-03</span><span class="number">-24</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">002824.</span>SZ  <span class="number">002824</span>  和胜股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">002838.</span>SZ  <span class="number">002838</span>  道恩股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>  <span class="number">002839.</span>SZ  <span class="number">002839</span>  张家港行           L <span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>     S</span><br></pre></td></tr></table></figure><ul><li>获取多个年份，比如2015-2017</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="string">'2015'</span>:<span class="string">'2017'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-26</span>  <span class="number">000166.</span>SZ  <span class="number">000166</span>  申万宏源           L <span class="number">2015</span><span class="number">-01</span><span class="number">-26</span>     S</span><br><span class="line"><span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>  <span class="number">001965.</span>SZ  <span class="number">001965</span>  招商公路           L <span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>     S</span><br><span class="line"><span class="number">2015</span><span class="number">-12</span><span class="number">-30</span>  <span class="number">001979.</span>SZ  <span class="number">001979</span>  招商蛇口           L <span class="number">2015</span><span class="number">-12</span><span class="number">-30</span>     S</span><br><span class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-27</span>  <span class="number">002734.</span>SZ  <span class="number">002734</span>  利民股份           L <span class="number">2015</span><span class="number">-01</span><span class="number">-27</span>     N</span><br><span class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-22</span>  <span class="number">002739.</span>SZ  <span class="number">002739</span>  万达电影           L <span class="number">2015</span><span class="number">-01</span><span class="number">-22</span>     S</span><br></pre></td></tr></table></figure><h4 id="2-1-2-按月度"><a href="#2-1-2-按月度" class="headerlink" title="2.1.2. 按月度"></a>2.1.2. 按月度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="string">'2017-1'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">002824.</span>SZ  <span class="number">002824</span>  和胜股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">002838.</span>SZ  <span class="number">002838</span>  道恩股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>  <span class="number">002839.</span>SZ  <span class="number">002839</span>  张家港行           L <span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>     S</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-10</span>  <span class="number">002840.</span>SZ  <span class="number">002840</span>  华统股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-19</span>  <span class="number">002841.</span>SZ  <span class="number">002841</span>  视源股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-19</span>     S</span><br></pre></td></tr></table></figure><h4 id="2-1-3-按具体天"><a href="#2-1-3-按具体天" class="headerlink" title="2.1.3. 按具体天"></a>2.1.3. 按具体天</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="string">'2017-1-12'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">002824.</span>SZ  <span class="number">002824</span>  和胜股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">300584.</span>SZ  <span class="number">300584</span>  海辰药业           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">603628.</span>SH  <span class="number">603628</span>  清源股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     H</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">603639.</span>SH  <span class="number">603639</span>   海利尔           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     H</span><br></pre></td></tr></table></figure><h3 id="2-2-to-period按日期显示数据"><a href="#2-2-to-period按日期显示数据" class="headerlink" title="2.2. to_period按日期显示数据"></a>2.2. to_period按日期显示数据</h3><p>dataframe.to_period方法只是用于显示数据，但不会进行统计。</p><h4 id="2-2-1-按年度"><a href="#2-2-1-按年度" class="headerlink" title="2.2.1. 按年度"></a>2.2.1. 按年度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data.to_period(<span class="string">'A'</span>)  <span class="comment"># 'A'默认是从'A-DEC'开始算,也可以根据情况设置为'A-JAN'</span></span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">             ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                      </span><br><span class="line"><span class="number">1991</span>       <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L <span class="number">1991</span><span class="number">-04</span><span class="number">-03</span>     S</span><br><span class="line"><span class="number">1991</span>       <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L <span class="number">1991</span><span class="number">-01</span><span class="number">-29</span>     S</span><br><span class="line"><span class="number">1991</span>       <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L <span class="number">1991</span><span class="number">-01</span><span class="number">-14</span>     N</span><br><span class="line"><span class="number">1990</span>       <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L <span class="number">1990</span><span class="number">-12</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">1992</span>       <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L <span class="number">1992</span><span class="number">-04</span><span class="number">-27</span>     S</span><br></pre></td></tr></table></figure><p>可以看到，相比上面筛选数据时是按原始的日期，这里利用to_period方法，设置参数为’A’后，可以直接显示为年，这在后期可视化绘图时非常有用。</p><h4 id="2-2-2-按季度"><a href="#2-2-2-按季度" class="headerlink" title="2.2.2. 按季度"></a>2.2.2. 按季度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data.to_period(<span class="string">'Q'</span>)   <span class="comment"># 'Q'默认是从'Q-DEC'开始算,也可以根据情况设置为“Q-SEP”，“Q-FEB”等</span></span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">             ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                      </span><br><span class="line"><span class="number">1991</span>Q2     <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L <span class="number">1991</span><span class="number">-04</span><span class="number">-03</span>     S</span><br><span class="line"><span class="number">1991</span>Q1     <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L <span class="number">1991</span><span class="number">-01</span><span class="number">-29</span>     S</span><br><span class="line"><span class="number">1991</span>Q1     <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L <span class="number">1991</span><span class="number">-01</span><span class="number">-14</span>     N</span><br><span class="line"><span class="number">1990</span>Q4     <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L <span class="number">1990</span><span class="number">-12</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">1992</span>Q2     <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L <span class="number">1992</span><span class="number">-04</span><span class="number">-27</span>     S</span><br></pre></td></tr></table></figure><h4 id="2-2-3-按月度"><a href="#2-2-3-按月度" class="headerlink" title="2.2.3. 按月度"></a>2.2.3. 按月度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data.to_period(<span class="string">'M'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">             ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                      </span><br><span class="line"><span class="number">1991</span><span class="number">-04</span>    <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L <span class="number">1991</span><span class="number">-04</span><span class="number">-03</span>     S</span><br><span class="line"><span class="number">1991</span><span class="number">-01</span>    <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L <span class="number">1991</span><span class="number">-01</span><span class="number">-29</span>     S</span><br><span class="line"><span class="number">1991</span><span class="number">-01</span>    <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L <span class="number">1991</span><span class="number">-01</span><span class="number">-14</span>     N</span><br><span class="line"><span class="number">1990</span><span class="number">-12</span>    <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L <span class="number">1990</span><span class="number">-12</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">1992</span><span class="number">-04</span>    <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L <span class="number">1992</span><span class="number">-04</span><span class="number">-27</span>     S</span><br></pre></td></tr></table></figure><h3 id="2-3-resample按日期统计数据"><a href="#2-3-resample按日期统计数据" class="headerlink" title="2.3. resample按日期统计数据"></a>2.3. resample按日期统计数据</h3><p>按日期进行统计数据，可以利用resample方法。</p><h4 id="2-3-1-按年度"><a href="#2-3-1-按年度" class="headerlink" title="2.3.1. 按年度"></a>2.3.1. 按年度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data.resample(<span class="string">'AS'</span>).count()[<span class="string">'name'</span>]  <span class="comment"># count对各年上市公司数量进行计数</span></span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span><span class="number">-01</span><span class="number">-01</span>      <span class="number">7</span></span><br><span class="line"><span class="number">1991</span><span class="number">-01</span><span class="number">-01</span>      <span class="number">4</span></span><br><span class="line"><span class="number">1992</span><span class="number">-01</span><span class="number">-01</span>     <span class="number">37</span></span><br><span class="line"><span class="number">1993</span><span class="number">-01</span><span class="number">-01</span>    <span class="number">106</span></span><br><span class="line"><span class="number">1994</span><span class="number">-01</span><span class="number">-01</span>     <span class="number">99</span></span><br></pre></td></tr></table></figure><h4 id="2-3-2-按季度"><a href="#2-3-2-按季度" class="headerlink" title="2.3.2. 按季度"></a>2.3.2. 按季度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data.resample(<span class="string">'Q'</span>).count()[<span class="string">'name'</span>]  </span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span><span class="number">-12</span><span class="number">-31</span>    <span class="number">7</span></span><br><span class="line"><span class="number">1991</span><span class="number">-03</span><span class="number">-31</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1991</span><span class="number">-06</span><span class="number">-30</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1991</span><span class="number">-09</span><span class="number">-30</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1991</span><span class="number">-12</span><span class="number">-31</span>    <span class="number">0</span></span><br></pre></td></tr></table></figure><h4 id="2-3-3-按月度"><a href="#2-3-3-按月度" class="headerlink" title="2.3.3. 按月度"></a>2.3.3. 按月度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data.resample(<span class="string">'M'</span>).count()[<span class="string">'name'</span>]  </span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span><span class="number">-12</span><span class="number">-31</span>    <span class="number">7</span></span><br><span class="line"><span class="number">1991</span><span class="number">-01</span><span class="number">-31</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1991</span><span class="number">-02</span><span class="number">-28</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1991</span><span class="number">-03</span><span class="number">-31</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1991</span><span class="number">-04</span><span class="number">-30</span>    <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="2-4-统计和显示结合"><a href="#2-4-统计和显示结合" class="headerlink" title="2.4. 统计和显示结合"></a>2.4. 统计和显示结合</h3><p>利用前面的resample和to.period方法，可以按年、季度、月份汇总数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 汇总各年上市公司数量</span></span><br><span class="line">data = data.set_index([<span class="string">'list_date'</span>])</span><br><span class="line">data = data.resample(<span class="string">'AS'</span>).count()[<span class="string">'ts_code'</span>]</span><br><span class="line">data = data.to_period(<span class="string">'A'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line">print(data.tail())</span><br></pre></td></tr></table></figure><p>结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">list_date</span><br><span class="line"><span class="number">1990</span>      <span class="number">7</span></span><br><span class="line"><span class="number">1991</span>      <span class="number">4</span></span><br><span class="line"><span class="number">1992</span>     <span class="number">37</span></span><br><span class="line"><span class="number">1993</span>    <span class="number">106</span></span><br><span class="line"><span class="number">1994</span>     <span class="number">99</span></span><br><span class="line">Freq: A-DEC, Name: name, dtype: int64</span><br><span class="line">list_date</span><br><span class="line"><span class="number">2014</span>    <span class="number">124</span></span><br><span class="line"><span class="number">2015</span>    <span class="number">223</span></span><br><span class="line"><span class="number">2016</span>    <span class="number">227</span></span><br><span class="line"><span class="number">2017</span>    <span class="number">438</span></span><br><span class="line"><span class="number">2018</span>     <span class="number">78</span></span><br><span class="line">Freq: A-DEC, Name: name, dtype: int64</span><br></pre></td></tr></table></figure></p><p>基于上述数据，可以利用matplotlib绘制出历年上市公司数量的折线图：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-17/25609726.jpg" alt=""></p><p>折线图的具体绘制方法，见后续文章。</p><p>以上就是简单利用了Tushare的一个接口返回的数据，介绍了日期型数据的转换和处理。</p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以调用大名鼎鼎的Tushare包案例，介绍日期数据的处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Python数据清洗处理" scheme="https://www.makcyun.top/categories/Python%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E5%A4%84%E7%90%86/"/>
    
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="Python包" scheme="https://www.makcyun.top/tags/Python%E5%8C%85/"/>
    
      <category term="Tushare" scheme="https://www.makcyun.top/tags/Tushare/"/>
    
  </entry>
  
  <entry>
    <title>Python可视化(2)：折线图</title>
    <link href="https://www.makcyun.top/python_data_visualization01.html"/>
    <id>https://www.makcyun.top/python_data_visualization01.html</id>
    <published>2018-09-11T01:36:41.000Z</published>
    <updated>2018-10-12T00:07:27.130Z</updated>
    
    <content type="html"><![CDATA[<p>日期型数据的折线图绘制。</p><a id="more"></a>  <p><strong>摘要：</strong> 利用matplotlib绘制横轴为日期格式的折线图时，存在不少技巧。本文借助Tushare包返回的股票数据，介绍日期折线图绘制的方法。</p><p><a href="https://www.makcyun.top/python_data_analysis01.html">上一篇文章</a>的最后讲到了折线图的绘制，本文详细介绍绘制方法。</p><p>折线图绘制的数据源，采用Tushare包获取上市公司基本数据表，格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read(<span class="string">'get_stock_basics.csv'</span>,encoding = <span class="string">'utf8'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line"></span><br><span class="line">ts_codesymbolnamelist_statuslist_dateis_hs</span><br><span class="line"><span class="number">000001.</span>SZ<span class="number">1</span>平安银行L<span class="number">19910403</span>S</span><br><span class="line"><span class="number">000002.</span>SZ<span class="number">2</span>万科AL<span class="number">19910129</span>S</span><br><span class="line"><span class="number">000004.</span>SZ<span class="number">4</span>国农科技L<span class="number">19910114</span>N</span><br><span class="line"><span class="number">000005.</span>SZ<span class="number">5</span>世纪星源L<span class="number">19901210</span>N</span><br></pre></td></tr></table></figure><p>然后利用<code>resample</code>和<code>to.period</code>方法汇总各年度的上市公司数量数据，格式为Pandas.Series数组格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 汇总各年上市公司数量</span></span><br><span class="line">data = data.set_index([<span class="string">'list_date'</span>])</span><br><span class="line">data = data.resample(<span class="string">'AS'</span>).count()[<span class="string">'ts_code'</span>]</span><br><span class="line">data = data.to_period(<span class="string">'A'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line">print(data.tail())</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span>      <span class="number">7</span></span><br><span class="line"><span class="number">1991</span>      <span class="number">4</span></span><br><span class="line"><span class="number">1992</span>     <span class="number">37</span></span><br><span class="line"><span class="number">1993</span>    <span class="number">106</span></span><br><span class="line"><span class="number">1994</span>     <span class="number">99</span></span><br><span class="line">...</span><br><span class="line">list_date</span><br><span class="line"><span class="number">2014</span>    <span class="number">124</span></span><br><span class="line"><span class="number">2015</span>    <span class="number">223</span></span><br><span class="line"><span class="number">2016</span>    <span class="number">227</span></span><br><span class="line"><span class="number">2017</span>    <span class="number">438</span></span><br><span class="line"><span class="number">2018</span>     <span class="number">78</span></span><br></pre></td></tr></table></figure><h2 id="1-Series直接绘制折线图"><a href="#1-Series直接绘制折线图" class="headerlink" title="1. Series直接绘制折线图"></a>1. Series直接绘制折线图</h2><p>首先，我们可以直接利用pandas的数组Series绘制折线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)  <span class="comment"># 设置绘图风格</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">10</span>,<span class="number">6</span>))  <span class="comment"># 设置图框的大小</span></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">data.plot() <span class="comment"># 绘制折线图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment">#设置标题颜色为灰色</span></span><br><span class="line">plt.title(<span class="string">'历年中国内地上市公司数量变化'</span>,color = colors1,fontsize = <span class="number">18</span>)</span><br><span class="line">plt.xlabel(<span class="string">'年份'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(家)'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-18/15047153.jpg" alt=""></p><p>可以发现，图中存在两个问题：一是缺少数值标签，二是横坐标年份被自动分割了。我们希望能够添加上数值标签，然后坐标轴显示每一年的年份值。接下来，需要采用新的方法重新绘制折线图。</p><h2 id="2-折线图完善"><a href="#2-折线图完善" class="headerlink" title="2. 折线图完善"></a>2. 折线图完善</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建x,y轴标签</span></span><br><span class="line">x = np.arange(<span class="number">0</span>,len(data),<span class="number">1</span>)</span><br><span class="line">    ax1.plot(x,data.values, <span class="comment">#x、y坐标</span></span><br><span class="line">    color = <span class="string">'#C42022'</span>, <span class="comment">#折线图颜色为红色</span></span><br><span class="line">    marker = <span class="string">'o'</span>,markersize = <span class="number">4</span> <span class="comment">#标记形状、大小设置</span></span><br><span class="line">    )</span><br><span class="line">ax1.set_xticks(x) <span class="comment"># 设置x轴标签为自然数序列</span></span><br><span class="line">ax1.set_xticklabels(data.index) <span class="comment"># 更改x轴标签值为年份</span></span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>) <span class="comment"># 旋转90度，不至太拥挤</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data.values):</span><br><span class="line">    plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors1,fontsize = <span class="number">10</span> )</span><br><span class="line">    <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line"><span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">plt.title(<span class="string">'历年中国内地上市公司数量变化'</span>,color = colors1,fontsize = <span class="number">18</span>)</span><br><span class="line">plt.xlabel(<span class="string">'年份'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(家)'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('stock.png',bbox_inches = 'tight',dpi = 300)</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>完善后的折线图如下：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-19/84420184.jpg" alt=""></p><p>可以看到，x轴逐年的数据都显示并且数值标签也添加上了。</p><h2 id="3-多元折线图"><a href="#3-多元折线图" class="headerlink" title="3. 多元折线图"></a>3. 多元折线图</h2><p>上面介绍了一元折线图的绘制，当需要绘制多元折线图时，方法也很简单，只要重复绘图函数即可。这里我们以二元折线图为例，绘制国内两家知名地产公司万科和保利地产2017年的市值变化对比折线图。</p><h3 id="3-1-数据来源"><a href="#3-1-数据来源" class="headerlink" title="3.1. 数据来源"></a>3.1. 数据来源</h3><p>数据源仍然采用tushare包的<code>pro.daily_basic()</code>接口，该接口能够返回股票的每日股市数据，其中包括每日市值<code>total_mv</code>。我们需要的两只股票分别是万科地产(000002.SZ)和保利地产(600048.SH)，下面就来获取两只股票2017年的市值数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">ts.set_token(<span class="string">'你的token'</span>)  <span class="comment"># 官网注册后可以获得</span></span><br><span class="line">pro = ts.pro_api()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stock</span><span class="params">()</span>:</span></span><br><span class="line">    lst = []</span><br><span class="line">    ts_codes = [<span class="string">'000002.SZ'</span>, <span class="string">'600048.SH'</span>]</span><br><span class="line">    <span class="keyword">for</span> ts_code <span class="keyword">in</span> ts_codes:</span><br><span class="line">        data = pro.daily_basic(</span><br><span class="line">            ts_code=ts_code, start_date=<span class="string">'20170101'</span>, end_date=<span class="string">'20180101'</span>)</span><br><span class="line">    print(lst)</span><br><span class="line">    reutrn lst</span><br><span class="line"><span class="comment"># 结果如下，total_mv为当日市值（万元）：</span></span><br><span class="line">    <span class="comment">#万科地产数据</span></span><br><span class="line">    ts_codetrade_dateclose…total_mvcirc_mv</span><br><span class="line"><span class="number">0</span><span class="number">000002.</span>SZ<span class="number">20171229</span><span class="number">31.06</span>…<span class="number">3.43E+07</span><span class="number">3.02E+07</span></span><br><span class="line"><span class="number">1</span><span class="number">000002.</span>SZ<span class="number">20171228</span><span class="number">30.7</span>…<span class="number">3.39E+07</span><span class="number">2.98E+07</span></span><br><span class="line"><span class="number">2</span><span class="number">000002.</span>SZ<span class="number">20171227</span><span class="number">30.79</span>…<span class="number">3.40E+07</span><span class="number">2.99E+07</span></span><br><span class="line"><span class="number">3</span><span class="number">000002.</span>SZ<span class="number">20171226</span><span class="number">30.5</span>…<span class="number">3.37E+07</span><span class="number">2.96E+07</span></span><br><span class="line"><span class="number">4</span><span class="number">000002.</span>SZ<span class="number">20171225</span><span class="number">30.37</span>…<span class="number">3.35E+07</span><span class="number">2.95E+07</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保利地产数据</span></span><br><span class="line">    ts_codetrade_dateclose…total_mvcirc_mv</span><br><span class="line"><span class="number">0</span><span class="number">600048.</span>SH<span class="number">20171229</span><span class="number">14.15</span>…<span class="number">1.68E+07</span><span class="number">1.66E+07</span></span><br><span class="line"><span class="number">1</span><span class="number">600048.</span>SH<span class="number">20171228</span><span class="number">13.71</span>…<span class="number">1.63E+07</span><span class="number">1.61E+07</span></span><br><span class="line"><span class="number">2</span><span class="number">600048.</span>SH<span class="number">20171227</span><span class="number">13.65</span>…<span class="number">1.62E+07</span><span class="number">1.60E+07</span></span><br><span class="line"><span class="number">3</span><span class="number">600048.</span>SH<span class="number">20171226</span><span class="number">13.85</span>…<span class="number">1.64E+07</span><span class="number">1.63E+07</span></span><br><span class="line"><span class="number">4</span><span class="number">600048.</span>SH<span class="number">20171225</span><span class="number">13.55</span>…<span class="number">1.61E+07</span><span class="number">1.59E+07</span></span><br></pre></td></tr></table></figure><p>下面对数据作进一步修改，从DataFrame中提取total_mv列，index设置为日期。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'trade_date'</span>] = pd.to_datetime(data[<span class="string">'trade_date'</span>])</span><br><span class="line"><span class="comment"># 设置index为日期</span></span><br><span class="line">data = data.set_index(data[<span class="string">'trade_date'</span>]).sort_index(ascending=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 按月汇总和显示</span></span><br><span class="line">data = data.resample(<span class="string">'m'</span>)</span><br><span class="line">data = data.to_period()</span><br><span class="line"><span class="comment"># 市值改为亿元</span></span><br><span class="line">market_value = data[<span class="string">'total_mv'</span>]/<span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二者结果分别如下，万科地产：</span></span><br><span class="line"><span class="number">2017</span><span class="number">-01</span>    <span class="number">2291.973270</span></span><br><span class="line"><span class="number">2017</span><span class="number">-02</span>    <span class="number">2286.331037</span></span><br><span class="line"><span class="number">2017</span><span class="number">-03</span>    <span class="number">2306.894790</span></span><br><span class="line"><span class="number">2017</span><span class="number">-04</span>    <span class="number">2266.337906</span></span><br><span class="line"><span class="number">2017</span><span class="number">-05</span>    <span class="number">2131.053098</span></span><br><span class="line"><span class="number">2017</span><span class="number">-06</span>    <span class="number">2457.716659</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span>    <span class="number">2686.982164</span></span><br><span class="line"><span class="number">2017</span><span class="number">-08</span>    <span class="number">2524.462077</span></span><br><span class="line"><span class="number">2017</span><span class="number">-09</span>    <span class="number">2904.085487</span></span><br><span class="line"><span class="number">2017</span><span class="number">-10</span>    <span class="number">2976.999550</span></span><br><span class="line"><span class="number">2017</span><span class="number">-11</span>    <span class="number">3263.374043</span></span><br><span class="line"><span class="number">2017</span><span class="number">-12</span>    <span class="number">3317.107474</span></span><br><span class="line"><span class="comment"># 保利地产：</span></span><br><span class="line"><span class="number">2017</span><span class="number">-01</span>    <span class="number">1089.008286</span></span><br><span class="line"><span class="number">2017</span><span class="number">-02</span>    <span class="number">1120.023350</span></span><br><span class="line"><span class="number">2017</span><span class="number">-03</span>    <span class="number">1145.731640</span></span><br><span class="line"><span class="number">2017</span><span class="number">-04</span>    <span class="number">1153.760435</span></span><br><span class="line"><span class="number">2017</span><span class="number">-05</span>    <span class="number">1108.230609</span></span><br><span class="line"><span class="number">2017</span><span class="number">-06</span>    <span class="number">1157.276044</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span>    <span class="number">1244.966905</span></span><br><span class="line"><span class="number">2017</span><span class="number">-08</span>    <span class="number">1203.580209</span></span><br><span class="line"><span class="number">2017</span><span class="number">-09</span>    <span class="number">1290.706606</span></span><br><span class="line"><span class="number">2017</span><span class="number">-10</span>    <span class="number">1244.438756</span></span><br><span class="line"><span class="number">2017</span><span class="number">-11</span>    <span class="number">1336.661916</span></span><br><span class="line"><span class="number">2017</span><span class="number">-12</span>    <span class="number">1531.150616</span></span><br></pre></td></tr></table></figure><h3 id="3-2-绘制二元折线图"><a href="#3-2-绘制二元折线图" class="headerlink" title="3.2. 绘制二元折线图"></a>3.2. 绘制二元折线图</h3><p>利用上面的Series数据就可以作图了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 设置绘图风格</span><br><span class="line">plt.style.use(&apos;ggplot&apos;)</span><br><span class="line">fig = plt.figure(figsize = (10,6))</span><br><span class="line">colors1 = &apos;#6D6D6D&apos;  #标题颜色</span><br><span class="line"></span><br><span class="line"># data1万科，data2保利</span><br><span class="line">data1 = lst[0]</span><br><span class="line">data2 = lst[1]</span><br><span class="line"># 绘制第一条折线图</span><br><span class="line">data1.plot(</span><br><span class="line">color = &apos;#C42022&apos;, #折线图颜色</span><br><span class="line">marker = &apos;o&apos;,markersize = 4, #标记形状、大小设置</span><br><span class="line">label = &apos;万科&apos;</span><br><span class="line">)</span><br><span class="line"># 绘制第二条折线图</span><br><span class="line">data2.plot(</span><br><span class="line">color = &apos;#4191C0&apos;, #折线图颜色</span><br><span class="line">marker = &apos;o&apos;,markersize = 4, #标记形状、大小设置</span><br><span class="line">label = &apos;保利&apos;</span><br><span class="line">)</span><br><span class="line"># 还可以绘制更多条</span><br><span class="line"># 设置标题及横纵坐标轴标题</span><br><span class="line">plt.title(&apos;2017年万科与保利地产市值对比&apos;,color = colors1,fontsize = 18)</span><br><span class="line">plt.xlabel(&apos;月份&apos;)</span><br><span class="line">plt.ylabel(&apos;市值(亿元)&apos;)</span><br><span class="line">plt.savefig(&apos;stock1.png&apos;,bbox_inches = &apos;tight&apos;,dpi = 300)</span><br><span class="line">plt.legend() # 显示图例</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>绘图结果如下：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-28/31064491.jpg" alt=""></p><p>如果想添加数值标签，则可以使用下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制第一条折线图</span></span><br><span class="line"><span class="comment"># 创建x,y轴标签</span></span><br><span class="line">x = np.arange(<span class="number">0</span>,len(data1),<span class="number">1</span>)</span><br><span class="line">ax1.plot(x,data1.values, <span class="comment">#x、y坐标</span></span><br><span class="line">color = <span class="string">'#C42022'</span>, <span class="comment">#折线图颜色红色</span></span><br><span class="line">marker = <span class="string">'o'</span>,markersize = <span class="number">4</span>, <span class="comment">#标记形状、大小设置</span></span><br><span class="line">label = <span class="string">'万科'</span></span><br><span class="line">)</span><br><span class="line">ax1.set_xticks(x) <span class="comment"># 设置x轴标签</span></span><br><span class="line">ax1.set_xticklabels(data1.index) <span class="comment"># 设置x轴标签值</span></span><br><span class="line"><span class="comment"># plt.xticks(rotation=90)</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data1.values):</span><br><span class="line">    plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors1,fontsize = <span class="number">10</span> )</span><br><span class="line">    <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制第二条折线图</span></span><br><span class="line">x = np.arange(<span class="number">0</span>,len(data2),<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">ax1.plot(x,data2.values, <span class="comment">#x、y坐标</span></span><br><span class="line">color = <span class="string">'#4191C0'</span>, <span class="comment">#折线图颜色蓝色</span></span><br><span class="line">marker = <span class="string">'o'</span>,markersize = <span class="number">4</span>, <span class="comment">#标记形状、大小设置</span></span><br><span class="line">label = <span class="string">'保利'</span></span><br><span class="line">)</span><br><span class="line">ax1.set_xticks(x) <span class="comment"># 设置x轴标签</span></span><br><span class="line">ax1.set_xticklabels(data2.index) <span class="comment"># 设置x轴标签值</span></span><br><span class="line"><span class="comment"># plt.xticks(rotation=90)</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data2.values):</span><br><span class="line">    plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors1,fontsize = <span class="number">10</span> )</span><br><span class="line">    <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">plt.title(<span class="string">'2017年万科与保利地产市值对比'</span>,color = colors1,fontsize = <span class="number">18</span>)</span><br><span class="line">plt.xlabel(<span class="string">'月份'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'市值(亿元)'</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'stock1.png'</span>,bbox_inches = <span class="string">'tight'</span>,dpi = <span class="number">300</span>)</span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图所示：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-28/45373948.jpg" alt=""></p><p>可以看到，两只股票市值从2017年初开始一直在上涨，万科的市值是保利的2倍左右。</p><p>本文仅简单提取了两只股票的市值数据，只要你愿意，3000多只股票的数据都可以拿来绘图。</p><p>文章代码及素材可在下面链接中获得：</p><p><a href="https://github.com/makcyun/web_scraping_with_python/tree/master/date_plot" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python/tree/master/date_plot</a></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日期型数据的折线图绘制。&lt;/p&gt;
    
    </summary>
    
      <category term="Python可视化" scheme="https://www.makcyun.top/categories/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>每周分享第 1 期：关于 PDF 阅读处理软件，你需要的都在这里了</title>
    <link href="https://www.makcyun.top/fuli01.html"/>
    <id>https://www.makcyun.top/fuli01.html</id>
    <published>2018-09-08T12:16:37.349Z</published>
    <updated>2018-10-24T02:39:31.452Z</updated>
    
    <content type="html"><![CDATA[<p>分享几款佳软。</p><a id="more"></a>  <p>从今天起，我将开一个新的栏目「<strong>每周分享</strong>」，顾名思义，就是会在每个周末分享一篇文章。</p><p>内容涵盖范围会比较广，主要集中在软件、App、PPT、摄影Ps等几个方面。之后，你可以在公众号里面的 「不务正业」菜单里集中查看。插一句，另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p><code>为什么会想起开这个栏目呢，主要有两个原因。</code></p><p>第一，我的公众号前不久更名为「<strong>第2大脑</strong>」，是因为我想在这里分享一些一直存在我大脑里的好用的软件、App等黑科技玩意儿。</p><p>一直以来，我对软件、App这块儿保有很强的好奇心，总喜欢尝试些新鲜有趣或者能提高效率的东西。时间长了，发现积累了蛮多，但是都散乱地分布在电脑、手机或者其他地方，而没有好好地进行总结，也没拿出来分享过。本着开源精神，接下来我想陆陆续续把一些可能会对你用的东西，分享出来。</p><p>第二，大多数 Python 类公众号的内容基本上都是跟 Python 相关的，我觉得太单调。虽然「<strong>人生苦短，快用 Python</strong>」没错，但是同时关注一些别的，既不枯燥也能让生活更加有意思，是不是。</p><hr><p>好，说完了原因，那么下面就直奔主题吧。</p><p>第 1 期，就先分享几款「<strong>PDF 软件</strong>」。</p><p>为什么呢？ 因为，之前遇到过很多人在处理和 PDF 文件相关的问题时，总是很头疼，到处求助软件。这里我就分享几款，个人觉得非常好用的 PDF 软件，几乎能满足你对 PDF 所有的需求。有了它们，之后再遇到格式转换、提取分割、复制、压缩等问题时，很轻松就能解决了。</p><p>先推荐一款难得的国产良心软件。</p><h2 id="1-Foxit-Reader"><a href="#1-Foxit-Reader" class="headerlink" title="1. Foxit Reader"></a>1. <a href="https://www.foxitsoftware.com/pdf-reader/" target="_blank" rel="noopener">Foxit Reader</a></h2><p>从网站全英文 Style 就可以看出这款国产软件是走国际路线的。口碑非常好，足以媲美大名鼎鼎的 Adobe Acrobat，但体积要小地多。这款软件的功能主要是浏览 PDF ，同时打开几十个 PDF 文件也不会卡。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/96139143.jpg" alt=""></p><p>除了正常浏览 PDF以外，更多的需求是对 PDF进行处理，最常见的就是 PDF 转换为 Word/Excel/PPT。</p><h2 id="2-Solid-PDF-Converter"><a href="#2-Solid-PDF-Converter" class="headerlink" title="2. Solid PDF Converter"></a>2. <a href="https://www.soliddocuments.com/zh/features.htm?product=SolidConverterPDF" target="_blank" rel="noopener">Solid PDF Converter</a></h2><p>在 PDF文件是可以 <strong>复制</strong> 的前提下，这款软件可以说是 PDF 转换为 Office 格式最好用的软件。价格不便宜，官方售价 100 美元。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/15794080.jpg" alt=""></p><p>当 PDF 文件不能复制（扫描件或者图片）时，上面的软件就无能为力了，这时就需要下面这款大名鼎鼎的 OCR （文字识别）软件了。</p><h2 id="3-最好用的OCR文字识别软件"><a href="#3-最好用的OCR文字识别软件" class="headerlink" title="3. 最好用的OCR文字识别软件"></a>3. <a href="https://www.abbyy.com/en-us/finereader/" target="_blank" rel="noopener">最好用的OCR文字识别软件</a></h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/50294768.jpg" alt=""></p><p>它到底有多好，一句话就够了：知乎上  <a href="https://www.zhihu.com/topic/19556393/top-answers" target="_blank" rel="noopener">PDF</a> 话题中排名第一的软件。  </p><p>当你需要从 PDF 中复制，但却无法选择时，这款软件能帮你搞定一切。另外，手机拍的文字照片，也能转成 Word。</p><p>这么好用，自然价格也不便宜，官方售价 500 RMB。</p><h2 id="4-Small-PDF"><a href="#4-Small-PDF" class="headerlink" title="4. Small PDF"></a>4. <a href="https://smallpdf.com/cn" target="_blank" rel="noopener">Small PDF</a></h2><p>有时，我们得到的 PDF 文件体积很大，打开或者传输给别人很不方便。那么就需要进行压缩了，压缩效果最好的软件就是这款 Small PDF了。 它的网页版是我们经常在搜索时会遇到的。</p><p>除此之外，它还有很多其他非常实用的功能。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/89711681.jpg" alt=""></p><h2 id="5-PDFdo"><a href="#5-PDFdo" class="headerlink" title="5. PDFdo"></a>5. <a href="http://www.pdfdo.com/" target="_blank" rel="noopener">PDFdo</a></h2><p>除上面的之外，还会有几类需求，比如：只需要 PDF 文件中的某一部分，那么需要从 PDF 中进行提取或者分割；要将多个 PDF 合并成一个文件，那么需要合并 PDF；要将好几个 Word 文件同时转换为 PDF，那么有批量转换就最好了。</p><p>如果有上面的需求，那么这款软件是最佳选择，速度非常快。官方售价 99 RMB。  </p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/19958488.jpg" alt=""></p><p>以上这 5 款软件就足以应对和 PDF 相关的大部分问题了，但他们都是电脑软件。</p><p>这里再推荐一款手机上的软件。</p><h2 id="6-WPS-Office"><a href="#6-WPS-Office" class="headerlink" title="6. WPS Office"></a>6. WPS Office</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-9/89469122.jpg" alt=""></p><p>别看到是「WPS」就摇头，手机版的和电脑版的 WPS 可谓 「天壤之别」。</p><p>除了阅读以外，还拥有：转换为 Word、提取 PDF、合并 PDF、转为图片、拍照扫描等几项核心功能。这款软件有免费版，不过高级版是需要付费的。</p><p>以上就是我一直在用的几款 PDF 软件，推荐给你。</p><p>有条件请支持正版，没有条件，就百度/谷歌/某宝搜索吧，实在找不到可以给我留言。</p><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分享几款佳软。&lt;/p&gt;
    
    </summary>
    
      <category term="python爬虫" scheme="https://www.makcyun.top/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="pdf" scheme="https://www.makcyun.top/tags/pdf/"/>
    
      <category term="佳软" scheme="https://www.makcyun.top/tags/%E4%BD%B3%E8%BD%AF/"/>
    
  </entry>
  
  <entry>
    <title>Pyhton可视化(1): 20秒纵览十年中国大学排行榜变化</title>
    <link href="https://www.makcyun.top/Python_visualization01.html"/>
    <id>https://www.makcyun.top/Python_visualization01.html</id>
    <published>2018-09-05T05:27:22.000Z</published>
    <updated>2018-10-20T01:43:01.948Z</updated>
    
    <content type="html"><![CDATA[<p>Python爬取近十年中国大学Top20强并结合D3.js做动态数据可视化表。</p><a id="more"></a>  <p><strong>摘要：</strong>：最近在朋友圈看到一个很酷炫的动态数据可视化表，介绍了新中国成立后各省GDP的发展历程，非常惊叹竟然还有这种操作，也想试试。于是，照葫芦画瓢虎，在网上爬取了历年中国大学学术排行榜，制作了一个中国大学排名Top20强动态表。</p><h2 id="1-作品介绍"><a href="#1-作品介绍" class="headerlink" title="1. 作品介绍"></a>1. 作品介绍</h2><p>这里先放一下这个动态表是什么样的：<br><a href="https://www.bilibili.com/video/av24503002" target="_blank" rel="noopener">https://www.bilibili.com/video/av24503002</a></p><p>不知道你看完是什么感觉，至少我是挺震惊的，想看看作者是怎么做出来的，于是追到了作者的B站主页，发现了更多有意思的动态视频：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/84261830.jpg" alt="">  </p><p>这些作品的作者是：@Jannchie见齐，他的主页：<br><a href="https://space.bilibili.com/1850091/#/video" target="_blank" rel="noopener">https://space.bilibili.com/1850091/#/video</a></p><p>这些会动的图表是如何做出来的呢？他用到的是一个动态图形显示数据的JavaScript库：<strong>D3.js</strong>，一种前端技术。难怪不是一般地酷炫。<br>那么，如果不会D3.js是不是就做不出来了呢？当然不是，Jannchie非常Open地给出了一个手把手简单教程：<br><a href="https://www.bilibili.com/video/av28087807" target="_blank" rel="noopener">https://www.bilibili.com/video/av28087807</a> </p><p>他同时还开放了程序源码，你只需要做2步就能够实现：  </p><ul><li><p>到他的Github主页下载源码到本地电脑：<br><a href="https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js" target="_blank" rel="noopener">https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</a><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/38399216.jpg" alt=""></p></li><li><p>打开<code>dist</code>文件夹里面的<code>exampe.csv</code>文件，放进你想要展示的数据，再用浏览器打开<code>bargraph.html</code>网页，就可以实现动态效果了。</p></li></ul><p>下面，我们稍微再说详细一点，实现这种效果的关键点。<br>最重要的是要有数据。观察一下上面的作品可以看到，横向柱状图中的数据要满足两个条件：一是要有多个对比的对象，二是要在时间上连续。这样才可以做出动态效果来。</p><p>看完后我立马就有了一个想法：<strong>想看看近十年中国的各个大学排名是个什么情况</strong>。下面我们就通过实实例来操作下。</p><h2 id="2-案例操作：中国大学Top20强"><a href="#2-案例操作：中国大学Top20强" class="headerlink" title="2. 案例操作：中国大学Top20强"></a>2. 案例操作：中国大学Top20强</h2><h3 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1. 数据来源"></a>2.1. 数据来源</h3><p>世界上最权威的大学排名有4类，分别是：</p><ul><li>原上海交通大学的ARWU<br> <a href="http://www.shanghairanking.com/ARWU2018.html" target="_blank" rel="noopener">http://www.shanghairanking.com/ARWU2018.html</a></li><li>英国教育组织的QS<br><a href="https://www.topuniversities.com/university-rankings/world-university-rankings/2018" target="_blank" rel="noopener">https://www.topuniversities.com/university-rankings/world-university-rankings/2018</a></li><li>泰晤士的THE<br><a href="https://www.timeshighereducation.com/world-university-rankings" target="_blank" rel="noopener">https://www.timeshighereducation.com/world-university-rankings</a>  </li><li>美国的usnews<br><a href="https://www.usnews.com/best-colleges/rankings" target="_blank" rel="noopener">https://www.usnews.com/best-colleges/rankings</a></li></ul><p>关于，这四类排名的更多介绍，可以看这个：<br><a href="https://www.zhihu.com/question/20825030/answer/71336291" target="_blank" rel="noopener">https://www.zhihu.com/question/20825030/answer/71336291</a></p><p>这里，我们选取相对比较权威也比较符合国情的第一个ARWU的排名结果。打开官网，可以看到有英文版和中文版排名，这里选取中文版。<br>排名非常齐全，从2003年到最新的2018年都有，非常好。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/79756091.jpg" alt=""></p><p>同时，可以看到这是世界500强的大学排名，而我们需要的是中国（包括港澳台）的大学排名。怎么办呢？ 当然不能一年年地复制然后再从500条数据里一条条筛选出中国的，这里就要用爬虫来实现了。可以参考不久前的一篇爬取表格的文章：<br><a href="https://www.makcyun.top/web_scraping_withpython2.html">https://www.makcyun.top/web_scraping_withpython2.html</a></p><h3 id="2-2-抓取数据"><a href="#2-2-抓取数据" class="headerlink" title="2.2. 抓取数据"></a>2.2. 抓取数据</h3><h4 id="2-2-1-分析url"><a href="#2-2-1-分析url" class="headerlink" title="2.2.1. 分析url"></a>2.2.1. 分析url</h4><p>首先，分析一下URL:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http://www.zuihaodaxue.com/ARWU2018.html</span><br><span class="line">http://www.zuihaodaxue.com/ARWU2017.html</span><br><span class="line">...</span><br><span class="line">http://www.zuihaodaxue.com/ARWU2009.html</span><br></pre></td></tr></table></figure><p>可以看到，url非常有规律，只有年份数字在变，很简单就能构造出for循环。<br>格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br></pre></td></tr></table></figure><p>下面就可以开始写爬虫了。  </p><h4 id="2-2-2-获取网页内容"><a href="#2-2-2-获取网页内容" class="headerlink" title="2.2.2. 获取网页内容"></a>2.2.2. 获取网页内容</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="comment"># 2009-2015用'gbk'，2016-2018用'utf-8'</span></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="comment"># return response.text  # text会乱码，content没有问题</span></span><br><span class="line">        <span class="keyword">return</span> response.content</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br></pre></td></tr></table></figure><p>上面需要注意的是，不同年份网页采用的编码不同，返回response.test会乱码，返回response.content则不会。关于编码乱码的问题，以后单独写一篇文章。</p><h4 id="2-2-3-解析表格"><a href="#2-2-3-解析表格" class="headerlink" title="2.2.3. 解析表格"></a>2.2.3. 解析表格</h4><p>用read_html函数一行代码来抓取表格，然后输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line">print(tb)</span><br></pre></td></tr></table></figure></p><p>可以看到，很顺利地表格就被抓取了下来：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/80641562.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/80641562.jpg"><br>但是表格需要进行处理，比如删除掉不需要的评分列，增加年份列等，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 重命名表格列，不需要的列用数字表示</span></span><br><span class="line">tb.columns = [<span class="string">'world rank'</span>,<span class="string">'university'</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="string">'score'</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">tb.drop([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],axis = <span class="number">1</span>,inplace = <span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 删除后面不需要的评分列</span></span><br><span class="line"><span class="comment"># rank列100名后是区间，需需唯一化，增加一列index作为排名</span></span><br><span class="line">tb[<span class="string">'index_rank'</span>] = tb.index</span><br><span class="line">tb[<span class="string">'index_rank'</span>] = tb[<span class="string">'index_rank'</span>].astype(int) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一列年份列</span></span><br><span class="line">tb[<span class="string">'year'</span>] = i</span><br><span class="line"><span class="comment"># read_html没有爬取country，需定义函数单独爬取</span></span><br><span class="line">tb[<span class="string">'country'</span>] = get_country(html)</span><br><span class="line"><span class="keyword">return</span> tb</span><br></pre></td></tr></table></figure><p>需要注意的是，国家没有被抓取下来，因为国家是用的图片表示的，定位到国家代码位置：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/39145641.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/39145641.jpg"></p><p>可以看到美国是用英文的USA表示的，那么我们可以单独提取出src属性，然后用正则提取出国家名称就可以了，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取国家名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_country</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    countries = soup.select(<span class="string">'td &gt; a &gt; img'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> countries:</span><br><span class="line">        src = i[<span class="string">'src'</span>]</span><br><span class="line">        pattern = re.compile(<span class="string">'flag.*\/(.*?).png'</span>)</span><br><span class="line">        country = re.findall(pattern,src)[<span class="number">0</span>]</span><br><span class="line">        lst.append(country)</span><br><span class="line">    <span class="keyword">return</span> lst</span><br></pre></td></tr></table></figure><p>然后，我们就可以输出一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">    world rank    university  score  index_rank  year      country</span><br><span class="line"><span class="number">0</span>            <span class="number">1</span>          哈佛大学  <span class="number">100.0</span>           <span class="number">1</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">1</span>            <span class="number">2</span>         斯坦福大学   <span class="number">75.6</span>           <span class="number">2</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">2</span>            <span class="number">3</span>          剑桥大学   <span class="number">71.8</span>           <span class="number">3</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">3</span>            <span class="number">4</span>        麻省理工学院   <span class="number">69.9</span>           <span class="number">4</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">4</span>            <span class="number">5</span>      加州大学-伯克利   <span class="number">68.3</span>           <span class="number">5</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">5</span>            <span class="number">6</span>        普林斯顿大学   <span class="number">61.0</span>           <span class="number">6</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">6</span>            <span class="number">7</span>          牛津大学   <span class="number">60.0</span>           <span class="number">7</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">7</span>            <span class="number">8</span>        哥伦比亚大学   <span class="number">58.2</span>           <span class="number">8</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">8</span>            <span class="number">9</span>        加州理工学院   <span class="number">57.4</span>           <span class="number">9</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">9</span>           <span class="number">10</span>         芝加哥大学   <span class="number">55.5</span>          <span class="number">10</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">10</span>          <span class="number">11</span>      加州大学-洛杉矶   <span class="number">51.2</span>          <span class="number">11</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">11</span>          <span class="number">12</span>         康奈尔大学   <span class="number">50.7</span>          <span class="number">12</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">12</span>          <span class="number">12</span>          耶鲁大学   <span class="number">50.7</span>          <span class="number">13</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">13</span>          <span class="number">14</span>     华盛顿大学-西雅图   <span class="number">50.0</span>          <span class="number">14</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">14</span>          <span class="number">15</span>     加州大学-圣地亚哥   <span class="number">47.8</span>          <span class="number">15</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">15</span>          <span class="number">16</span>       宾夕法尼亚大学   <span class="number">46.4</span>          <span class="number">16</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">16</span>          <span class="number">17</span>        伦敦大学学院   <span class="number">46.1</span>          <span class="number">17</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">17</span>          <span class="number">18</span>      约翰霍普金斯大学   <span class="number">45.4</span>          <span class="number">18</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">18</span>          <span class="number">19</span>     苏黎世联邦理工学院   <span class="number">43.9</span>          <span class="number">19</span>  <span class="number">2018</span>  Switzerland</span><br><span class="line"><span class="number">19</span>          <span class="number">20</span>    华盛顿大学-圣路易斯   <span class="number">42.1</span>          <span class="number">20</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">20</span>          <span class="number">21</span>      加州大学-旧金山   <span class="number">41.9</span>          <span class="number">21</span>  <span class="number">2018</span>          USA</span><br></pre></td></tr></table></figure><p>数据很完美，接下来就可以按照D3.js模板中的example.csv文件的格式作进一步的处理了。  </p><h3 id="2-3-数据处理"><a href="#2-3-数据处理" class="headerlink" title="2.3. 数据处理"></a>2.3. 数据处理</h3><p>这里先将数据输出为<code>university.csv</code>文件，结果见下表：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/44505347.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/44505347.jpg"></p><p>10年一共5011行×6列数据。接着，读入该表作进一步数据处理，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'university.csv'</span>)</span><br><span class="line"><span class="comment"># 包含港澳台</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只包括内地</span></span><br><span class="line">df = df.query(<span class="string">"(country == 'China')"</span>)</span><br><span class="line">df[<span class="string">'index_rank_score'</span>] = df[<span class="string">'index_rank'</span>]</span><br><span class="line"><span class="comment"># 将index_rank列转为整形</span></span><br><span class="line">df[<span class="string">'index_rank'</span>] = df[<span class="string">'index_rank'</span>].astype(int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 美国</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'UnitedStates')|(country == 'USA')")</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#求topn名</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(df)</span>:</span></span><br><span class="line">    top = df.sort_values([<span class="string">'year'</span>,<span class="string">'index_rank'</span>],ascending = <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> top[:<span class="number">20</span>].reset_index()</span><br><span class="line">df = df.groupby(by =[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改列顺序</span></span><br><span class="line">df = df[[<span class="string">'university'</span>,<span class="string">'index_rank_score'</span>,<span class="string">'index_rank'</span>,<span class="string">'year'</span>]]</span><br><span class="line"><span class="comment"># 重命名列</span></span><br><span class="line">df.rename (columns = &#123;<span class="string">'university'</span>:<span class="string">'name'</span>,<span class="string">'index_rank_score'</span>:<span class="string">'type'</span>,<span class="string">'index_rank'</span>:<span class="string">'value'</span>,<span class="string">'year'</span>:<span class="string">'date'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">df.to_csv(<span class="string">'university_ranking.csv'</span>,mode =<span class="string">'w'</span>,encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># index可以设置</span></span><br></pre></td></tr></table></figure><p>上面需要注意两点：</p><ul><li>可以提取包含港澳台在内的大中华区所有的大学，也可以只提取内地的大学，还可以提取世界、美国等各种排名。</li><li>定义了一个求Topn的函数，能够按年份分别求出各年的前20名大学名单。</li></ul><p>打开输出的<code>university_ranking.csv</code>文件：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/64400003.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/64400003.jpg"></p><p>结果非常好，可以直接作为D3.js的导入文件了。</p><h4 id="2-3-1-完整代码"><a href="#2-3-1-完整代码" class="headerlink" title="2.3.1. 完整代码"></a>2.3.1. 完整代码</h4><p>将代码再稍微完善一下，完整地代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"><span class="comment"># 获取网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(year)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            headers = &#123;</span><br><span class="line">                <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment"># 英文版</span></span><br><span class="line">            <span class="comment"># url = 'http://www.shanghairanking.com/ARWU%s.html' % (str(year))</span></span><br><span class="line">            <span class="comment"># 中文版</span></span><br><span class="line">            url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br><span class="line">            response = requests.get(url,headers = headers)</span><br><span class="line">            <span class="comment"># 2009-2015用'gbk'，2016-2018用'utf-8'</span></span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="comment"># return response.text  # text会乱码，content没有问题</span></span><br><span class="line">                <span class="comment"># https://stackoverflow.com/questions/17011357/what-is-the-difference-between-content-and-text</span></span><br><span class="line">                <span class="keyword">return</span> response.content</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">except</span> RequestException:</span><br><span class="line">            print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html,i)</span>:</span></span><br><span class="line">        tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 重命名表格列，不需要的列用数字表示</span></span><br><span class="line">        tb.columns = [<span class="string">'world rank'</span>,<span class="string">'university'</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="string">'score'</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">        tb.drop([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],axis = <span class="number">1</span>,inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 删除后面不需要的评分列</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># rank列100名后是区间，需需唯一化，增加一列index作为排名</span></span><br><span class="line">        tb[<span class="string">'index_rank'</span>] = tb.index</span><br><span class="line">        tb[<span class="string">'index_rank'</span>] = tb[<span class="string">'index_rank'</span>].astype(int) + <span class="number">1</span></span><br><span class="line">        <span class="comment"># 增加一列年份列</span></span><br><span class="line">        tb[<span class="string">'year'</span>] = i</span><br><span class="line">        <span class="comment"># read_html没有爬取country，需定义函数单独爬取</span></span><br><span class="line">        tb[<span class="string">'country'</span>] = get_country(html)</span><br><span class="line">        <span class="comment"># print(tb) # 测试表格ok</span></span><br><span class="line">        <span class="keyword">return</span> tb</span><br><span class="line">        <span class="comment"># print(tb.info()) # 查看表信息</span></span><br><span class="line">        <span class="comment"># print(tb.columns.values) # 查看列表名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_country</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    countries = soup.select(<span class="string">'td &gt; a &gt; img'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> countries:</span><br><span class="line">        src = i[<span class="string">'src'</span>]</span><br><span class="line">        pattern = re.compile(<span class="string">'flag.*\/(.*?).png'</span>)</span><br><span class="line">        country = re.findall(pattern,src)[<span class="number">0</span>]</span><br><span class="line">        lst.append(country)</span><br><span class="line">    <span class="keyword">return</span> lst</span><br><span class="line">    <span class="comment"># print(lst) # 测试提取国家是否成功ok</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存表格为csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_csv</span><span class="params">(tb)</span>:</span></span><br><span class="line">    tb.to_csv(<span class="string">r'university.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    endtime = time.time()-start_time</span><br><span class="line">    <span class="comment"># print('程序运行了%.2f秒' %endtime)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis</span><span class="params">()</span>:</span></span><br><span class="line">    df = pd.read_csv(<span class="string">'university.csv'</span>)</span><br><span class="line">    <span class="comment"># 包含港澳台</span></span><br><span class="line">    <span class="comment"># df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]</span></span><br><span class="line">    <span class="comment"># 只包括内地</span></span><br><span class="line">    df = df.query(<span class="string">"(country == 'China')"</span>)[[<span class="string">'university'</span>,<span class="string">'year'</span>,<span class="string">'index_rank'</span>]]</span><br><span class="line"></span><br><span class="line">    df[<span class="string">'index_rank_score'</span>] = df[<span class="string">'index_rank'</span>]</span><br><span class="line">    <span class="comment"># 将index_rank列转为整形</span></span><br><span class="line">    df[<span class="string">'index_rank'</span>] = df[<span class="string">'index_rank'</span>].astype(int)</span><br><span class="line">    <span class="comment"># 美国</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'UnitedStates')|(country == 'USA')")[['university','year','index_rank']]</span></span><br><span class="line">    <span class="comment">#求topn名</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(df)</span>:</span></span><br><span class="line">        top = df.sort_values([<span class="string">'year'</span>,<span class="string">'index_rank'</span>],ascending = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> top[:<span class="number">20</span>].reset_index()</span><br><span class="line">    df = df.groupby(by =[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line">    <span class="comment"># 更改列顺序</span></span><br><span class="line">    df = df[[<span class="string">'university'</span>,<span class="string">'index_rank_score'</span>,<span class="string">'index_rank'</span>,<span class="string">'year'</span>]]</span><br><span class="line">    <span class="comment"># 重命名列</span></span><br><span class="line">    df.rename (columns = &#123;<span class="string">'university'</span>:<span class="string">'name'</span>,<span class="string">'index_rank_score'</span>:<span class="string">'type'</span>,<span class="string">'index_rank'</span>:<span class="string">'value'</span>,<span class="string">'year'</span>:<span class="string">'date'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    df.to_csv(<span class="string">'university_ranking.csv'</span>,mode =<span class="string">'w'</span>,encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># index可以设置</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(year)</span>:</span></span><br><span class="line">    <span class="comment"># generate_mysql()</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2009</span>,year):  <span class="comment">#抓取10年</span></span><br><span class="line">        <span class="comment"># get_one_page(i)</span></span><br><span class="line">        html = get_one_page(i)</span><br><span class="line">        <span class="comment"># parse_one_page(html,i)  # 测试表格ok</span></span><br><span class="line">        tb = parse_one_page(html,i)</span><br><span class="line">        save_csv(tb)</span><br><span class="line">        print(i,<span class="string">'年排名提取完成完成'</span>)</span><br><span class="line">        analysis()</span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(<span class="number">2019</span>)</span><br><span class="line">    <span class="comment"># 2016-2018采用gb2312编码，2009-2015采用utf-8编码</span></span><br></pre></td></tr></table></figure><p>至此，我们已经有<code>university_ranking.csv</code>基础数据，下面就可以进行可视化呈现了。</p><h3 id="2-4-可视化呈现"><a href="#2-4-可视化呈现" class="headerlink" title="2.4. 可视化呈现"></a>2.4. 可视化呈现</h3><p>首先，到见齐的github主页：<br><a href="https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js" target="_blank" rel="noopener">https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</a> </p><h4 id="2-4-1-克隆仓库文件"><a href="#2-4-1-克隆仓库文件" class="headerlink" title="2.4.1. 克隆仓库文件"></a>2.4.1. 克隆仓库文件</h4><p>如果你平常使用github或者Git软件的话，那么就找个合适文件存放目录，然后直接在 GitBash里分别输入下面3条命令就搭建好环境了：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 克隆项目仓库</span><br><span class="line">git clone https:<span class="comment">//github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</span></span><br><span class="line"># 切换到项目根目录</span><br><span class="line">cd Historical-ranking-data-visualization-based-on-d3.js</span><br><span class="line"># 安装依赖</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>如果你此前没有用过上面的软件，你可以直接点击<code>Download Zip</code>下载下来然后解压即可，不过还是强烈建议使用第一种方法，因为后面如果要自定义可视化效果的话，需要修改代码然后执行<code>npm run build</code>命令才能够看到效果。</p><h4 id="2-4-2-效果呈现"><a href="#2-4-2-效果呈现" class="headerlink" title="2.4.2. 效果呈现"></a>2.4.2. 效果呈现</h4><p>好，所有基本准备都已完成，下面就可以试试看效果了。<br>任意浏览器打开<code>bargraph.html</code>网页，点击选择文件，然后选择：前面输出的<code>university_ranking.csv</code>文件，看下效果吧：<br><a href="https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0" target="_blank" rel="noopener">https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0</a></p><p>可以看到，有了大致的可视化效果，但还存在很多瑕疵，比如：表顺序颠倒了、字体不合适、配色太花哨等。可不可以修改呢？  </p><p>当然是可以的，只需要分别修改文件夹中这几个文件的参数就可以了：  </p><ul><li>config.js 全局设置各项功能的开关，比如配色、字体、文字名称、反转图表等等功能；</li><li>color.css 修改柱形图的配色；</li><li>stylesheet.css 具体修改配色、字体、文字名称等的css样式；</li><li>visual.js 更进一步的修改，比如图表的透明度等。</li></ul><p>知道在哪里修改了以后，那么，如何修改呢？很简单，只需要简单的几步就可以实现：  </p><ul><li><p>打开网页，<code>右键-检查</code>，箭头指向想要修改的元素，然后在右侧的css样式表里，双击各项参数修改参数，修改完元素就会发生变化，可以不断微调，直至满意为止。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/63524613.jpg" alt=""></p></li><li><p>把参数复制到四个文件中对应的文件里并保存。</p></li><li>Git Bash不断重复运行<code>npm run build</code>，之后刷新网页就可以看到优化后的效果。</li></ul><p>最后，再添加一个合适的BGM就可以了。以下是我优化之后的效果：<br><a href="https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0" target="_blank" rel="noopener">https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0</a><br><em>BGM：ツナ覚醒</em></p><p>如果你不太会调整，没有关系，我会分享优化后的配置文件。</p><p>以上，就是实现动态可视化表的步骤。 同样地，只要更改数据源可以很方便地做出世界、美国等大学的动态效果，可以看看：<br>中国（含港澳台）大学排名：<br><a href="http://pc1lljdwb.bkt.clouddn.com/Greater_China_uni_ranking.mp4" target="_blank" rel="noopener">http://pc1lljdwb.bkt.clouddn.com/Greater_China_uni_ranking.mp4</a><br>美国大学排名：<br><a href="http://pc1lljdwb.bkt.clouddn.com/USA_uni_ranking.mp4" target="_blank" rel="noopener">http://pc1lljdwb.bkt.clouddn.com/USA_uni_ranking.mp4</a></p><p>文章所有的素材，在公众号后台回复<strong>大学排名</strong>就可以得到，或者到我的github下载：<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a><br>感兴趣的话就动手试试吧。</p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python爬取近十年中国大学Top20强并结合D3.js做动态数据可视化表。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://www.makcyun.top/categories/Python/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(4)：图片批量下载-以澎湃网信息图为例</title>
    <link href="https://www.makcyun.top/web_scraping_withpython4.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython4.html</id>
    <published>2018-09-02T00:00:57.468Z</published>
    <updated>2018-10-18T08:39:22.647Z</updated>
    
    <content type="html"><![CDATA[<p>澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。</p><a id="more"></a>  <p><strong>摘要：</strong> 上一篇文章介绍了单页图片的爬取，但是当爬取多页时，难度会增加。同时，前几篇爬虫文章中的网站有一个明显的特点是：可以通过点击鼠标实现网页的翻页，并且url会发生相应的变化。除了此类网站以外，还有一类非常常见的网站特点是：没有”下一页”这样的按钮，而是”加载更多”或者会不断自动刷新从而呈现出更多的内容，同时网页url也不发生变化。这种类型的网页通常采用的是Ajax技术，要抓取其中的网页内容需要采取一定的技巧。本文以信息图做得非常棒的澎湃”美数课”为例，抓取该栏目至今所有文章的图片。<br>栏目网址：<a href="https://www.thepaper.cn/list_25635" target="_blank" rel="noopener">https://www.thepaper.cn/list_25635</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/30853746.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/51553778.jpg" alt=""></p><p><strong>本文知识点：</strong>  </p><ul><li>Ajax知识</li><li>多页图片爬取</li></ul><h2 id="1-Ajax知识"><a href="#1-Ajax知识" class="headerlink" title="1. Ajax知识"></a>1. Ajax知识</h2><p>在该主页上尝试不断下拉，会发现网页不断地加载出新的文章内容来，而并不需要通过点击”下一页”来实现，而且网址url也保持不变。也就是说在同一个网页中通过下拉源源不断地刷新出了网页内容。这种形式的网页在今天非常常见，它们普遍是采用了<strong>Ajax</strong>技术。   </p><blockquote><p>Ajax 全称是 Asynchronous JavaScript and XML（异步 JavaScript 和 XML）。<br>它不是一门编程语言，而是利用 JavaScript 在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。</p></blockquote><p>Ajax更多参考：<br><a href="https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html</a></p><p>采用了Ajax的网页和普通的网页有一定的区别，普通网页的爬虫代码放在这种类型的网页上就行不通了，必须另辟出路。下面我们就来尝试一下如何爬取网易”数读”所有的文章。 </p><p>主页<code>右键-检查</code>，然后按<code>f5</code>刷新，会弹出很多链接文件。鼠标上拉回到第一个文件：<strong>list_25635</strong>，在右侧按<code>ctrl+f</code>搜索一下第一篇文章的标题：”娃娃机生意经”，可以看到在html网页中找到了对应的源代码。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/67533549.jpg" alt=""></p><p>接着，我们拖动下拉鼠标，显示出更多文章。然后再次搜索一篇文章的标题：”金砖峰会”，会发现搜不到相应的内容了。是不是感觉很奇怪？</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/72910700.jpg" alt=""></p><p>其实，这里就是用了Ajax的技术，和普通网页翻页是刷新整个网页不同，这种类型网页可以再保持url不变的前提下只刷新部分内容。这就为我们进行爬虫带来了麻烦。因为，我们通过解析网页的url：<code>https://www.thepaper.cn/list_25635</code>只能爬取前面部分的内容而后面通过下拉刷新出来的内容是爬取不到的。这显然不完美，那么怎么才能够爬取到后面不断刷新出来的网页内容呢？   </p><h2 id="2-url分析"><a href="#2-url分析" class="headerlink" title="2. url分析"></a>2. url分析</h2><p>我们把右侧的选项卡从<code>ALL</code>切换到<code>Network</code>，然后按再次按<code>f5</code>刷新，可以发现<code>Name</code>列有4个结果。选择第3个链接打开并点击<code>Response</code>，通过滑动可以看到一些文本内容和网页中的文章标题是一一对应的。比如第一个是：<strong>娃娃机生意经｜有没有好奇过抓娃娃机怎么又重新火起来了？</strong>，一直往下拖拽可以看到有很多篇文章。此时，再切换到headers选项卡，复制<code>Request URL</code>后面的链接并打开，会显示一部分文章的标题和图片内容。数一下的话，可以发现一共有20个文章标题，也就是对应着20篇文章。  </p><p>这个链接其实和上面的<strong>list_25635</strong>链接的内容是一致的。这样看来，好像发现不了什么东西，不过不要着急。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/40705043.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/642179.jpg" alt=""></p><p>接下来，回到<code>Name</code>列，尝试滚动下拉鼠标，会发现弹出好几个新的开头为<code>load_index</code>的链接来。选中第一个<code>load_index</code>的链接，点击<code>Response</code>查看一下html源代码，尝试在网页中搜索一下：<code>十年金砖峰</code>这个文章的标题，惊奇地发现，在网页中找到了对于的文章标题。而前面，我们搜索这个词时，是没有搜索到的。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/83879931.jpg" alt=""></p><p>这说明了什么呢？说明<code>十年金砖峰</code>这篇文章的内容不在第一个<strong>list_25635</strong>链接中，而在这个<code>load_index</code>的链接里。鼠标点击<code>headers</code>，复制<code>Request URL</code>后面的链接并打开，就可以再次看到包括这篇文章在内的新的20篇文章。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/4302921.jpg" alt=""></p><p>是不是发现了点了什么？接着，我们继续下拉，会发现弹出更多的<code>load_index</code>的链接。再搜索一个标题：<code>地图湃｜海外港口热</code>，可以发现在网页中也同样找到了文章标题。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/68563785.jpg" alt=""></p><p>回到我们的初衷：<strong>下载所有网页的图片内容</strong>。那么现在就有解决办法礼：一个个地把出现的这些url网址中图片下载下来就大功告成了。</p><p>好，我们先来分析一下这些url，看看有没有相似性，如果有很明显的相似性，那么就可以像普通网页那样，通过构造翻页页数的url，实现for循环就可以批量下载所有网页的图片了。复制前3个链接如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=2&amp;isList=true&amp;lastTime=1533169319712  </span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=3&amp;isList=true&amp;lastTime=1528625875167  </span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=4&amp;isList=true&amp;lastTime=1525499007926</span><br></pre></td></tr></table></figure></p><p>发现<code>pageidx</code>键的值呈现规律的数字递增变化，看起来是个好消息。但同时发现后面的lastTime键的值看起来是随机变化的，这个有没有影响呢？ 来测试一下，复制第一个链接，删掉<code>&amp;lastTime=1533169319712</code>这一串字符，会发现网页一样能够正常打开，就说明着一对参数不影响网页内容，那就太好了。我们可以删除掉，这样所有url的区别只剩<code>pageidx</code>的值了，这时就可以构造url来实现for循环了。构造的url形式如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=2</span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=3</span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=4</span><br></pre></td></tr></table></figure></p><p>同时，尝试把数字2改成1并打开链接看看会有什么变化，发现呈现的内容就是第1页的内容。这样，我们就可以从第一页开始构造url循环了。<br><code>https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=1</code>  </p><p>既然确定了首页，那么也要相应地确定一下尾页。很简单，我们把数字改大然后打开链接看是否有内容即可。比如改为<strong>10 </strong>，打开发现有内容显示，很好。接着，再改为30，发现没有内容了。说明该栏目的页数介于这两个数之间，尝试几次后，发现<code>25</code>是最后一个有内容的网页，也意味着能够爬取的页数一共是25页。  </p><p>确定了首页和尾页后，下面我们就可以开始构造链接，先爬取第一篇文章网页里的图片（这个爬取过程，我们上一篇爬取网易”数读”已经尝试过了），然后爬取这一整页的图片，最后循环25页，爬取所有图片，下面开始吧。</p><h2 id="3-程序代码"><a href="#3-程序代码" class="headerlink" title="3. 程序代码"></a>3. 程序代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 获取索引界面网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_index</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment"># 下载1页</span></span><br><span class="line">    <span class="comment"># url = 'https://www.thepaper.cn/newsDetail_forward_2370041'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2下载多页，构造url</span></span><br><span class="line">    paras = &#123;</span><br><span class="line">        <span class="string">'nodeids'</span>: <span class="number">25635</span>,</span><br><span class="line">        <span class="string">'pageidx'</span>: i</span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'https://www.thepaper.cn/load_index.jsp?'</span> + urlencode(paras)</span><br><span class="line"></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="comment"># print(response.text)  # 测试网页内容是否提取成功ok</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 解析索引界面网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_index</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每页文章数</span></span><br><span class="line">    num = soup.find_all(name = <span class="string">'div'</span>,class_=<span class="string">'news_li'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)):</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="comment"># 获取title</span></span><br><span class="line">        <span class="string">'title'</span>:soup.select(<span class="string">'h2 a'</span>)[i].get_text(),</span><br><span class="line">        <span class="comment"># 获取图片url，需加前缀</span></span><br><span class="line">        <span class="string">'url'</span>:<span class="string">'https://www.thepaper.cn/'</span> + soup.select(<span class="string">'h2 a'</span>)[i].attrs[<span class="string">'href'</span>]</span><br><span class="line">        <span class="comment"># print(url)  # 测试图片链接</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 获取每条文章的详情页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_detail</span><span class="params">(item)</span>:</span></span><br><span class="line">    url = item.get(<span class="string">'url'</span>)</span><br><span class="line">    <span class="comment"># 增加异常捕获语句</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,headers = headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="comment"># print(response.text)  # 测试网页内容是否提取成功</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        print(<span class="string">'网页请求失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 4 解析每条文章的详情页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_detail</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># 获取title</span></span><br><span class="line">    <span class="keyword">if</span> soup.h1:  <span class="comment">#有的网页没有h1节点，因此必须要增加判断，否则会报错</span></span><br><span class="line">        title = soup.h1.string</span><br><span class="line">        <span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">        items = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>,<span class="string">'600'</span>])</span><br><span class="line">        <span class="comment"># 有的图片节点用width='100%'表示，有的用600表示，因此用list合并选择</span></span><br><span class="line">        <span class="comment"># https://blog.csdn.net/w_xuechun/article/details/76093950</span></span><br><span class="line">        <span class="comment"># print(items) # 测试返回的img节点ok</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(items)):</span><br><span class="line">            pic = items[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">            <span class="comment"># print(pic) #测试图片链接ok</span></span><br><span class="line">            <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'title'</span>:title,</span><br><span class="line">            <span class="string">'pic'</span>:pic,</span><br><span class="line">            <span class="string">'num'</span>:i  <span class="comment"># 图片添加编号顺序</span></span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment"># 5 下载图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pic</span><span class="params">(pic)</span>:</span></span><br><span class="line">    title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">    <span class="comment"># 标题规范命名：去掉符号非法字符| 等</span></span><br><span class="line">     title = re.sub(<span class="string">'[\/:*?"&lt;&gt;|]'</span>,<span class="string">'-'</span>,title).strip()</span><br><span class="line">    url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line">    <span class="comment"># 设置图片编号顺序</span></span><br><span class="line">    num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">        os.mkdir(title)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片url网页信息</span></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 建立图片存放地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line">            <span class="comment"># 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest()</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="comment"># 开始下载图片</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">                    print(<span class="string">'文章"&#123;0&#125;"的第&#123;1&#125;张图片下载完成'</span> .format(title,num))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'该图片%s 已下载'</span> %title)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e,<span class="string">'图片获取失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment"># get_page_index(i) # 测试索引界面网页内容是否获取成功ok</span></span><br><span class="line"></span><br><span class="line">    html = get_page_index(i)</span><br><span class="line">    data = parse_page_index(html)  <span class="comment"># 测试索引界面url是否获取成功ok</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># print(item)  #测试返回的dict</span></span><br><span class="line">        html = get_page_detail(item)</span><br><span class="line">        data = parse_page_detail(html)</span><br><span class="line">        <span class="keyword">for</span> pic <span class="keyword">in</span> data:</span><br><span class="line">            save_pic(pic)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>):</span><br><span class="line">        main(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    pool.map(main,[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>)])</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/46673865.jpg" alt="">  </p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-3/4372426.jpg" alt="">  </p><p>文章代码和栏目从2015年至今437篇文章共1509张图片资源，可在下方链接中得到。<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫(3)：单页图片下载-网易&quot;数读&quot;信息图</title>
    <link href="https://www.makcyun.top/web_scraping_withpython3.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython3.html</id>
    <published>2018-09-01T06:49:30.854Z</published>
    <updated>2018-10-20T07:31:01.300Z</updated>
    
    <content type="html"><![CDATA[<p>下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。</p><a id="more"></a>  <p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/65507772.jpg" alt="">  </p><p><strong>本文知识点：</strong>  </p><ul><li>单张图片下载</li><li>单页图片下载  </li><li>Ajax技术介绍</li></ul><h2 id="1-单张图片下载"><a href="#1-单张图片下载" class="headerlink" title="1. 单张图片下载"></a>1. 单张图片下载</h2><p>以一篇最近比较热的涨房价的文章为例：<a href="http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html" target="_blank" rel="noopener">暴涨的房租，正在摧毁中国年轻人的生活</a>，从文章里随意挑选一张<a href="http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png" target="_blank" rel="noopener">北京房租地图图片</a>，通过<strong>Requests的content属性</strong>来实现单张图片的下载。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/14528855.jpg" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'北京房租地图.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure></p><p>5行代码就能将这张图片下载到电脑上。不只是该张图片，任意图片都可以下载，只要替换图片的url即可。<br>这里用到了<strong>Requests的content属性</strong>，将图片存储为二进制数据。至于，图片为什么可以用二进制数据进行存储，可以参考这个教程：<br><a href="https://www.zhihu.com/question/36269548/answer/66734582" target="_blank" rel="noopener">https://www.zhihu.com/question/36269548/answer/66734582</a></p><p>5行代码看起来很短，但如果只是下载一张图片显然没有必要写代码，”右键另存为”更快。现在，我们放大一下范围，去下载这篇文章中的所有图片。粗略数一下，网页里有超过15张图片，这时，如果再用”右键另存为”的方法，显然就比较繁琐了。下面，我们用代码来实现下载该网页中的所有图片。</p><h2 id="2-单页图片下载"><a href="#2-单页图片下载" class="headerlink" title="2. 单页图片下载"></a>2. 单页图片下载</h2><h3 id="2-1-Requests获取网页内容"><a href="#2-1-Requests获取网页内容" class="headerlink" title="2.1. Requests获取网页内容"></a>2.1. Requests获取网页内容</h3><p>首先，用堪称python”爬虫利器”的<strong>Requests库</strong>来获取该篇文章的html内容。<br>Requests库可以说是一款python爬虫的利器，它的更多用法，可参考下面的教程：<br><a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/index.html</a><br><a href="https://cuiqingcai.com/2556.html" target="_blank" rel="noopener">https://cuiqingcai.com/2556.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">url = <span class="string">'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'</span></span><br><span class="line"></span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="comment"># return response.text</span></span><br><span class="line">    print(response.text)  <span class="comment"># 测试网页内容是否提取成功ok</span></span><br></pre></td></tr></table></figure><h3 id="2-2-解析网页内容"><a href="#2-2-解析网页内容" class="headerlink" title="2.2. 解析网页内容"></a>2.2. 解析网页内容</h3><p>通过上面方法可以获取到html内容，接下来解析html字符串内容，从中提取出网页内的图片url。解析和提取url的方法有很多种，常见的有5种，分别是：正则表达式、Xpath、BeautifulSoup、CSS、PyQuery。任选一种即可，这里为了再次加强练习，5种方法全部尝试一遍。<br>首先，在网页中定位到图片url所在的位置，如下图所示：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/91340067.jpg" alt=""></p><p>从外到内定位url的位置：<code>&lt;p&gt;节点-&lt;a&gt;节点-&lt;img&gt;节点里的src属性值</code>。   </p><h4 id="2-2-1-正则表达式"><a href="#2-2-1-正则表达式" class="headerlink" title="2.2.1. 正则表达式"></a>2.2.1. 正则表达式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern =re.compile(<span class="string">'&lt;p&gt;.*?&lt;img alt="房租".*?src="(.*?)".*?style'</span>,re.S)</span><br><span class="line">    items = re.findall(pattern,html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行结果如下:<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/425eca61322a4f99837988bb78a001ac.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/d6cb58a6bb014b8683b232f3c00f0e39.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/88d2e535765a4ed09e03877238647aa5.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/09/01/98d2f9579e9e49aeb76ad6155e8fc4ea.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7410ed4041a94cab8f30e8de53aaaaa1.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/49a0c80a140b4f1aa03724654c5a39af.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3070964278bf4637ba3d92b6bb771cea.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/812b7a51475246a9b57f467940626c5c.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/8bcbc7d180f74397addc74e47eaa1f63.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/e593efca849744489096a77aafd10d3e.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7653feecbfd94758a8a0ff599915d435.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/edbaa24a17dc4cca9430761bfc557ffb.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/f768d440d9f14b8bb58e3c425345b97e.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3430043fd305411782f43d3d8635d632.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/111ba73d11084c68b8db85cdd6d474a7.png'&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-Xpath语法"><a href="#2-2-2-Xpath语法" class="headerlink" title="2.2.2. Xpath语法"></a>2.2.2. Xpath语法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'*//p//img[@alt = "房租"]/@src'</span>)</span><br><span class="line">    print(items)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>结果同上。</p><h4 id="2-2-3-CSS选择器"><a href="#2-2-3-CSS选择器" class="headerlink" title="2.2.3. CSS选择器"></a>2.2.3. CSS选择器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = soup.select(<span class="string">'p &gt; a &gt; img'</span>) <span class="comment">#&gt;表示下级绝对节点</span></span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item[<span class="string">'src'</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="2-2-4-BeautifulSoup-find-all方法"><a href="#2-2-4-BeautifulSoup-find-all方法" class="headerlink" title="2.2.4. BeautifulSoup find_all方法"></a>2.2.4. BeautifulSoup find_all方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"><span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">item = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(item)):</span><br><span class="line">    url = item[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'url'</span>:url</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># print(pic) #测试图片链接ok</span></span><br></pre></td></tr></table></figure><h4 id="2-2-5-PyQuery"><a href="#2-2-5-PyQuery" class="headerlink" title="2.2.5. PyQuery"></a>2.2.5. PyQuery</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">data = pq(html)</span><br><span class="line">data2 = data(<span class="string">'p &gt; a &gt; img'</span>)</span><br><span class="line"><span class="comment"># print(items)</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> data2.items():   <span class="comment">#注意这里和BeautifulSoup 的css用法不同</span></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'url'</span>:item.attr(<span class="string">'src'</span>)</span><br><span class="line">    <span class="comment"># 或者'url':item.attr.src</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>以上用了5种方法提取出了该网页的url地址，任选一种即可。这里假设选择了第4种方法，接下来就可以下载图片了。提取出的网址是一个<strong>dict字典</strong>，通过dict的get方法调用里面的键和值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line"><span class="comment"># 设置图片编号顺序</span></span><br><span class="line">num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">    os.mkdir(title)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图片url网页信息</span></span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立图片存放地址</span></span><br><span class="line">file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line"><span class="comment"># 文件名采用编号方便按顺序查看</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始下载图片</span></span><br><span class="line"><span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">   f.write(response.content)</span><br><span class="line">   print(<span class="string">'该图片已下载完成'</span>,title)</span><br></pre></td></tr></table></figure></p><p>很快，15张图片就按着文章的顺序下载下来了。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/94009879.jpg" alt=""></p><p>将上述代码整理一下，增加一点异常处理和图片的标题、编号的代码以让爬虫更健壮，完整的代码如下所示：</p><h3 id="2-3-全部代码"><a href="#2-3-全部代码" class="headerlink" title="2.3. 全部代码"></a>2.3. 全部代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 下载1页</span></span><br><span class="line">    url = <span class="string">'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加异常捕获语句</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,headers = headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="comment"># print(response.text)  # 测试网页内容是否提取成功</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        print(<span class="string">'网页请求失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># 获取title</span></span><br><span class="line">    title = soup.h1.string</span><br><span class="line">    <span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">    item = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>])</span><br><span class="line">    <span class="comment"># print(item) # 测试</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(item)):</span><br><span class="line">        pic = item[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'title'</span>:title,</span><br><span class="line">        <span class="string">'pic'</span>:pic,</span><br><span class="line">        <span class="string">'num'</span>:i  <span class="comment"># 图片添加编号顺序</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># print(pic) #测试图片链接ok</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pic</span><span class="params">(pic)</span>:</span></span><br><span class="line">    title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">    url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line">    <span class="comment"># 设置图片编号顺序</span></span><br><span class="line">    num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">        os.mkdir(title)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片url网页信息</span></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 建立图片存放地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line">            <span class="comment"># 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest()</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="comment"># 开始下载图片</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">                    print(<span class="string">'该图片已下载完成'</span>,title)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'该图片%s 已下载'</span> %title)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e,<span class="string">'图片获取失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># get_page() # 测试网页内容是获取成功ok</span></span><br><span class="line">    html = get_page()</span><br><span class="line">    <span class="comment"># parse_page(html) # 测试网页内容是否解析成功ok</span></span><br><span class="line"></span><br><span class="line">    data = parse_page(html)</span><br><span class="line">    <span class="keyword">for</span> pic <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># print(pic) #测试dict</span></span><br><span class="line">        save_pic(pic)</span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>小结</strong><br>上面通过爬虫实现下载一张图片延伸到下载一页图片，相比于手动操作，爬虫的优势逐渐显现。那么，能否实现多页循环批量下载更多的图片呢，当然可以，下一篇文章将进行介绍。  </p><p>你也可以尝试一下，这里先放上”福利”：网易”数读”栏目从2012年至今350篇文章的全部图片已下载完成。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/90267464.jpg" alt=""></p><p>如果你需要，可以到我的github下载。<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a>  </p><p>本文完。  </p><p><img src="http://pbscl931v.bkt.clouddn.com/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(2)：10行代码爬取全国所有A股/港股/新三板上市公司信息</title>
    <link href="https://www.makcyun.top/web_scraping_withpython2.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython2.html</id>
    <published>2018-08-27T00:26:57.000Z</published>
    <updated>2018-10-22T05:18:45.390Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>python爬虫第2篇</em></b><br>利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。</p><a id="more"></a>  <p><strong>摘要：</strong> 我们平常在浏览网页中会遇到一些表格型的数据信息，除了表格本身体现的内容以外，你可能想透过表格再更进一步地进行汇总、筛选、处理分析等操作从而得到更多有价值的信息，这时可用python爬虫来实现。本文采用pandas库中的read_html方法来快速准确地抓取表格数据。</p><p><strong>本文知识点：</strong>  </p><ul><li>Table型表格抓取</li><li>DataFrame.read_html函数使用  </li><li>爬虫数据存储到mysql数据库</li><li>Navicat数据库的使用</li></ul><h2 id="1-table型表格"><a href="#1-table型表格" class="headerlink" title="1. table型表格"></a>1. table型表格</h2><p>我们在网页上会经常看到这样一些表格，比如：<br><a href="http://ranking.promisingedu.com/qs" target="_blank" rel="noopener">QS2018世界大学排名</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/59439970.jpg" alt=""></p><p><a href="http://www.fortunechina.com/fortune500/c/2018-07/19/content_311046.htm" target="_blank" rel="noopener">财富世界500强企业排名</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/66712901.jpg" alt=""></p><p><a href="https://www.boxofficemojo.com/" target="_blank" rel="noopener">IMDB世界电影票房排行榜</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/76002510.jpg" alt=""></p><p><a href="http://pbscl931v.bkt.clouddn.com/18-8-27/78659021.jpg" target="_blank" rel="noopener">中国上市公司信息</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/78659021.jpg" alt=""></p><p>他们除了都是表格以外，还一个共同点就是当你点击右键-定位时，可以看到他们都是table类型的表格形式。<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/87245193.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/54573575.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/21054545.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/30765316.jpg" alt=""></p><p>从中可以看到table类型的表格网页结构大致如下：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">class</span>=<span class="string">"..."</span> <span class="attr">id</span>=<span class="string">"..."</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">th</span>&gt;</span>...<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tbody</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>...<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">tbody</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>先来简单解释一下上文出现的几种标签含义：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>: 定义表格</span><br><span class="line"><span class="tag">&lt;<span class="name">thead</span>&gt;</span>: 定义表格的页眉</span><br><span class="line"><span class="tag">&lt;<span class="name">tbody</span>&gt;</span>: 定义表格的主体</span><br><span class="line"><span class="tag">&lt;<span class="name">tr</span>&gt;</span>: 定义表格的行</span><br><span class="line"><span class="tag">&lt;<span class="name">th</span>&gt;</span>: 定义表格的表头</span><br><span class="line"><span class="tag">&lt;<span class="name">td</span>&gt;</span>: 定义表格单元</span><br></pre></td></tr></table></figure></p><p>这样的表格数据，就可以利用pandas模块里的read_html函数方便快捷地抓取下来。下面我们就来操作一下。</p><h2 id="2-快速抓取"><a href="#2-快速抓取" class="headerlink" title="2. 快速抓取"></a>2. 快速抓取</h2><p>下面以中国上市公司信息这个网页中的表格为例，感受一下read_html函数的强大之处。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">178</span>):  <span class="comment"># 爬取全部177页数据</span></span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s'</span> % (str(i))</span><br><span class="line">tb = pd.read_html(url)[<span class="number">3</span>] <span class="comment">#经观察发现所需表格是网页中第4个表格，故为[3]</span></span><br><span class="line">tb.to_csv(<span class="string">r'1.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="number">1</span>, index=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'第'</span>+str(i)+<span class="string">'页抓取完成'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/96662344.jpg" alt=""><br>只需不到十行代码，1分钟左右就可以将全部178页共3536家A股上市公司的信息干净整齐地抓取下来。比采用正则表达式、xpath这类常规方法要省心省力地多。如果采取人工一页页地复制粘贴到excel中，就得操作到猴年马月去了。<br>上述代码除了能爬上市公司表格以外，其他几个网页的表格都可以爬，只需做简单的修改即可。因此，可作为一个简单通用的代码模板。但是，为了让代码更健壮更通用一些，接下来，以爬取177页的A股上市公司信息为目标，讲解一下详细的代码实现步骤。</p><h2 id="3-详细代码实现"><a href="#3-详细代码实现" class="headerlink" title="3. 详细代码实现"></a>3. 详细代码实现</h2><h3 id="3-1-read-html函数"><a href="#3-1-read-html函数" class="headerlink" title="3.1. read_html函数"></a>3.1. read_html函数</h3><p>先来了解一下<strong>read_html</strong>函数的api:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pandas.read_html(io, match=<span class="string">'.+'</span>, flavor=<span class="keyword">None</span>, header=<span class="keyword">None</span>, index_col=<span class="keyword">None</span>, skiprows=<span class="keyword">None</span>, attrs=<span class="keyword">None</span>, parse_dates=<span class="keyword">False</span>, tupleize_cols=<span class="keyword">None</span>, thousands=<span class="string">', '</span>, encoding=<span class="keyword">None</span>, decimal=<span class="string">'.'</span>, converters=<span class="keyword">None</span>, na_values=<span class="keyword">None</span>, keep_default_na=<span class="keyword">True</span>, displayed_only=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">常用的参数：</span><br><span class="line">io:可以是url、html文本、本地文件等；</span><br><span class="line">flavor：解析器；</span><br><span class="line">header：标题行；</span><br><span class="line">skiprows：跳过的行；</span><br><span class="line">attrs：属性，比如 attrs = &#123;<span class="string">'id'</span>: <span class="string">'table'</span>&#125;；</span><br><span class="line">parse_dates：解析日期</span><br><span class="line"></span><br><span class="line">注意：返回的结果是**DataFrame**组成的**list**。</span><br></pre></td></tr></table></figure></p><p>参考：</p><blockquote><p>1 <a href="http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html</a><br>2 <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html</a></p></blockquote><h3 id="3-2-分析网页url"><a href="#3-2-分析网页url" class="headerlink" title="3.2. 分析网页url"></a>3.2. 分析网页url</h3><p>首先，观察一下中商情报网第1页和第2页的网址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=1#QueryCondition</span><br><span class="line">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=2#QueryCondition</span><br></pre></td></tr></table></figure></p><p>可以发现，只有<strong>pageNum</strong>的值随着翻页而变化，所以基本可以断定pageNum=1代表第1页，pageNum=10代表第10页，以此类推。这样比较容易用for循环构造爬取的网址。<br>试着把<strong>#QueryCondition</strong>删除，看网页是否同样能够打开，经尝试发现网页依然能正常打开，因此在构造url时，可以使用这样的格式：<br><strong><a href="http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i" target="_blank" rel="noopener">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i</a></strong><br>再注意一下其他参数：<br><strong>a</strong>：表示A股，把a替换为<strong>h</strong>，表示<strong>港股</strong>；把a替换为<strong>xsb</strong>，则表示<strong>新三板</strong>。那么，在网址分页for循环外部再加一个for循环，就可以爬取这三个股市的股票了。  </p><h3 id="3-3-定义函数"><a href="#3-3-定义函数" class="headerlink" title="3.3. 定义函数"></a>3.3. 定义函数</h3><p>将整个爬取分为网页提取、内容解析、数据存储等步骤，依次建立相应的函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网页提取函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,   </span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># beatutiful soup解析然后提取表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line"></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(tbl)</span><br><span class="line"><span class="comment"># return tbl</span></span><br><span class="line"><span class="comment"># rename将表格15列的中文名改为英文名，便于存储到mysql及后期进行数据分析</span></span><br><span class="line"><span class="comment"># tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):   <span class="comment"># page表示提取页数</span></span><br><span class="line">html = get_one_page(i)</span><br><span class="line">parse_one_page(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)   <span class="comment">#共提取n页</span></span><br></pre></td></tr></table></figure></p><p>上面两个函数相比于快速抓取的方法代码要多一些，如果需要抓的表格很少或只需要抓一次，那么推荐快速抓取法。如果页数比较多，这种方法就更保险一些。解析函数用了BeautifulSoup和css选择器，这种方法定位提取表格所在的<strong>id为#myTable04</strong>的table代码段，更为准确。</p><h3 id="3-4-存储到MySQL"><a href="#3-4-存储到MySQL" class="headerlink" title="3.4. 存储到MySQL"></a>3.4. 存储到MySQL</h3><p>接下来，我们可以将结果保存到本地csv文件，也可以保存到MySQL数据库中。这里为了练习一下MySQL，因此选择保存到MySQL中。</p><p>首先，需要先在数据库建立存放数据的表格，这里命名为<strong>listed_company</strong>。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,   <span class="comment"># 本地服务器</span></span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,  <span class="comment"># 你的数据库密码</span></span><br><span class="line">port=<span class="number">3306</span>,          <span class="comment"># 默认端口</span></span><br><span class="line">charset = <span class="string">'utf8'</span>,</span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company2 (serial_number INT(30) NOT NULL,stock_code INT(30) ,stock_abbre VARCHAR(30) ,company_name VARCHAR(30) ,province VARCHAR(30) ,city VARCHAR(30) ,main_bussiness_income VARCHAR(30) ,net_profit VARCHAR(30) ,employees INT(30) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(30) ,financial_report VARCHAR(30) , industry_classification VARCHAR(255) ,industry_type VARCHAR(255) ,main_business VARCHAR(255) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">    <span class="comment"># listed_company是要在wade数据库中建立的表，用于存放数据</span></span><br><span class="line">    </span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line">generate_mysql()</span><br></pre></td></tr></table></figure></p><p>上述代码定义了generate_mysql()函数，用于在MySQL中wade数据库下生成一个listed_company的表。表格包含15个列字段。根据每列字段的属性，分别设置为INT整形（长度为30）、VARCHAR字符型(长度为30) 、DATETIME(0) 日期型等。<br>在Navicat中查看建立好之后的表格：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/33076915.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/97554452.jpg" alt=""></p><p>接下来就可以往这个表中写入数据，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="comment"># db = 'wade'表示存储到wade这个数据库中,root后面的*是密码</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># 因为要循环网页不断数据库写入内容，所以if_exists选择append，同时该表要有表头，parse_one_page（）方法中df.rename已设置</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure></p><p>以上就完成了单个页面的表格爬取和存储工作，接下来只要在main()函数进行for循环，就可以完成所有总共178页表格的爬取和存储，完整代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode  <span class="comment"># 编码 URL 字符串</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,</span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(tbl)</span></span><br><span class="line"><span class="keyword">return</span> tbl</span><br><span class="line"><span class="comment"># rename将中文名改为英文名，便于存储到mysql及后期进行数据分析</span></span><br><span class="line"><span class="comment"># tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,</span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,</span><br><span class="line">port=<span class="number">3306</span>,</span><br><span class="line">charset = <span class="string">'utf8'</span>,  </span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">    <span class="comment"># listed_company是要在wade数据库中建立的表，用于存放数据</span></span><br><span class="line">    </span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># df = pd.read_csv(df)</span></span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company2'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># append表示在原有表基础上增加，但该表要有表头</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    generate_mysql()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):  </span><br><span class="line">html = get_one_page(i)</span><br><span class="line">tbl = parse_one_page(html)</span><br><span class="line">write_to_sql(tbl)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)</span><br><span class="line"></span><br><span class="line">endtime = time.time()-start_time</span><br><span class="line">print(<span class="string">'程序运行了%.2f秒'</span> %endtime)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="comment"># from multiprocessing import Pool</span></span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment"># pool = Pool(4)</span></span><br><span class="line"><span class="comment"># pool.map(main, [i for i in range(1,178)])  #共有178页</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># endtime = time.time()-start_time</span></span><br><span class="line"><span class="comment"># print('程序运行了%.2f秒' %(time.time()-start_time))</span></span><br></pre></td></tr></table></figure></p><p>最终，A股所有3535家企业的信息已经爬取到mysql中，如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/63973864.jpg" alt=""></p><p>最后，需说明不是所有表格都可以用这种方法爬取，比如这个网站中的表格，表面是看起来是表格，但在html中不是前面的table格式，而是list列表格式。这种表格则不适用read_html爬取。得用其他的方法，比如selenium，以后再进行介绍。<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/3402980.jpg" alt=""></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;python爬虫第2篇&lt;/em&gt;&lt;/b&gt;&lt;br&gt;利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="pandas" scheme="https://www.makcyun.top/tags/pandas/"/>
    
      <category term="数据抓取" scheme="https://www.makcyun.top/tags/%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(1):多种方法爬取猫眼top100电影</title>
    <link href="https://www.makcyun.top/web_scraping_withpython1.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython1.html</id>
    <published>2018-08-20T11:18:14.973Z</published>
    <updated>2018-09-01T07:32:56.056Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>python爬虫第1篇</em></b><br>利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。</p><a id="more"></a>  <p><strong>摘要：</strong> 作为小白，<strong>爬虫可以说是入门python最快和最容易获得成就感的途径</strong>。因为初级爬虫的套路相对固定，常见的方法只有几种，比较好上手。最近，跟着崔庆才大佬的书：<em>python3网络爬虫开发实战</em> 学习爬虫。选取网页结构较为简单的猫眼top100电影为案例进行练习。 <strong>重点是用上述所说的4种方法提取出关键内容</strong>。一个问题采用不同的解决方法有助于拓展思维，通过不断练习就能够灵活运用。</p><blockquote><p><strong>本文知识点：</strong><br>Requsts 请求库的使用<br>beautiful+lxml两大解析库使用<br>正则表达式 、xpath、css选择器的使用  </p></blockquote><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/49818413.jpg" alt=""></p><h2 id="1-为什么爬取该网页？"><a href="#1-为什么爬取该网页？" class="headerlink" title="1. 为什么爬取该网页？"></a>1. 为什么爬取该网页？</h2><ul><li>比较懒，不想一页页地去翻100部电影的介绍，<strong>想在一个页面内进行总体浏览</strong>（比如在excel表格中）；</li></ul><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/28479553.jpg" alt=""></p><ul><li>想<strong>深入了解一些比较有意思的信息</strong>，比如：哪部电影的评分最高？哪位演员的作品数量最多？哪个国家/地区上榜的电影数量最多？哪一年上榜的电影作品最多等。这些信息在网页上是不那么容易能直接获得的，所以需要爬虫。</li></ul><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/66062822.jpg" alt=""></p><h2 id="2-爬虫目标"><a href="#2-爬虫目标" class="headerlink" title="2. 爬虫目标"></a>2. 爬虫目标</h2><ul><li>从网页中提取出top100电影的电影名称、封面图片、排名、评分、演员、上映国家/地区、评分等信息，并保存为csv文本文件。</li><li>根据爬取结果，进行简单的可视化分析。</li></ul><p>平台：windows7 + SublimeText3</p><h2 id="3-爬取步骤"><a href="#3-爬取步骤" class="headerlink" title="3. 爬取步骤"></a>3. 爬取步骤</h2><h3 id="3-1-网址URL分析"><a href="#3-1-网址URL分析" class="headerlink" title="3.1. 网址URL分析"></a>3.1. 网址URL分析</h3><p>首先，打开猫眼Top100的url网址： <a href="http://maoyan.com/board/4?offset=0" target="_blank" rel="noopener"><strong>http://maoyan.com/board/4?offset=0</strong></a>。页面非常简单，所包含的信息就是上述所说的爬虫目标。下拉页面到底部，点击第2页可以看到网址变为：<strong><a href="http://maoyan.com/board/4?offset=10" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10</a></strong>。因此，可以推断出url的变化规律：offset表示偏移，10代表一个页面的电影偏移数量，即：第一页电影是从0-10，第二页电影是从11-20。因此，获取全部100部电影，只需要构造出10个url，然后依次获取网页内容，再用不同的方法提取出所需内容就可以了。<br>下面，用requests方法获取第一个页面。</p><h3 id="3-2-Requests获取首页数据"><a href="#3-2-Requests获取首页数据" class="headerlink" title="3.2. Requests获取首页数据"></a>3.2. Requests获取首页数据</h3><p>先定义一个获取单个页面的函数：get_one_page()，传入url参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line">        <span class="comment"># 不加headers爬不了</span></span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># try-except语句捕获异常</span></span><br></pre></td></tr></table></figure><p>接下来在main()函数中设置url。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    print(html)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>运行上述程序后，首页的源代码就被爬取下来了。如下图所示：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/18415362.jpg" alt=""></p><p>接下来就需要从整个网页中提取出几项我们需要的内容，用到的方法就是上述所说的四种方法，下面分别进行说明。</p><h3 id="3-3-4种内容解析提取方法"><a href="#3-3-4种内容解析提取方法" class="headerlink" title="3.3. 4种内容解析提取方法"></a>3.3. 4种内容解析提取方法</h3><h4 id="3-3-1-正则表达式提取"><a href="#3-3-1-正则表达式提取" class="headerlink" title="3.3.1. 正则表达式提取"></a>3.3.1. 正则表达式提取</h4><p>第一种是利用<strong>正则表达式</strong>提取。<br>什么是正则表达式？ 下面这串看起来乱七八糟的符号就是正则表达式的语法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&apos;</span><br></pre></td></tr></table></figure><p>它是一种强大的字符串处理工具。之所以叫正则表达式，是因为它们可以识别正则字符串（regular string）。可以这么定义：“ 如果你给我的字符串符合规则，我就返回它”；“如果字符串不符合规则，我就忽略它”。通过requests抓取下来的网页是一堆大量的字符串，用它处理后便可提取出我们想要的内容。</p><p>如果还不了解它，可以参考下面的教程：</p><blockquote><p><a href="http://www.runoob.com/regexp/regexp-syntax.html" target="_blank" rel="noopener">http://www.runoob.com/regexp/regexp-syntax.html</a><br><a href="https://www.w3cschool.cn/regexp/zoxa1pq7.html" target="_blank" rel="noopener">https://www.w3cschool.cn/regexp/zoxa1pq7.html</a></p></blockquote><p><strong>正则表达式常用语法：</strong></p><style>table th:nth-of-type(1) {    width: 60px;}</style><table><thead><tr><th style="text-align:center">模式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">\w</td><td style="text-align:center">匹配字母数字及下划线</td></tr><tr><td style="text-align:center">\W</td><td style="text-align:center">匹配非字母数字及下划线</td></tr><tr><td style="text-align:center">\s</td><td style="text-align:center">匹配任意空白字符，等价于 [\t\n\r\f]</td></tr><tr><td style="text-align:center">\S</td><td style="text-align:center">匹配任意非空字符</td></tr><tr><td style="text-align:center">\d</td><td style="text-align:center">匹配任意数字，等价于 [0-9]</td></tr><tr><td style="text-align:center">\D</td><td style="text-align:center">匹配任意非数字</td></tr><tr><td style="text-align:center">\n</td><td style="text-align:center">匹配一个换行符</td></tr><tr><td style="text-align:center">\t</td><td style="text-align:center">匹配一个制表符</td></tr><tr><td style="text-align:center">^</td><td style="text-align:center">匹配字符串开始位置的字符</td></tr><tr><td style="text-align:center">$</td><td style="text-align:center">匹配字符串的末尾</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">匹配任意字符，除了换行符</td></tr><tr><td style="text-align:center">[…]</td><td style="text-align:center">用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’</td></tr><tr><td style="text-align:center">[^…]</td><td style="text-align:center">不在 [ ] 中的字符</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">匹配前面的字符、子表达式或括号里的字符 0 次或多次</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">同上，匹配至少一次</td></tr><tr><td style="text-align:center">?</td><td style="text-align:center">同上，匹配0到1次</td></tr><tr><td style="text-align:center">{n}</td><td style="text-align:center">匹配前面的字符、子表达式或括号里的字符 n 次</td></tr><tr><td style="text-align:center">{n, m}</td><td style="text-align:center">同上，匹配 m 到n 次（包含 m 或 n）</td></tr><tr><td style="text-align:center">( )</td><td style="text-align:center">匹配括号内的表达式，也表示一个组</td></tr></tbody></table><p>下面，开始提取关键内容。右键网页-检查-Network选项，选中左边第一个文件然后定位到电影信息的相应位置，如下图：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/84055565.jpg" alt=""></p><p>可以看到每部电影的相关信息都在<strong>dd</strong>这个节点之中。所以就可以从该节点运用正则进行提取。<br>第1个要提取的内容是电影的排名。它位于class=”board-index”的<strong>i</strong>节点内。不需要提取的内容用’.*?’替代，需要提取的数字排名用（）括起来，（）里面的数字表示为（\d+）。正则表达式可以写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;&apos;</span><br></pre></td></tr></table></figure><p>接着，第2个需要提取的是封面图片，图片网址位于img节点的’data-src’属性中，正则表达式可写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;data-src=&quot;(.*?)&quot;.*?&apos;</span><br></pre></td></tr></table></figure><p>第1和第2个正则之间的代码是不需要的，用’.*?’替代，所以这两部分合起来写就是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;</span><br></pre></td></tr></table></figure><p>同理，可以依次用正则写下主演、上映时间和评分等内容,完整的正则表达式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;</span><br></pre></td></tr></table></figure><p>正则表达式写好以后，可以定义一个页面解析提取方法：parse_one_page（），用来提取内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(</span><br><span class="line">        <span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>, re.S)</span><br><span class="line">    <span class="comment"># re.S表示匹配任意字符，如果不加，则无法匹配换行符</span></span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'index'</span>: item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(item[<span class="number">1</span>]),  <span class="comment"># 定义get_thumb()方法进一步处理网址</span></span><br><span class="line">            <span class="string">'name'</span>: item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'star'</span>: item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="comment"># 'time': item[4].strip()[5:],</span></span><br><span class="line">            <span class="comment"># 用两个方法分别提取time里的日期和地区</span></span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: item[<span class="number">5</span>].strip() + item[<span class="number">6</span>].strip()</span><br><span class="line">            <span class="comment"># 评分score由整数+小数两部分组成</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>re.S:</strong>匹配任意字符，如果不加，则无法匹配换行符；<br><strong>yield:</strong>使用yield的好处是作为生成器，可以遍历迭代，并且将数据整理形成字典，输出结果美观。具体用法可参考：<a href="https://blog.csdn.net/zhangpinghao/article/details/18716275；" target="_blank" rel="noopener">https://blog.csdn.net/zhangpinghao/article/details/18716275；</a><br><strong>.strip():</strong>用于去掉字符串中的空格。</p></blockquote><p>上面程序为了便于提取内容，又定义了3个方法：get_thumb（）、get_release_time（）和 get_release_area（）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取封面大图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_thumb</span><span class="params">(url)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)@.*?'</span>)</span><br><span class="line">    thumb = re.search(pattern, url)</span><br><span class="line">    <span class="keyword">return</span> thumb.group(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c</span></span><br><span class="line"><span class="comment"># 去掉@160w_220h_1e_1c就是大图    </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取上映时间函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_time</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)(\(|$)'</span>)</span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)  <span class="comment"># 返回匹配到的第一个括号(.*?)中结果即时间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家/地区函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_area</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'.*\((.*)\)'</span>)</span><br><span class="line">    <span class="comment"># $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?)</span></span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>‘r’：</strong>正则前面加上’r’ 是为了告诉编译器这个string是个raw string，不要转意’\’。当一个字符串使用了正则表达式后，最好在前面加上’r’；<br><strong>‘|’ ‘$’：</strong>  正则’|’表示或’，’$’表示匹配一行字符串的结尾；<br><strong>.group(1)</strong>：意思是返回search匹配的第一个括号中的结果，即(.*?)，gropup()则返回所有结果2013-12-18(，group(1)返回’（’。</p></blockquote><p>接下来，修改main()函数来输出爬取的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        print(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>if <strong> name</strong> == ‘_ _main__’:</strong>当.py文件被直接运行时，if <strong> name</strong> == ‘_ <em>main__’之下的代码块将被运行；当.py文件以模块形式被导入时，if <strong> name</strong> == ‘</em> _main__’之下的代码块不被运行。<br>参考：<a href="https://blog.csdn.net/yjk13703623757/article/details/77918633。" target="_blank" rel="noopener">https://blog.csdn.net/yjk13703623757/article/details/77918633。</a></p></blockquote><p>运行程序，就可成功地提取出所需内容，结果如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'1'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg'</span>, <span class="string">'name'</span>: <span class="string">'霸王别姬'</span>, <span class="string">'star'</span>: <span class="string">'张国荣,张丰毅,巩俐'</span>, <span class="string">'time'</span>: <span class="string">'1993-01-01'</span>, <span class="string">'area'</span>: <span class="string">'中国香港'</span>, <span class="string">'score'</span>: <span class="string">'9.6'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'2'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/54617769d96807e4d81804284ffe2a27239007.jpg'</span>, <span class="string">'name'</span>: <span class="string">'罗马假日'</span>, <span class="string">'star'</span>: <span class="string">'格利高里·派克,奥黛丽·赫本,埃迪·艾伯特'</span>, <span class="string">'time'</span>: <span class="string">'1953-09-02'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.1'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'3'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg'</span>, <span class="string">'name'</span>: <span class="string">'肖申克的救赎'</span>, <span class="string">'star'</span>: <span class="string">'蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿'</span>, <span class="string">'time'</span>: <span class="string">'1994-10-14'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.5'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'4'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/e55ec5d18ccc83ba7db68caae54f165f95924.jpg'</span>, <span class="string">'name'</span>: <span class="string">'这个杀手不太冷'</span>, <span class="string">'star'</span>: <span class="string">'让·雷诺,加里·奥德曼,娜塔莉·波特曼'</span>, <span class="string">'time'</span>: <span class="string">'1994-09-14'</span>, <span class="string">'area'</span>: <span class="string">'法国'</span>, <span class="string">'score'</span>: <span class="string">'9.5'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'5'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p1.meituan.net/movie/f5a924f362f050881f2b8f82e852747c118515.jpg'</span>, <span class="string">'name'</span>: <span class="string">'教父'</span>, <span class="string">'star'</span>: <span class="string">'马龙·白兰度,阿尔·帕西诺,詹姆斯·肯恩'</span>, <span class="string">'time'</span>: <span class="string">'1972-03-24'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.3'</span>&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">[Finished <span class="keyword">in</span> <span class="number">1.9</span>s]</span><br></pre></td></tr></table></figure><p>以上是第1种提取方法，如果还不习惯正则表达式这种复杂的语法，可以试试下面的第2种方法。</p><h4 id="3-3-2-lxml结合xpath提取"><a href="#3-3-2-lxml结合xpath提取" class="headerlink" title="3.3.2. lxml结合xpath提取"></a>3.3.2. lxml结合xpath提取</h4><p>该方法需要用到<strong>lxml</strong>这款解析利器，同时搭配xpath语法，利用它的的路径选择表达式，来高效提取所需内容。lxml包为第三方包，需要自行安装。如果对xpath的语法还不太熟悉，可参考下面的教程：<br><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_syntax.asp</a></p><p><strong>xpath常用的规则</strong>    </p><table><thead><tr><th style="text-align:center">表达式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">nodename</td><td style="text-align:center">选取此节点的所有子节点</td></tr><tr><td style="text-align:center">/</td><td style="text-align:center">从当前节点选取直接子节点</td></tr><tr><td style="text-align:center">//</td><td style="text-align:center">从当前节点选取子孙节点</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">选取当前节点</td></tr><tr><td style="text-align:center">..</td><td style="text-align:center">选取当前节点的父节点</td></tr><tr><td style="text-align:center">@</td><td style="text-align:center">选取属性</td></tr></tbody></table><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span> <span class="attr">id</span>=<span class="string">"app"</span> <span class="attr">class</span>=<span class="string">"page-board/index"</span> &gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"main"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"update-time"</span>&gt;</span>2018-08-18<span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"has-fresh-text"</span>&gt;</span>已更新<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"board-content"</span>&gt;</span>榜单规则：将猫眼电影库中的经典影片，按照评分和评分人数从高到低综合排序取前100名，每天上午10点更新。相关数据来源于“猫眼电影库”。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dl</span> <span class="attr">class</span>=<span class="string">"board-wrapper"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">dd</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"board-index board-index-1"</span>&gt;</span>1<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/films/1203"</span> <span class="attr">title</span>=<span class="string">"霸王别姬"</span> <span class="attr">class</span>=<span class="string">"image-link"</span> <span class="attr">data-act</span>=<span class="string">"boarditem-click"</span> <span class="attr">data-val</span>=<span class="string">"&#123;movieId:1203&#125;"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"//ms0.meituan.net/mywww/image/loading_2.e3d934bf.png"</span> <span class="attr">alt</span>=<span class="string">""</span> <span class="attr">class</span>=<span class="string">"poster-default"</span> /&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-src</span>=<span class="string">"http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c"</span> <span class="attr">alt</span>=<span class="string">"霸王别姬"</span> <span class="attr">class</span>=<span class="string">"board-img"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"board-item-main"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"board-item-content"</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"movie-item-info"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"name"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/films/1203"</span> <span class="attr">title</span>=<span class="string">"霸王别姬"</span> <span class="attr">data-act</span>=<span class="string">"boarditem-click"</span> <span class="attr">data-val</span>=<span class="string">"&#123;movieId:1203&#125;"</span>&gt;</span>霸王别姬<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"star"</span>&gt;</span></span><br><span class="line">                主演：张国荣,张丰毅,巩俐</span><br><span class="line">        <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"releasetime"</span>&gt;</span>上映时间：1993-01-01(中国香港)<span class="tag">&lt;/<span class="name">p</span>&gt;</span>    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"movie-item-number score-num"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"score"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"integer"</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fraction"</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                <span class="tag">&lt;/<span class="name">dd</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">dd</span>&gt;</span></span><br></pre></td></tr></table></figure><p>根据截取的部分html网页，先来提取第1个电影排名信息，有两种方法。<br><strong>第一种：</strong>直接复制。<br>右键-Copy-Copy Xpath，得到xpath路径为：<strong>//*[@id=”app”]/div/div/div[1]/dl/dd[1]/i</strong>,为了能够提取到页面所有的排名信息，需进一步修改为：<strong>//*[@id=”app”]/div/div/div[1]/dl/dd/i/text()</strong>，如果想要再精简一点，可以省去中间部分绝对路径’/‘然后用相对路径’//‘代替，最后进一步修改为：<strong>//*[@id=”app”]//div//dd/i/text()</strong>。<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/840042.jpg" alt=""></p><p><strong>第二种：</strong>观察网页结构自己写。<br>首先注意到<strong>id = app</strong>的div节点，因为在整个网页结构id是唯一的不会有第二个相同的，所有可以将该div节点作为xpath语法的起点，然后往下观察分别是3级div节点，可以省略写为：<strong>//div</strong>,再往下分别是是两个并列的<strong>p</strong>节点、<strong>dl</strong>节点、<strong>dd</strong>节点和最后的<strong>i</strong>节点文本。中间可以随意省略，只要保证该路径能够选择到唯一的文本值<strong>‘1’</strong>即可，例如省去p和dl节点，只保留后面的节点。这样，完整路径可以为：<strong>//*[@id=”app”]//div//dd/i/text()</strong>，和上式一样。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/87341266.jpg" alt=""></p><p>根据上述思路，可以写下其他内容的xpath路径。观察到路径的前一部分：<strong>//*[@id=”app”]//div//dd</strong>都是一样的，从后面才开始不同，因此为了能够精简代码，将前部分路径赋值为一个变量items，最终提取的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2 用lxml结合xpath提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page2</span><span class="params">(html)</span>:</span></span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'//*[@id="app"]//div//dd'</span>)</span><br><span class="line">    <span class="comment"># 完整的是//*[@id="app"]/div/div/div[1]/dl/dd</span></span><br><span class="line">    <span class="comment"># print(type(items))</span></span><br><span class="line">    <span class="comment"># *代表匹配所有节点，@表示属性</span></span><br><span class="line">    <span class="comment"># 第一个电影是dd[1],要提取页面所有电影则去掉[1]</span></span><br><span class="line">    <span class="comment"># xpath://*[@id="app"]/div/div/div[1]/dl/dd[1]    </span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>: item.xpath(<span class="string">'./i/text()'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="comment">#./i/text()前面的点表示从items节点开始</span></span><br><span class="line">            <span class="comment">#/text()提取文本</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(str(item.xpath(<span class="string">'./a/img[2]/@data-src'</span>)[<span class="number">0</span>].strip())),</span><br><span class="line">            <span class="comment"># 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。</span></span><br><span class="line">            <span class="string">'name'</span>: item.xpath(<span class="string">'./a/@title'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'star'</span>: item.xpath(<span class="string">'.//p[@class = "star"]/text()'</span>)[<span class="number">0</span>].strip(),</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span> : item.xpath(<span class="string">'.//p[@class = "score"]/i[1]/text()'</span>)[<span class="number">0</span>] + \</span><br><span class="line">            item.xpath(<span class="string">'.//p[@class = "score"]/i[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>[0]：</strong>xpath后面添加了[0]是因为返回的是只有1个字符串的list，添加[0]是将list提取为字符串，使其简洁；<br><strong>Network：</strong>要在最原始的Network选项卡中定位，而不是Elements中，不然提取不到相关内容；<br><strong>class属性：</strong>p[@class = “star”]/text()表示提取class属性为”star”的p节点的文本值；<br><strong>提取属性值：</strong>img[2]/@data-src’：提取img节点的data-src属性值，属性值后面无需添加’/text()’</p></blockquote><p>运行程序，就可成功地提取出所需内容，结果和第一种方法一样。</p><p>以上是第2种提取方法，如果也不太习惯xpath语法，可以试试下面的第3种方法。</p><h4 id="3-3-3-Beautiful-Soup-css选择器"><a href="#3-3-3-Beautiful-Soup-css选择器" class="headerlink" title="3.3.3. Beautiful Soup + css选择器"></a>3.3.3. Beautiful Soup + css选择器</h4><p>Beautiful Soup 同lxml一样，是一个非常强大的python解析库，可以从HTML或XML文件中提取效率非常高。关于它的用法，可参考下面的教程：<br><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></p><p>css选择器选是一种模式，用于选择需要添加样式的元素，使用它的语法同样能够快速定位到所需节点，然后提取相应内容。使用方法可参考下面的教程：<br><a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a></p><p><strong>css选择器常用的规则</strong>  </p><style>table th:nth-of-type(1) {    width: 30%;}</style><table><thead><tr><th style="text-align:center">选择器</th><th style="text-align:center">例子</th><th style="text-align:center">例子描述</th></tr></thead><tbody><tr><td style="text-align:center">.class</td><td style="text-align:center">.intro</td><td style="text-align:center">选择 class=”intro” 的所有元素。</td></tr><tr><td style="text-align:center">#id</td><td style="text-align:center">#firstname</td><td style="text-align:center">选择 id=”firstname” 的所有元素。</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">*</td><td style="text-align:center">选择所有元素。</td></tr><tr><td style="text-align:center">element</td><td style="text-align:center">p</td><td style="text-align:center">选择所有p元素。</td></tr><tr><td style="text-align:center">element,element</td><td style="text-align:center">div,p</td><td style="text-align:center">选择所有div元素和所有p元素。</td></tr><tr><td style="text-align:center">element?element</td><td style="text-align:center">div p</td><td style="text-align:center">选择div元素内部的所有p元素。</td></tr><tr><td style="text-align:center">element&gt;element</td><td style="text-align:center">div&gt;p</td><td style="text-align:center">选择父元素为div元素的所有p元素。</td></tr><tr><td style="text-align:center">element+element</td><td style="text-align:center">div+p</td><td style="text-align:center">选择紧接在div元素之后的所有p元素。</td></tr><tr><td style="text-align:center">[attribute]</td><td style="text-align:center">[target]</td><td style="text-align:center">选择带有 target 属性所有元素。</td></tr><tr><td style="text-align:center">[attribute=value]</td><td style="text-align:center">[target=_blank]</td><td style="text-align:center">选择 target=”_blank” 的所有元素。</td></tr></tbody></table><p>下面就利用这种方法进行提取：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3 用beautifulsoup + css选择器提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page3</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    <span class="comment"># print(type(content))</span></span><br><span class="line">    <span class="comment"># print('------------')</span></span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.select(<span class="string">'dd i.board-index'</span>)[item].string,</span><br><span class="line">            <span class="comment"># iclass节点完整地为'board-index board-index-1',写board-index即可</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(soup.select(<span class="string">'a &gt; img.board-img'</span>)[item][<span class="string">"data-src"</span>]),</span><br><span class="line">            <span class="comment"># 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值</span></span><br><span class="line"></span><br><span class="line">            <span class="string">'name'</span>: soup.select(<span class="string">'.name a'</span>)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.select(<span class="string">'.star'</span>)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: soup.select(<span class="string">'.integer'</span>)[item].string + soup.select(<span class="string">'.fraction'</span>)[item].string</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></p><p>运行上述程序，结果同第1种方法一样。</p><h4 id="3-3-4-Beautiful-Soup-find-all函数提取"><a href="#3-3-4-Beautiful-Soup-find-all函数提取" class="headerlink" title="3.3.4. Beautiful Soup + find_all函数提取"></a>3.3.4. Beautiful Soup + find_all函数提取</h4><p>Beautifulsoup除了和css选择器搭配，还可以直接用它自带的find_all函数进行提取。<br><strong>find_all</strong>，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。<br>它的API如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find_all(name , attrs , recursive , text , **kwargs)</span><br></pre></td></tr></table></figure><blockquote><p><strong>常用的语法规则如下：</strong><br>soup.find_all(name=’ul’)： 查找所有<strong>ul</strong>节点，ul节点内还可以嵌套；<br>li.string和li.get_text()：都是获取<strong>li</strong>节点的文本，但推荐使用后者；<br>soup.find_all(attrs={‘id’: ‘list-1’}))：传入 attrs 参数，参数的类型是字典类型，表示查询 <strong>id</strong> 为 <strong>list-1</strong> 的节点；<br>常用的属性比如 id、class 等，可以省略attrs采用更简洁的形式，例如：<br>soup.find_all(id=’list-1’)<br>soup.find_all(class_=’element’)</p></blockquote><p>根据上述常用语法，可以提取网页中所需内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page4</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.find_all(class_=<span class="string">'board-index'</span>)[item].string,</span><br><span class="line">            <span class="string">'thumb'</span>: soup.find_all(class_ = <span class="string">'board-img'</span>)[item].attrs[<span class="string">'data-src'</span>],</span><br><span class="line">            <span class="comment"># 用.get('data-src')获取图片src链接，或者用attrs['data-src']</span></span><br><span class="line">            <span class="string">'name'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span> : <span class="string">'name'</span>&#125;)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'star'</span>&#125;)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>:soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'integer'</span>&#125;)[item].string.strip() + soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'fraction'</span>&#125;)[item].string.strip()</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>以上就是4种不同的内容提取方法。</p><h3 id="3-4-数据存储"><a href="#3-4-数据存储" class="headerlink" title="3.4. 数据存储"></a>3.4. 数据存储</h3><p>上述输出的结果为字典格式，可利用csv包的DictWriter函数将字典格式数据存储到csv文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储到csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file3</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼top100.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 'a'为追加模式（添加）</span></span><br><span class="line">        <span class="comment"># utf_8_sig格式导出csv不乱码 </span></span><br><span class="line">        fieldnames = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]</span><br><span class="line">        w = csv.DictWriter(f,fieldnames = fieldnames)</span><br><span class="line">        <span class="comment"># w.writeheader()</span></span><br><span class="line">        w.writerow(item)</span><br></pre></td></tr></table></figure><p>然后修改一下main()方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line">        write_to_csv(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/66910595.jpg" alt=""></p><p>再将封面的图片下载下来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_thumb</span><span class="params">(name, url,num)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'封面图/'</span> + name + <span class="string">'.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">            print(<span class="string">'第%s部电影封面下载完毕'</span> %num)</span><br><span class="line">            print(<span class="string">'------'</span>)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     <span class="comment"># 不能是w，否则会报错，因为图片是二进制数据所以要用wb</span></span><br></pre></td></tr></table></figure><h3 id="3-5-分页爬取"><a href="#3-5-分页爬取" class="headerlink" title="3.5. 分页爬取"></a>3.5. 分页爬取</h3><p>上面完成了一页电影数据的提取，接下来还需提取剩下9页共90部电影的数据。对网址进行遍历，给网址传入一个offset参数即可，修改如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset='</span> + str(offset)</span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line">        write_to_csv(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        main(offset = i*<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>这样就完成了所有电影的爬取。结果如下：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/28479553.jpg" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/34117279.jpg" alt=""></p><h2 id="4-可视化分析"><a href="#4-可视化分析" class="headerlink" title="4. 可视化分析"></a>4. 可视化分析</h2><p>俗话说“文不如表，表不如图”。下面根据excel的数据结果，进行简单的数据可视化分析，并用图表呈现。</p><h3 id="4-1-电影评分最高top10"><a href="#4-1-电影评分最高top10" class="headerlink" title="4.1. 电影评分最高top10"></a>4.1. 电影评分最高top10</h3><p>首先，想看一看评分最高的前10部电影是哪些？</p><p>程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment">#用于修改x轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)   <span class="comment">#默认绘图风格很难看，替换为好看的ggplot风格</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))   <span class="comment">#设置图片大小</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment">#设置图表title、text标注的颜色</span></span><br><span class="line"></span><br><span class="line">columns = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]  <span class="comment">#设置表头</span></span><br><span class="line">df = pd.read_csv(<span class="string">'maoyan_top100.csv'</span>,encoding = <span class="string">"utf-8"</span>,header = <span class="keyword">None</span>,names =columns,index_col = <span class="string">'index'</span>)  <span class="comment">#打开表格</span></span><br><span class="line"><span class="comment"># index_col = 'index' 将索引设为index</span></span><br><span class="line"></span><br><span class="line">df_score = df.sort_values(<span class="string">'score'</span>,ascending = <span class="keyword">False</span>)  <span class="comment">#按得分降序排列</span></span><br><span class="line"></span><br><span class="line">name1 = df_score.name[:<span class="number">10</span>]      <span class="comment">#x轴坐标</span></span><br><span class="line">score1 = df_score.score[:<span class="number">10</span>]    <span class="comment">#y轴坐标  </span></span><br><span class="line">plt.bar(range(<span class="number">10</span>),score1,tick_label = name1)  <span class="comment">#绘制条形图，用range()能搞保持x轴正确顺序</span></span><br><span class="line">plt.ylim ((<span class="number">9</span>,<span class="number">9.8</span>))  <span class="comment">#设置纵坐标轴范围</span></span><br><span class="line">plt.title(<span class="string">'电影评分最高top10'</span>,color = colors1) <span class="comment">#标题</span></span><br><span class="line">plt.xlabel(<span class="string">'电影名称'</span>)      <span class="comment">#x轴标题</span></span><br><span class="line">plt.ylabel(<span class="string">'评分'</span>)          <span class="comment">#y轴标题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个条形图添加数值标签</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(score1)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.01</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line"></span><br><span class="line">pl.xticks(rotation=<span class="number">270</span>)   <span class="comment">#x轴名称太长发生重叠，旋转为纵向显示</span></span><br><span class="line">plt.tight_layout()    <span class="comment">#自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line"><span class="comment"># plt.savefig('电影评分最高top10.png')   #保存图片</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/80083042.jpg" alt=""><br>可以看到：排名最高的分别是两部国产片”霸王别姬”和”大话西游”，其他还包括”肖申克的救赎”、”教父”等。<br>嗯，还好基本上都看过。</p><h3 id="4-2-各国家的电影数量比较"><a href="#4-2-各国家的电影数量比较" class="headerlink" title="4.2. 各国家的电影数量比较"></a>4.2. 各国家的电影数量比较</h3><p>然后，想看看100部电影都是来自哪些国家？<br>程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">area_count = df.groupby(by = <span class="string">'area'</span>).area.count().sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图方法1</span></span><br><span class="line">area_count.plot.bar(color = <span class="string">'#4652B1'</span>)  <span class="comment">#设置为蓝紫色</span></span><br><span class="line">pl.xticks(rotation=<span class="number">0</span>)   <span class="comment">#x轴名称太长重叠，旋转为纵向</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图方法2</span></span><br><span class="line"><span class="comment"># plt.bar(range(11),area_count.values,tick_label = area_count.index)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(area_count.values)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.5</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line">plt.title(<span class="string">'各国/地区电影数量排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'国家/地区'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('各国(地区)电影数量排名.png')</span></span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-21/57684234.jpg" alt=""><br>可以看到，除去网站自身没有显示国家的电影以外，上榜电影被10个国家/地区”承包”了。其中，美国以30部电影的绝对优势占据第1名，其次是8部的日本，韩国第3，居然有7部上榜。<br>不得不说的是香港有5部，而内地一部都没有。。。</p><h3 id="4-3-电影作品数量集中的年份"><a href="#4-3-电影作品数量集中的年份" class="headerlink" title="4.3. 电影作品数量集中的年份"></a>4.3. 电影作品数量集中的年份</h3><p>接下来站在漫长的百年电影史的时间角度上，分析一下哪些年份”贡献了”最多的电影数量，也可以说是”电影大年”。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从日期中提取年份</span></span><br><span class="line">df[<span class="string">'year'</span>] = df[<span class="string">'time'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">'/'</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># print(df.info())</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计各年上映的电影数量</span></span><br><span class="line">grouped_year = df.groupby(<span class="string">'year'</span>)</span><br><span class="line">grouped_year_amount = grouped_year.year.count()</span><br><span class="line">top_year = grouped_year_amount.sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">top_year.plot(kind = <span class="string">'bar'</span>,color = <span class="string">'orangered'</span>) <span class="comment">#颜色设置为橙红色</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(top_year.values)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.1</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line">plt.title(<span class="string">'电影数量年份排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'年份(年)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line"><span class="comment"># plt.savefig('电影数量年份排名.png')</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/32342735.jpg" alt=""></p><p>可以看到，100部电影来自37个年份。其中2011年上榜电影数量最多，达到9部；其次是前一年的7部。回忆一下，那会儿正是上大学的头两年，可怎么感觉除了阿凡达之外，没有什么其他有印象的电影了。。。<br>另外，网上传的号称”电影史奇迹年”的1994年仅排名第6。这让我进一步对猫眼榜单的权威性产生了质疑。<br>再往后看，发现遥远的1939和1940年也有电影上榜。那会儿应该还是黑白电影时代吧，看来电影的口碑好坏跟外在的技术没有绝对的关系，质量才是王道。</p><h3 id="4-4-拥有电影作品数量最多的演员"><a href="#4-4-拥有电影作品数量最多的演员" class="headerlink" title="4.4 拥有电影作品数量最多的演员"></a>4.4 拥有电影作品数量最多的演员</h3><p>最后，看看前100部电影中哪些演员的作品数量最多。<br>程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中</span></span><br><span class="line">starlist = []</span><br><span class="line">star_total = df.star</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df.star.str.replace(<span class="string">' '</span>,<span class="string">''</span>).str.split(<span class="string">','</span>):</span><br><span class="line">    starlist.extend(i)  </span><br><span class="line"><span class="comment"># print(starlist)</span></span><br><span class="line"><span class="comment"># print(len(starlist))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set去除重复的演员名</span></span><br><span class="line">starall = set(starlist)</span><br><span class="line"><span class="comment"># print(starall)</span></span><br><span class="line"><span class="comment"># print(len(starall))</span></span><br><span class="line"></span><br><span class="line">starall2 = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> starall:</span><br><span class="line">    <span class="keyword">if</span> starlist.count(i)&gt;<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 筛选出电影数量超过1部的演员</span></span><br><span class="line">        starall2[i] = starlist.count(i)</span><br><span class="line"></span><br><span class="line">starall2 = sorted(starall2.items(),key = <span class="keyword">lambda</span> starlist:starlist[<span class="number">1</span>] ,reverse = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">starall2 = dict(starall2[:<span class="number">10</span>])  <span class="comment">#将元组转为字典格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">x_star = list(starall2.keys())      <span class="comment">#x轴坐标</span></span><br><span class="line">y_star = list(starall2.values())    <span class="comment">#y轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.bar(range(<span class="number">10</span>),y_star,tick_label = x_star)</span><br><span class="line">pl.xticks(rotation = <span class="number">270</span>)</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(y_star):</span><br><span class="line">    plt.text(x,y+<span class="number">0.1</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'演员电影作品数量排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'演员'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()    </span><br><span class="line"><span class="comment"># plt.savefig('演员电影作品数量排名.png')</span></span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/66062822.jpg" alt=""></p><p>张国荣排在了第一位，这是之前没有猜到的。其次是梁朝伟和星爷，再之后是布拉德·皮特。惊奇地发现，前十名影星中，香港影星居然占了6位。有点严重怀疑这是不是香港版的top100电影。。。</p><p>对张国荣以7部影片的巨大优势雄霸榜单第一位感到好奇，想看看是哪7部电影。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'star1'</span>] = df[<span class="string">'star'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">','</span>)[<span class="number">0</span>])  <span class="comment">#提取1号演员</span></span><br><span class="line">df[<span class="string">'star2'</span>] = df[<span class="string">'star'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">','</span>)[<span class="number">1</span>])  <span class="comment">#提取2号演员</span></span><br><span class="line">star_most = df[(df.star1 == <span class="string">'张国荣'</span>) | (df.star2 == <span class="string">'张国荣'</span>)][[<span class="string">'star'</span>,<span class="string">'name'</span>]].reset_index(<span class="string">'index'</span>)</span><br><span class="line"><span class="comment"># |表示两个条件或查询，之后重置索引</span></span><br><span class="line">print(star_most)</span><br></pre></td></tr></table></figure></p><p>可以看到包括排名第1的”霸王别姬”、第17名的”春光乍泄”、第27名的”射雕英雄传之东成西就”等。<br>突然发现，好像只看过”英雄本色”。。。有时间，去看看他其他的作品。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">     index        star              name</span><br><span class="line">0      1   张国荣,张丰毅,巩俐        霸王别姬</span><br><span class="line">1     17   张国荣,梁朝伟,张震        春光乍泄</span><br><span class="line">2     27  张国荣,梁朝伟,张学友  射雕英雄传之东成西就</span><br><span class="line">3     37  张国荣,梁朝伟,刘嘉玲        东邪西毒</span><br><span class="line">4     70   张国荣,王祖贤,午马        倩女幽魂</span><br><span class="line">5     99  张国荣,张曼玉,刘德华        阿飞正传</span><br><span class="line">6    100   狄龙,张国荣,周润发        英雄本色</span><br></pre></td></tr></table></figure><p>由于数据量有限，故仅作了上述简要的分析。</p><h2 id="5-完整程序"><a href="#5-完整程序" class="headerlink" title="5. 完整程序"></a>5. 完整程序</h2><p>最后，将前面爬虫的所有代码整理一下，完整的代码如下：<br>一、爬虫部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line">        <span class="comment"># 不加headers爬不了</span></span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 用正则提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(</span><br><span class="line">        <span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>, re.S)</span><br><span class="line">    <span class="comment"># re.S表示匹配任意字符，如果不加.无法匹配换行符</span></span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'index'</span>: item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(item[<span class="number">1</span>]),</span><br><span class="line">            <span class="string">'name'</span>: item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'star'</span>: item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="comment"># 'time': item[4].strip()[5:],</span></span><br><span class="line">            <span class="comment"># 用函数分别提取time里的日期和地区</span></span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: item[<span class="number">5</span>].strip() + item[<span class="number">6</span>].strip()</span><br><span class="line">        &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 2 用lxml结合xpath提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page2</span><span class="params">(html)</span>:</span></span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'//*[@id="app"]//div//dd'</span>)</span><br><span class="line">    <span class="comment"># 完整的是//*[@id="app"]/div/div/div[1]/dl/dd</span></span><br><span class="line">    <span class="comment"># print(type(items))</span></span><br><span class="line">    <span class="comment"># *代表匹配所有节点，@表示属性</span></span><br><span class="line">    <span class="comment"># 第一个电影是dd[1],要提取页面所有电影则去掉[1]</span></span><br><span class="line">    <span class="comment"># xpath://*[@id="app"]/div/div/div[1]/dl/dd[1]</span></span><br><span class="line">    <span class="comment"># lst = []</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>: item.xpath(<span class="string">'./i/text()'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="comment">#./i/text()前面的点表示从items节点开始</span></span><br><span class="line">            <span class="comment">#/text()提取文本</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(str(item.xpath(<span class="string">'./a/img[2]/@data-src'</span>)[<span class="number">0</span>].strip())),</span><br><span class="line">            <span class="comment"># 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。</span></span><br><span class="line">            <span class="string">'name'</span>: item.xpath(<span class="string">'./a/@title'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'star'</span>: item.xpath(<span class="string">'.//p[@class = "star"]/text()'</span>)[<span class="number">0</span>].strip(),</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span> : item.xpath(<span class="string">'.//p[@class = "score"]/i[1]/text()'</span>)[<span class="number">0</span>] + \</span><br><span class="line">            item.xpath(<span class="string">'.//p[@class = "score"]/i[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 3 用beautifulsoup + css选择器提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page3</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    <span class="comment"># print(type(content))</span></span><br><span class="line">    <span class="comment"># print('------------')</span></span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.select(<span class="string">'dd i.board-index'</span>)[item].string,</span><br><span class="line">            <span class="comment"># iclass节点完整地为'board-index board-index-1',写board-inde即可</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(soup.select(<span class="string">'a &gt; img.board-img'</span>)[item][<span class="string">"data-src"</span>]),</span><br><span class="line">            <span class="comment"># 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值</span></span><br><span class="line"></span><br><span class="line">            <span class="string">'name'</span>: soup.select(<span class="string">'.name a'</span>)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.select(<span class="string">'.star'</span>)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: soup.select(<span class="string">'.integer'</span>)[item].string + soup.select(<span class="string">'.fraction'</span>)[item].string</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 用beautifulsoup + find_all提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page4</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.find_all(class_=<span class="string">'board-index'</span>)[item].string,</span><br><span class="line">            <span class="string">'thumb'</span>: soup.find_all(class_ = <span class="string">'board-img'</span>)[item].attrs[<span class="string">'data-src'</span>],</span><br><span class="line">            <span class="comment"># 用.get('data-src')获取图片src链接，或者用attrs['data-src']</span></span><br><span class="line">            <span class="string">'name'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span> : <span class="string">'name'</span>&#125;)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'star'</span>&#125;)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>:soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'integer'</span>&#125;)[item].string.strip() + soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'fraction'</span>&#125;)[item].string.strip()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取时间函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_time</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)(\(|$)'</span>)</span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)  <span class="comment"># 返回匹配到的第一个括号(.*?)中结果即时间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家/地区函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_area</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'.*\((.*)\)'</span>)</span><br><span class="line">    <span class="comment"># $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?)</span></span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取封面大图</span></span><br><span class="line"><span class="comment"># http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c</span></span><br><span class="line"><span class="comment"># 去掉@160w_220h_1e_1c就是大图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_thumb</span><span class="params">(url)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)@.*?'</span>)</span><br><span class="line">    thumb = re.search(pattern, url)</span><br><span class="line">    <span class="keyword">return</span> thumb.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据存储到csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file3</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼top100.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 'a'为追加模式（添加）</span></span><br><span class="line">        <span class="comment"># utf_8_sig格式导出csv不乱码 </span></span><br><span class="line">        fieldnames = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]</span><br><span class="line">        w = csv.DictWriter(f,fieldnames = fieldnames)</span><br><span class="line">        <span class="comment"># w.writeheader()</span></span><br><span class="line">        w.writerow(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 封面下载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_thumb</span><span class="params">(name, url,num)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'封面图/'</span> + name + <span class="string">'.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">            print(<span class="string">'第%s部电影封面下载完毕'</span> %num)</span><br><span class="line">            print(<span class="string">'------'</span>)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     <span class="comment"># 存储格式是wb,因为图片是二进制数格式，不能用w，否则会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset='</span> + str(offset)</span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    <span class="comment"># print(html)</span></span><br><span class="line">    <span class="comment"># parse_one_page2(html)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  <span class="comment"># 切换内容提取方法</span></span><br><span class="line">        print(item)</span><br><span class="line">        write_to_file(item)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下载封面图</span></span><br><span class="line">        download_thumb(item[<span class="string">'name'</span>], item[<span class="string">'thumb'</span>],item[<span class="string">'index'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment">#     for i in range(10):</span></span><br><span class="line"><span class="comment">#         main(i * 10)</span></span><br><span class="line">        <span class="comment"># time.sleep(0.5)</span></span><br><span class="line">        <span class="comment"># 猫眼增加了反爬虫，设置0.5s的延迟时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 使用多进程提升抓取效率</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    pool.map(main, [i * <span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure></p><p>二、可视化部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 可视化分析</span></span><br><span class="line"><span class="comment"># -------------------------------</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment"># 用于修改x轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)  <span class="comment"># 默认绘图风格很难看，替换为好看的ggplot风格</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))  <span class="comment"># 设置图片大小</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment"># 设置图表title、text标注的颜色</span></span><br><span class="line">columns = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]  <span class="comment"># 设置表头</span></span><br><span class="line">df = pd.read_csv(<span class="string">'maoyan_top100.csv'</span>, encoding=<span class="string">"utf-8"</span>,</span><br><span class="line">                 header=<span class="keyword">None</span>, names=columns, index_col=<span class="string">'index'</span>)  <span class="comment"># 打开表格</span></span><br><span class="line"><span class="comment"># index_col = 'index' 将索引设为index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1电影评分最高top10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_1</span><span class="params">()</span>:</span></span><br><span class="line">    df_score = df.sort_values(<span class="string">'score'</span>, ascending=<span class="keyword">False</span>)  <span class="comment"># 按得分降序排列</span></span><br><span class="line"></span><br><span class="line">    name1 = df_score.name[:<span class="number">10</span>]  <span class="comment"># x轴坐标</span></span><br><span class="line">    score1 = df_score.score[:<span class="number">10</span>]  <span class="comment"># y轴坐标</span></span><br><span class="line">    plt.bar(range(<span class="number">10</span>), score1, tick_label=name1)  <span class="comment"># 绘制条形图，用range()能搞保持x轴正确顺序</span></span><br><span class="line">    plt.ylim((<span class="number">9</span>, <span class="number">9.8</span>))  <span class="comment"># 设置纵坐标轴范围</span></span><br><span class="line">    plt.title(<span class="string">'电影评分最高top10'</span>, color=colors1)  <span class="comment"># 标题</span></span><br><span class="line">    plt.xlabel(<span class="string">'电影名称'</span>)  <span class="comment"># x轴标题</span></span><br><span class="line">    plt.ylabel(<span class="string">'评分'</span>)  <span class="comment"># y轴标题</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为每个条形图添加数值标签</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(score1)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.01</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line"></span><br><span class="line">    pl.xticks(rotation=<span class="number">270</span>)  <span class="comment"># x轴名称太长发生重叠，旋转为纵向显示</span></span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line">    <span class="comment"># plt.savefig('电影评分最高top10.png')   #保存图片</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 2各国家的电影数量比较</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_2</span><span class="params">()</span>:</span></span><br><span class="line">    area_count = df.groupby(</span><br><span class="line">        by=<span class="string">'area'</span>).area.count().sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图方法1</span></span><br><span class="line">    area_count.plot.bar(color=<span class="string">'#4652B1'</span>)  <span class="comment"># 设置为蓝紫色</span></span><br><span class="line">    pl.xticks(rotation=<span class="number">0</span>)  <span class="comment"># x轴名称太长重叠，旋转为纵向</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图方法2</span></span><br><span class="line">    <span class="comment"># plt.bar(range(11),area_count.values,tick_label = area_count.index,color</span></span><br><span class="line">    <span class="comment"># = '#4652B1')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(area_count.values)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.5</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line">    plt.title(<span class="string">'各国/地区电影数量排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'国家/地区'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('各国(地区)电影数量排名.png')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 3电影作品数量集中的年份</span></span><br><span class="line"><span class="comment"># 从日期中提取年份</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_3</span><span class="params">()</span>:</span></span><br><span class="line">    df[<span class="string">'year'</span>] = df[<span class="string">'time'</span>].map(<span class="keyword">lambda</span> x: x.split(<span class="string">'/'</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(df.info())</span></span><br><span class="line">    <span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计各年上映的电影数量</span></span><br><span class="line">    grouped_year = df.groupby(<span class="string">'year'</span>)</span><br><span class="line">    grouped_year_amount = grouped_year.year.count()</span><br><span class="line">    top_year = grouped_year_amount.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    top_year.plot(kind=<span class="string">'bar'</span>, color=<span class="string">'orangered'</span>)  <span class="comment"># 颜色设置为橙红色</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(top_year.values)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.1</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line">    plt.title(<span class="string">'电影数量年份排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'年份(年)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment"># plt.savefig('电影数量年份排名.png')</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 4拥有电影作品数量最多的演员</span></span><br><span class="line"><span class="comment"># 表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_4</span><span class="params">()</span>:</span></span><br><span class="line">    starlist = []</span><br><span class="line">    star_total = df.star</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> df.star.str.replace(<span class="string">' '</span>, <span class="string">''</span>).str.split(<span class="string">','</span>):</span><br><span class="line">        starlist.extend(i)</span><br><span class="line">    <span class="comment"># print(starlist)</span></span><br><span class="line">    <span class="comment"># print(len(starlist))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set去除重复的演员名</span></span><br><span class="line">    starall = set(starlist)</span><br><span class="line">    <span class="comment"># print(starall)</span></span><br><span class="line">    <span class="comment"># print(len(starall))</span></span><br><span class="line"></span><br><span class="line">    starall2 = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> starall:</span><br><span class="line">        <span class="keyword">if</span> starlist.count(i) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 筛选出电影数量超过1部的演员</span></span><br><span class="line">            starall2[i] = starlist.count(i)</span><br><span class="line"></span><br><span class="line">    starall2 = sorted(starall2.items(),</span><br><span class="line">                      key=<span class="keyword">lambda</span> starlist: starlist[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    starall2 = dict(starall2[:<span class="number">10</span>])  <span class="comment"># 将元组转为字典格式</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    x_star = list(starall2.keys())  <span class="comment"># x轴坐标</span></span><br><span class="line">    y_star = list(starall2.values())  <span class="comment"># y轴坐标</span></span><br><span class="line"></span><br><span class="line">    plt.bar(range(<span class="number">10</span>), y_star, tick_label=x_star)</span><br><span class="line">    pl.xticks(rotation=<span class="number">270</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(y_star):</span><br><span class="line">        plt.text(x, y + <span class="number">0.1</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'演员电影作品数量排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'演员'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('演员电影作品数量排名.png')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    annalysis_1()</span><br><span class="line">    annalysis_2()</span><br><span class="line">    annalysis_3()</span><br><span class="line">    annalysis_4()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;python爬虫第1篇&lt;/em&gt;&lt;/b&gt;&lt;br&gt;利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="requests" scheme="https://www.makcyun.top/tags/requests/"/>
    
      <category term="正则表达式" scheme="https://www.makcyun.top/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
      <category term="beautifulsoup" scheme="https://www.makcyun.top/tags/beautifulsoup/"/>
    
      <category term="css" scheme="https://www.makcyun.top/tags/css/"/>
    
      <category term="xpath" scheme="https://www.makcyun.top/tags/xpath/"/>
    
      <category term="lxml" scheme="https://www.makcyun.top/tags/lxml/"/>
    
  </entry>
  
  <entry>
    <title>4块钱,用Github+Hexo搭建你的个人博客：美化篇</title>
    <link href="https://www.makcyun.top/hexo02.html"/>
    <id>https://www.makcyun.top/hexo02.html</id>
    <published>2018-07-17T10:17:10.000Z</published>
    <updated>2018-08-22T06:18:21.193Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。</p><a id="more"></a><p><strong>摘要：</strong>搭建博客相对简单，而美化博客则要复杂一些，因为涉及到修改和增删源代码，对于没有前端基础的人来说，会比较费时间精力。为了尽可能在最短的时间里，打造一个总体看得过去的博客，本文以我的博客为例，介绍一些比较实用的博客美化操作和技巧。</p><h2 id="1-选择新的模板"><a href="#1-选择新的模板" class="headerlink" title="1. 选择新的模板"></a>1. 选择新的模板</h2><p>首先，是要更换非常难看的初始的博客界面。重新挑选一个好看的主题模板，然后在此基础上进行美化。<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/90405263.jpg" alt=""></p><p>主题寻找：<br><a href="https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories" target="_blank" rel="noopener">https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories</a></p><p>该网站按照模板的受欢迎程度进行排名，可以看到遥遥领先的第一名是一款叫作：<strong>next</strong>的主题，选用这款即可。进入到这个主题，可以阅读<strong>README.md</strong>模板使用说明，还可以查看模板示例网站。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FsgZW6JnrTdKpylVLyEORZbKJr9f" alt=""></p><p>模板使用：<br>打开博客根目录下的<strong>themes文件夹</strong>(注：后文所说的根目录指：<code>D:\blog</code>)，右键<strong>Git Bash</strong>运行下述命令：<br><code>git clone https://github.com/iissnan/hexo-theme-next themes/next</code><br>就可以把这款主题的安装文件下载到电脑中。接着，打开D:\blog_config.yml文件，找到 theme字段，修改参数为：theme: hexo-theme-next，然后根目录运行下述命令：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo s -g</span><br></pre></td></tr></table></figure></p><p>这样，便成功应用新的<strong>next</strong>主题，浏览器访问 :<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>，查看一下新的博客页面。<br><img src="http://pbscl931v.bkt.clouddn.com/FpHLTzWWl-JiakApNlw4nTH4hiin" alt=""><br>可以看到，博客变得非常清爽了，（可能和你实际看到的，略有不同，没有关系）。<br>这款主题包含4种风格，默认的是<strong>Muse</strong>，也可以尝试其他风格。具体操作：<br>打开<code>D:\blog\_config.yml</code>，定位到Schemes，想要哪款主题就取消前面的<strong>#</strong>，我的博客使用的是<strong>Pisces</strong>风格。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br></pre></td></tr></table></figure></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/78786445.jpg" alt=""></p><h2 id="2-模板美化"><a href="#2-模板美化" class="headerlink" title="2. 模板美化"></a>2. 模板美化</h2><p>接下来进行模板的美化。<br>根据网页的结构布局，将从以下几个部分进行针对性地美化：</p><ul><li>总体</li><li>侧边栏</li><li>页脚</li><li>文章</li></ul><p><strong>重要的文件</strong><br>美化需要主要是对几个模板文件进行修改和增删。为了便于后续进行操作，先列出文件名和所在的位置： </p><ul><li>站点文件。位于站点文件夹根目录内：<br><strong>D:/blog/_config.yml</strong></li><li>主题文件。位于主题文件夹根目录内：<br><strong>D:/blog/themes/next/_config.yml</strong></li><li>自定义样式文件。位于主题文件夹内：<br><strong>D:\blog\themes\hexo-theme-next\source\css_custom\custom.styl</strong></li></ul><h3 id="2-1-总体布置"><a href="#2-1-总体布置" class="headerlink" title="2.1. 总体布置"></a>2.1. 总体布置</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/38494028.jpg" alt=""></p><h4 id="2-1-1-设置中文界面"><a href="#2-1-1-设置中文界面" class="headerlink" title="2.1.1. 设置中文界面"></a>2.1.1. 设置中文界面</h4><p><strong>站点文件:</strong> language: zh-Hans<br>如果中文乱码，记事本另存为utf-8，最好不要用记事本编辑，用notepad。</p><h4 id="2-1-2-动态背景"><a href="#2-1-2-动态背景" class="headerlink" title="2.1.2. 动态背景"></a>2.1.2. 动态背景</h4><p><strong>主题文件：</strong> canvas_nest: true<br>背景的几何线条是采用的nest效果，一个基于html5 canvas绘制的网页背景效果，非常赞！来自github的开源项目canvas-nest：<a href="https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md" target="_blank" rel="noopener">https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md</a></p><p>如果感觉默认的线条太多的话，可以这么设置：<br>打开 <code>next/layout/_layout.swig</code>，在 &lt; /body&gt;之前添加代码(注意不要放在&lt; /head&gt;的后面)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.canvas_nest %&#125;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;</span><br><span class="line">color=&quot;233,233,233&quot; opacity=&apos;0.9&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>说明：<br>color ：线条颜色, 默认: ‘0,0,0’；三个数字分别为(R,G,B)<br>opacity: 线条透明度（0~1）, 默认: 0.5<br>count: 线条的总数量, 默认: 150<br>zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1</p><h3 id="2-2-侧边栏美化"><a href="#2-2-侧边栏美化" class="headerlink" title="2.2. 侧边栏美化"></a>2.2. 侧边栏美化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/97716423.jpg" alt=""></p><h4 id="2-2-1-添加博客名字和slogan"><a href="#2-2-1-添加博客名字和slogan" class="headerlink" title="2.2.1. 添加博客名字和slogan"></a>2.2.1. 添加博客名字和slogan</h4><p>修改<strong>站点文件</strong>如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Hexo Configuration</span><br><span class="line">## Docs: https://hexo.io/docs/configuration.html</span><br><span class="line">## Source: https://github.com/hexojs/hexo/</span><br><span class="line"></span><br><span class="line"># Site</span><br><span class="line">title: 高级农民工            # 更改为你自己的</span><br><span class="line">subtitle: Beginner<span class="string">'s Mind   </span></span><br><span class="line"><span class="string">description:</span></span><br><span class="line"><span class="string">keywords: python,hexo,神器,软件</span></span><br><span class="line"><span class="string">author: 高级农民工</span></span><br><span class="line"><span class="string">language: zh-Hans</span></span><br><span class="line"><span class="string">timezone:</span></span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-菜单设置"><a href="#2-2-2-菜单设置" class="headerlink" title="2.2.2. 菜单设置"></a>2.2.2. 菜单设置</h4><p>文件路径：<code>D:\blog\themes\hexo-theme-next\languages\zh-Hans.yml</code><br>修改如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: 首&amp;emsp;&amp;emsp;页</span><br><span class="line">  archives: 归&amp;emsp;&amp;emsp;档</span><br><span class="line">  categories: 分&amp;emsp;&amp;emsp;类</span><br><span class="line">  tags: 标&amp;emsp;&amp;emsp;签</span><br><span class="line">  about: 关于博主</span><br><span class="line">  search: 站内搜索</span><br><span class="line">  top: 最受欢迎</span><br><span class="line">  schedule: 日程表</span><br><span class="line">  sitemap: 站点地图</span><br><span class="line">  # commonweal: 公益404</span><br></pre></td></tr></table></figure></p><p>注意：两字的中间添加<code>&amp;emsp;&amp;emsp;</code>可实现列对齐。</p><h4 id="2-2-3-新建标签、分类、关于页面"><a href="#2-2-3-新建标签、分类、关于页面" class="headerlink" title="2.2.3. 新建标签、分类、关于页面"></a>2.2.3. 新建标签、分类、关于页面</h4><p>分别运行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;tags&quot; </span><br><span class="line">hexo new page &quot;categories&quot;  </span><br><span class="line">hexo new page &quot;about&quot;</span><br></pre></td></tr></table></figure></p><p>然后，打开<code>D:\blog\source</code>就可以看到上述三个文件夹。<br>要添加关于博主的介绍，只需要在<code>/about/index.md</code>文件中，用markdown书写内容即可，写完后运行：<code>hexo d -g</code>，便可看到效果。</p><h4 id="2-2-4-侧栏社交链接图标设置"><a href="#2-2-4-侧栏社交链接图标设置" class="headerlink" title="2.2.4. 侧栏社交链接图标设置"></a>2.2.4. 侧栏社交链接图标设置</h4><p>可以添加你的github、Email、知乎、简书等社交网站账号。<br><strong>主题文件：</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># ---------------------------------------------------------------</span><br><span class="line"># Sidebar Settings 侧栏社交链接图标设置</span><br><span class="line"># ---------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># Social Links.</span><br><span class="line"># Usage: `Key: permalink || icon`</span><br><span class="line"># Key is the link label showing to end users.</span><br><span class="line"># Value before `||` delimeter is the target permalink.</span><br><span class="line"># Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.</span><br><span class="line">social:</span><br><span class="line">  GitHub: https:<span class="comment">//github.com/makcyun || github</span></span><br><span class="line">  E-Mail: mailto:johnny824lee@gmail.com || envelope</span><br><span class="line">  #Google: https://plus.google.com/yourname || google</span><br><span class="line">  #Twitter: https://twitter.com/yourname || twitter</span><br><span class="line">  #FB Page: https://www.facebook.com/yourname || facebook</span><br><span class="line">  #VK Group: https://vk.com/yourname || vk</span><br><span class="line">  #StackOverflow: https://stackoverflow.com/yourname || stack-overflow</span><br><span class="line">  #YouTube: https://youtube.com/yourname || youtube</span><br><span class="line">  #Instagram: https://instagram.com/yourname || instagram</span><br><span class="line">  #Skype: skype:yourname?call|chat || skype</span><br><span class="line"></span><br><span class="line">social_icons:</span><br><span class="line">  enable: <span class="literal">true</span></span><br><span class="line">  icons_only: <span class="literal">false</span></span><br><span class="line">  transition: <span class="literal">false</span></span><br></pre></td></tr></table></figure></p><h4 id="2-2-5-添加头像并美化"><a href="#2-2-5-添加头像并美化" class="headerlink" title="2.2.5. 添加头像并美化"></a>2.2.5. 添加头像并美化</h4><p>博客添加头像有两种方法：第一种是放在本地文件夹中：D:\blog\public\uploads，并且命名为<strong>avatar.jpg</strong>。第二种是将图片放在七牛云中，然后传入链接。推荐这种方式，可以加快网页打开速度。<br><strong>站点文件</strong>任意行添加下面代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 添加头像</span><br><span class="line"># avatar: /uploads/avatar.jpg   #方法1本地图片</span><br><span class="line">avatar: http:<span class="comment">//pbscl931v.bkt.clouddn.com/18-8-3/40685653.jpg  # 方法2网络图片</span></span><br><span class="line"></span><br><span class="line">注意：uppoads文件夹是在主题里的文件夹，没有则新建</span><br><span class="line">D:\blog\themes\hexo-theme-next\source\uploads\avatar.jpg</span><br></pre></td></tr></table></figure></p><p><strong>头像变圆形</strong><br>可参考：<br><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html" target="_blank" rel="noopener">http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html</a><br><code>D:\blog\themes\next\source\css\_common\components\sidebar\sidebar-author.styl</code>，在里面添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">.site-author-image &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  margin: <span class="number">0</span> auto;</span><br><span class="line">  padding: $site-author-image-padding;</span><br><span class="line">  max-width: $site-author-image-width;</span><br><span class="line">  height: $site-author-image-height;</span><br><span class="line">  border: $site-author-image-border-width solid $site-author-image-border-color;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 头像圆形 */</span></span><br><span class="line">  border-radius: <span class="number">80</span>px;</span><br><span class="line">  -webkit-border-radius: <span class="number">80</span>px;</span><br><span class="line">  -moz-border-radius: <span class="number">80</span>px;</span><br><span class="line">  box-shadow: inset 0 -1px 0 #333sf;</span><br><span class="line">  <span class="comment">/* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 </span></span><br><span class="line"><span class="comment">    (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-transition: -webkit-transform <span class="number">1.0</span>s ease-out;</span><br><span class="line">  -moz-transition: -moz-transform <span class="number">1.0</span>s ease-out;</span><br><span class="line">  transition: transform <span class="number">1.0</span>s ease-out;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*再进一步想点击产生旋转效果，就继续在该文件下方添加代码：*/</span></span><br><span class="line"></span><br><span class="line">img:hover &#123;</span><br><span class="line">  <span class="comment">/* 鼠标经过停止头像旋转 </span></span><br><span class="line"><span class="comment">  -webkit-animation-play-state:paused;</span></span><br><span class="line"><span class="comment">  animation-play-state:paused;*/</span></span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">  -moz-transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">  transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* Z 轴旋转动画 */</span></span><br><span class="line">@-webkit-keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@-moz-keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    -moz-transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    -moz-transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="2-3-页脚美化"><a href="#2-3-页脚美化" class="headerlink" title="2.3. 页脚美化"></a>2.3. 页脚美化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/89825453.jpg" alt=""><br><strong>建站时间设置</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site's feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss:</span><br><span class="line"></span><br><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  # 建站年份</span><br><span class="line">  since: 2018           #根据实际情况修改</span><br></pre></td></tr></table></figure></p><h4 id="2-3-1-隐藏powered-By-Hexo-主题"><a href="#2-3-1-隐藏powered-By-Hexo-主题" class="headerlink" title="2.3.1. 隐藏powered By Hexo/主题"></a>2.3.1. 隐藏powered By Hexo/主题</h4><p>文件路径： D:\blog\themes\hexo-theme-next\layout_partials\ <strong>footer.swig</strong><br>更改该文件下面的代码：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="theme-info"&gt;&#123;#</span><br><span class="line">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line">  #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span><br><span class="line">#&#125;&lt;/div&gt;</span><br></pre></td></tr></table></figure></p><p>用<!--  -->注释两行如下语句，也可以直接删除掉这段代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--<span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"theme-info"</span>&gt;</span>&#123;#</span></span><br><span class="line"><span class="xml">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span></span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line"><span class="xml">  #&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span>&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span></span><br><span class="line"><span class="xml">#&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span>--&gt;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-2-next版本隐藏"><a href="#2-3-2-next版本隐藏" class="headerlink" title="2.3.2. next版本隐藏"></a>2.3.2. next版本隐藏</h4><p>继续在上面文件中修改代码如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 用&lt;!--注释语句--&gt;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.footer.theme.enable %&#125;</span><br><span class="line">  &lt;!--<span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"theme-info"</span>&gt;</span>&#123;#</span></span><br><span class="line"><span class="xml">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span></span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line"><span class="xml">  #&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span>&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span></span><br><span class="line"><span class="xml">#&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span>--&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-3-时间和用户名之间添加心形"><a href="#2-3-3-时间和用户名之间添加心形" class="headerlink" title="2.3.3. 时间和用户名之间添加心形"></a>2.3.3. 时间和用户名之间添加心形</h4><p><strong>主题文件：</strong>建站时间下面修改<code>icon: heart</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  # 建站年份</span><br><span class="line">  since: 2018</span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  # 年份后面的图标，为 Font Awesome 图标</span><br><span class="line">  # 自己去纠结 http://fontawesome.io/icons/</span><br><span class="line">  # 然后更改名字就行，下面的有关图标的设置都一样</span><br><span class="line"></span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  #icon: user</span><br><span class="line">  icon: heart</span><br></pre></td></tr></table></figure></p><p>如果还想让心变成跳动的红心，则继续在:上面的<strong>footer.swig</strong>文件中修改：<br><code>&lt;span class=&quot;with-love&quot;&gt;</code>为 <code>&lt;span class=&quot;with-love&quot; id=&quot;heart&quot;&gt;</code>  #一定要加id=”heart”<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="copyright"&gt;&#123;#</span><br><span class="line">#&#125;&#123;% set current = date(Date.now(), "YYYY") %&#125;&#123;#</span><br><span class="line">#&#125;&amp;copy; &#123;% if theme.footer.since and theme.footer.since != current %&#125;&#123;&#123; theme.footer.since &#125;&#125; &amp;mdash; &#123;% endif %&#125;&#123;#</span><br><span class="line">#&#125;&lt;span itemprop="copyrightYear"&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt;</span><br><span class="line">  &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"with-love"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-&#123;&#123; theme.footer.icon &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">  &lt;span class="author" itemprop="copyrightHolder"&gt;&#123;&#123; theme.footer.copyright || config.author &#125;&#125;&lt;/</span>span&gt;</span><br></pre></td></tr></table></figure></p><p>在自定义文件中添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 页脚加闪烁红心</span></span><br><span class="line"><span class="comment">// 自定义页脚跳动的心样式</span></span><br><span class="line">@keyframes heartAnimate &#123;</span><br><span class="line">    <span class="number">0</span>%,<span class="number">100</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    <span class="number">10</span>%,<span class="number">30</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    <span class="number">20</span>%,<span class="number">40</span>%,<span class="number">60</span>%,<span class="number">80</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    <span class="number">50</span>%,<span class="number">70</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line">#heart &#123;</span><br><span class="line">    animation: heartAnimate <span class="number">1.33</span>s ease-<span class="keyword">in</span>-out infinite;</span><br><span class="line">&#125;</span><br><span class="line">.with-love &#123;</span><br><span class="line">    color: rgb(<span class="number">192</span>, <span class="number">0</span>, <span class="number">39</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>接着在自定义<code>custom.styl</code>文件中，添加以下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 页脚加闪烁红心</span></span><br><span class="line"><span class="comment">// 自定义页脚跳动的心样式</span></span><br><span class="line">@keyframes heartAnimate &#123;</span><br><span class="line">    <span class="number">0</span>%,<span class="number">100</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    <span class="number">10</span>%,<span class="number">30</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    <span class="number">20</span>%,<span class="number">40</span>%,<span class="number">60</span>%,<span class="number">80</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    <span class="number">50</span>%,<span class="number">70</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line">#heart &#123;</span><br><span class="line">    animation: heartAnimate <span class="number">1.33</span>s ease-<span class="keyword">in</span>-out infinite;</span><br><span class="line">&#125;</span><br><span class="line">.with-love &#123;</span><br><span class="line">    color: rgb(192, 0, 39);   # rgb可随意修改</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-4-页脚显示总访客数和总浏览量"><a href="#2-3-4-页脚显示总访客数和总浏览量" class="headerlink" title="2.3.4. 页脚显示总访客数和总浏览量"></a>2.3.4. 页脚显示总访客数和总浏览量</h4><p>首先，在上述<code>footer.swig</code>文件首行添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;script <span class="keyword">async</span> src=<span class="string">"https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">#接着修改相应代码：</span><br><span class="line"># 添加总访客量</span><br><span class="line">&lt;span id=<span class="string">"busuanzi_container_site_uv"</span>&gt;</span><br><span class="line">  访客数:<span class="xml"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"busuanzi_value_site_uv"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span>人次</span><br><span class="line">&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&#123;% if theme.footer.powered %&#125;</span></span><br><span class="line"><span class="regexp">  &lt;!--&lt;div class="powered-by"&gt;&#123;#</span></span><br><span class="line"><span class="regexp">  #&#125;&#123;&#123; __('footer.powered', '&lt;a class="theme-link" target="_blank" href="https:/</span><span class="regexp">/hexo.io"&gt;Hexo&lt;/</span>a&gt;<span class="string">') &#125;&#125;&#123;#</span></span><br><span class="line"><span class="string">#&#125;&lt;/div&gt;--&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加'</span>|<span class="string">'符号</span></span><br><span class="line"><span class="string">&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125;</span></span><br><span class="line"><span class="string">  &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;% if theme.footer.custom_text %&#125;</span></span><br><span class="line"><span class="string">  &lt;div class="footer-custom"&gt;&#123;#</span></span><br><span class="line"><span class="string">  #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;#</span></span><br><span class="line"><span class="string">#&#125;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加总访问量</span></span><br><span class="line"><span class="string">&lt;span id="busuanzi_container_site_pv"&gt;</span></span><br><span class="line"><span class="string">   总访问量:&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次</span></span><br><span class="line"><span class="string">&lt;/span&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加'</span>|<span class="string">'符号</span></span><br><span class="line"><span class="string">&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125;</span></span><br><span class="line"><span class="string">  &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加博客全站共：</span></span><br><span class="line"><span class="string">&lt;div class="theme-info"&gt;</span></span><br><span class="line"><span class="string">  &lt;div class="powered-by"&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">  &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br></pre></td></tr></table></figure></p><h3 id="2-4-文章美化"><a href="#2-4-文章美化" class="headerlink" title="2.4. 文章美化"></a>2.4. 文章美化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/74366995.jpg" alt=""></p><h4 id="2-4-1-显示统计字数和估计阅读时长"><a href="#2-4-1-显示统计字数和估计阅读时长" class="headerlink" title="2.4.1. 显示统计字数和估计阅读时长"></a>2.4.1. 显示统计字数和估计阅读时长</h4><p>修改<strong>主题文件：</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Post wordcount display settings</span><br><span class="line"># Dependencies: https://github.com/willin/hexo-wordcount</span><br><span class="line"># 显示统计字数和估计阅读时长</span><br><span class="line"># 注意：这个要安装插件，先进入站点文件夹根目录</span><br><span class="line"># 然后：npm install hexo-wordcount --save</span><br><span class="line">post_wordcount:</span><br><span class="line">  item_text: <span class="literal">true</span></span><br><span class="line">  wordcount: <span class="literal">true</span></span><br><span class="line">  min2read: <span class="literal">true</span></span><br><span class="line">  totalcount: <span class="literal">false</span></span><br><span class="line">  separated_meta: <span class="literal">false</span></span><br></pre></td></tr></table></figure></p><p>注意，做了以上修改后，发现字数只显示了数字并没有带相应的单位:<strong>字</strong>和<strong>分钟</strong>。因此，还需做如下修改：<br>打开<code>D:\blog\themes\hexo-theme-next\layout\_macro\ **post.swig**</code>文件，添加单位：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> theme.post_wordcount.wordcount or theme.post_wordcount.min2read %&#125;</span><br><span class="line">            &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-wordcount"</span>&gt;</span><br><span class="line">              &#123;% <span class="keyword">if</span> theme.post_wordcount.wordcount %&#125;</span><br><span class="line">                &#123;% <span class="keyword">if</span> not theme.post_wordcount.separated_meta %&#125;</span><br><span class="line">                  &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-divider"</span>&gt;|<span class="xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-item-icon"</span>&gt;</span><br><span class="line">                  &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-file-word-o"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">                &#123;% if theme.post_wordcount.item_text %&#125;</span></span><br><span class="line"><span class="regexp">                  &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.wordcount') &#125;&#125;&amp;#58;&lt;/</span>span&gt;</span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span title=<span class="string">"&#123;&#123; __('post.wordcount') &#125;&#125;"</span>&gt;</span><br><span class="line">                  &#123;&#123; wordcount(post.content) &#125;&#125; 字</span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">              &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">              &#123;% if theme.post_wordcount.wordcount and theme.post_wordcount.min2read %&#125;</span></span><br><span class="line"><span class="regexp">                &lt;span class="post-meta-divider"&gt;|&lt;/</span>span&gt;</span><br><span class="line">              &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">              &#123;% <span class="keyword">if</span> theme.post_wordcount.min2read %&#125;</span><br><span class="line">                &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-item-icon"</span>&gt;</span><br><span class="line">                  &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-clock-o"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">                &#123;% if theme.post_wordcount.item_text %&#125;</span></span><br><span class="line"><span class="regexp">                  &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.min2read') &#125;&#125; &amp;asymp;&lt;/</span>span&gt;</span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span title=<span class="string">"&#123;&#123; __('post.min2read') &#125;&#125;"</span>&gt;</span><br><span class="line">                  &#123;&#123; min2read(post.content) &#125;&#125; 分钟 </span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">              &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;/</span>div&gt;</span><br><span class="line">          &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">          &#123;% <span class="keyword">if</span> post.description and (not theme.excerpt_description or not is_index) %&#125;</span><br><span class="line">              &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-description"</span>&gt;</span><br><span class="line">                  &#123;&#123; post.description &#125;&#125;</span><br><span class="line">              &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">          &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">      &lt;<span class="regexp">/header&gt;</span></span><br><span class="line"><span class="regexp">    &#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure></p><h4 id="2-4-2-添加阅读全文"><a href="#2-4-2-添加阅读全文" class="headerlink" title="2.4.2. 添加阅读全文"></a>2.4.2. 添加阅读全文</h4><p>实现在主页只展示部分文字，其他文字隐藏起来，通过点击’阅读更多’来阅读全文。<br>方法就是写每一篇文章的时候，在必要的地方添加<code>&lt;!-- more --&gt;</code>即可。<br>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 4块钱,用Github+Hexo搭建你的个人博客：搭建篇</span><br><span class="line">id: hexo01</span><br><span class="line">images: http://pbscl931v.bkt.clouddn.com/18-8-3/89578286.jpg</span><br><span class="line">categories: hexo博客</span><br><span class="line">tags: [hexo,个人博客,github]</span><br><span class="line">keywords: hexo,搭建博客,github pages,next</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。</span><br><span class="line">  </span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line"></span><br><span class="line">摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。</span><br></pre></td></tr></table></figure></p><h4 id="2-4-3-显示每篇文章的阅读量"><a href="#2-4-3-显示每篇文章的阅读量" class="headerlink" title="2.4.3. 显示每篇文章的阅读量"></a>2.4.3. 显示每篇文章的阅读量</h4><p>参考这个教程即可：<br><a href="http://www.jeyzhang.com/hexo-next-add-post-views.html" target="_blank" rel="noopener">http://www.jeyzhang.com/hexo-next-add-post-views.html</a></p><p>在这个过程中发现了一个问题：pc端正常显示阅读量，但是移动端没有显示具体的阅读量。解决办法：<br>在leancloud网站上，进入安全中心，检查web安全域名列表中是否添加了<strong>http：开头</strong>的域名，如果没有，则添加上应该就能解决，例如，我的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://makcyun.top/</span><br></pre></td></tr></table></figure></p><h4 id="2-4-4-文章摘要配图"><a href="#2-4-4-文章摘要配图" class="headerlink" title="2.4.4. 文章摘要配图"></a>2.4.4. 文章摘要配图</h4><p>参考这个教程即可：<br><a href="http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/" target="_blank" rel="noopener">http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/</a></p><p>附上我的设置：<br>在自定义文件中添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// img.img-topic &#123;</span></span><br><span class="line"><span class="comment">//    width: 100%;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//图片外部的容器方框</span></span><br><span class="line">.out-img-topic &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  max-height:<span class="number">350</span>px;      <span class="comment">//图片显示高度，如果不设置则每篇文章的图片高度会不一样，看起来不协调</span></span><br><span class="line">  margin-bottom: <span class="number">24</span>px;</span><br><span class="line">  overflow: hidden;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//图片</span></span><br><span class="line">img.img-topic &#123;</span><br><span class="line">  display: block ;</span><br><span class="line">  margin-left: <span class="number">.7</span>em;</span><br><span class="line">  margin-right: <span class="number">.7</span>em;</span><br><span class="line">  padding: <span class="number">0</span>;</span><br><span class="line">  float: right;</span><br><span class="line">  clear: right;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 去掉图片边框</span></span><br><span class="line">.posts-expand .post-body img &#123;</span><br><span class="line">    border: none;</span><br><span class="line">    padding: <span class="number">0</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-4-5-添加打赏功能"><a href="#2-4-5-添加打赏功能" class="headerlink" title="2.4.5. 添加打赏功能"></a>2.4.5. 添加打赏功能</h4><p>参考下面的教程：<br><a href="https://www.cnblogs.com/mrwuzs/p/7943337.html" target="_blank" rel="noopener">https://www.cnblogs.com/mrwuzs/p/7943337.html</a><br><a href="https://blog.csdn.net/lcyaiym/article/details/76796545" target="_blank" rel="noopener">https://blog.csdn.net/lcyaiym/article/details/76796545</a></p><p>以上，包括了博客美化的大部分操作。<br>如果，你觉得还不够，想做得更精致一些，那么推荐一个非常详细的美化教程：<br><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl" target="_blank" rel="noopener">https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl</a></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客" scheme="https://www.makcyun.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://www.makcyun.top/tags/hexo/"/>
    
      <category term="个人博客" scheme="https://www.makcyun.top/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github" scheme="https://www.makcyun.top/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>4块钱,用Github+Hexo搭建你的个人博客：搭建篇</title>
    <link href="https://www.makcyun.top/hexo01.html"/>
    <id>https://www.makcyun.top/hexo01.html</id>
    <published>2018-07-06T08:44:19.881Z</published>
    <updated>2018-08-16T09:27:15.222Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。</em></b><br>之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。</p><a id="more"></a>  <p><strong>【更新于2018/7/14】</strong></p><p><strong>摘要：</strong> 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。好在参考了很多大佬的教程后顺利搭建完成，但过程中还是踩了一些坑。这里及时进行总结，作为博客的第一篇文章。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><h3 id="1-网上有很多现成的博客不用，为什么要自己搭建"><a href="#1-网上有很多现成的博客不用，为什么要自己搭建" class="headerlink" title="1 网上有很多现成的博客不用，为什么要自己搭建?"></a>1 网上有很多现成的博客不用，为什么要自己搭建?</h3><p>可能有人会说：很多网站都能写博客，而且博客现在其实都有点过时了，为什么还要自己去搞？  </p><p>这里我说一下我想自己搭建的两点原因：<br><strong>一、</strong>网上的多数博客大家都共用一套相同的模板界面，没有特点、界面也杂乱充斥着很多不相关的东西，不论是自己写还是给人看，体验都不好。<br><strong>二、</strong>拥有一个你自己可以起名字的博客网站，里面的任何内容完全由你自己决定，这感觉是件很酷的事。 </p><p>这些普通的网站博客和一些个人博客，哪个好看，高下立判吧。<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/94423333.jpg" alt="新浪博客">  </p><p><center><strong>vs</strong></center><br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/12736339.jpg" alt="个人博客"></p><p>&nbsp;<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/26966.jpg" alt="CSDN博客">  </p><p><center><strong>vs</strong></center><br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/73613801.jpg" alt="个人博客"></p><p>更多个人博客：<br><strong>litten</strong> &nbsp; <a href="http://litten.me/" target="_blank" rel="noopener">http://litten.me/</a><br><strong>Ryan</strong> &nbsp; <a href="http://ryane.top/" target="_blank" rel="noopener">http://ryane.top/</a><br><strong>liyin</strong> &nbsp; <a href="https://liyin.date/" target="_blank" rel="noopener">https://liyin.date/</a><br><strong>reuixiy</strong> &nbsp; <a href="https://reuixiy.github.io/" target="_blank" rel="noopener">https://reuixiy.github.io/</a><br><strong>Tranquilpeak</strong> &nbsp; <a href="https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/" target="_blank" rel="noopener">https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/</a></p><p><br></p><h3 id="2-搭建博客难不难？"><a href="#2-搭建博客难不难？" class="headerlink" title="2 搭建博客难不难？"></a>2 搭建博客难不难？</h3><p>我之前认为搭建博客是一件只有能程序猿才能做出来的高大上的活。其实，只要跟着网上的教程一步步做下去，一个小时不到就可以搭建好你自己的个人博客。所以，搭建博客其实很简单。只不过，如果你想把博客做得好看一些的话，才会花费一些精力。</p><p><br></p><h2 id="二、开始搭建博客"><a href="#二、开始搭建博客" class="headerlink" title="二、开始搭建博客"></a>二、开始搭建博客</h2><p><strong>如果看到上面那些精美的博客，你已经心动了，那就开始动手吧。下面正式开始博客搭建步骤。</strong>  </p><p><strong>搭建教程参考</strong><br>搭建博客的教程网上一搜一大堆，为了节省你的搜索时间，这里我筛选出了下面几篇很棒的教程，我基本都是跟着一步步做下来的。</p><ol><li><a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">小白独立搭建博客</a>   </li><li><a href="http://ryane.top/2018/01/10/2018%EF%BC%8C%E4%BD%A0%E8%AF%A5%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E4%BA%86%EF%BC%81/" target="_blank" rel="noopener">2018，你该搭建自己的博客了！</a></li><li><a href="https://blog.csdn.net/gdutxiaoxu/article/details/53576018" target="_blank" rel="noopener">手把手教你用Hexo+Github 搭建属于自己的博客</a></li></ol><p>操作平台:Win7 64位。</p><p><br></p><p><strong>相关名词解释：</strong><br><strong>Hexo：</strong>一种常用的博客框架，有了它建立博客非常简单。你可以认为它是一种博客模板，只不过它比普通网站的那种博客模板要好看地多，并且有很高度的自定义性，只要你愿意，你可以建立一个独一无二的博客来。<br>若想详细了解Hexo的使用，移步 <strong>Hexo官方网站</strong> <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/</a>。  </p><p><strong>Github：</strong>一个全世界程序猿聚集的知名网站。免费的远程仓库和开源协作社区。我们需要利用网站里的Github Pages功能来托管需发布到网上的博客的相关文件和代码。</p><p><strong>Git：</strong> 一种版本控制系统。我们在自己的本地电脑写博客，如何把博客同步到Github，然后发布到网上去？就需要用这个软件去写几行代码然后就能搞定，后期用的最多的就是它。</p><p><strong>Node.js：</strong> 提供JavaScript的开发环境，安装好以后就不用跟它再打交道，所以不用太关注它。</p><h3 id="1-软件安装配置"><a href="#1-软件安装配置" class="headerlink" title="1 软件安装配置"></a>1 软件安装配置</h3><p>搭建博客需要先下载2个软件：Git和Nodejs。<br>软件安装过程很简单，一直点击Next默认直到安装完成就行了。</p><p><strong>Git</strong><br>官网：<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">https://git-scm.com/download/win</a><br>安装完，打开cmd窗口运行下面命令，如果有返回版本信息说明安装成功。</p><pre><code>git –version </code></pre><p><strong>Nodejs</strong><br>官网：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a><br>同样，安装完有返回版本信息说明安装成功，见下图。</p><pre><code>node -v  npm -v  </code></pre><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/43008634.jpg" alt="cmd命令"></p><p>至此，软件安装步骤完成。</p><h3 id="2-安装Hexo博客框架"><a href="#2-安装Hexo博客框架" class="headerlink" title="2 安装Hexo博客框架"></a>2 安装Hexo博客框架</h3><ul><li>安装hexo  </li></ul><p>这里开始就要用到使用频率最高的Git软件了。</p><p>桌面右键点击<strong>git bash here</strong>选项，会打开Git软件界面，输入下面每行命令并回车：  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>第一句是安装hexo，第二句是安装hexo部署到git page的deployer。代码命令看不懂没关系，一是这些命令之后几乎不再用到，二是用多了你会慢慢记住。<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/18599032.jpg" alt="">  </p><ul><li>设置博客存放文件夹  </li></ul><p>你需要预想一下你要把博客文件新建在哪个盘下的文件夹里，除了c盘都可以，例如d盘根目录下的blog文件夹，紧接着在桌面输入下面命令并回车：  </p><pre><code>hexo init /d/blogcd /d/blognpm install*注：/d/bog可以更改为你自己的文件夹*</code></pre><p>有的教程是先新建博客文件夹，在该文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，再执行以下操作。但是我操作过程中出现过这样的失败提示：<code>hexo:conmand not found</code>，但我执行上面的命令时就没有出现该问题。</p><pre><code>hexo init npm install</code></pre><ul><li>查看博客效果  </li></ul><p>至此，博客初步搭建好，输入下面一行本地部署生成的命令：  </p><pre><code>hexo s -g </code></pre><p>然后打开浏览器在网址栏输入：<code>localhost:4000</code>就可以看到博客的样子，如果无法打开，则继续输入下面命令：    </p><pre><code>npm install hexo-deployer-git --savehexo cleanhexo s -g </code></pre><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/90405263.jpg" alt="">  </p><p>打开该网址，你可以看到第一篇默认的博客：<strong>Hello World</strong>。但看起来很难看，后续会通过重新选择模板来对博客进行美化。  </p><div class="note primary"><p>现在你就可以开始写博客了，但是博客只能在你自己的电脑上看到，别人无法在网上看到你的博客。接下来需要利用前面提到的的Github Pages功能进行设置，设置完成之后别人通过搜索就可以看到你的博客。</p></div><h3 id="3-把你的博客部署到Github-Pages上去"><a href="#3-把你的博客部署到Github-Pages上去" class="headerlink" title="3 把你的博客部署到Github Pages上去"></a>3 把你的博客部署到Github Pages上去</h3><p>这是搭建博客相对比较复杂也是容易出错的一部分。</p><p><strong>1. Github账号注册及配置</strong>  </p><p>如果你没有github帐号，就新建一个，然后去邮箱进行验证；如果你有帐号则直接登录。<br>官网：<a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a> </p><p>配置步骤：  </p><ul><li>建立new repository</li></ul><p>只填写username.github.io即可，然后点击<code>create repositrory</code>。<br>注意：<code>username.github.io</code> 的<code>username</code>要和用户名保持一致，不然后面会失败。以我的为例：  </p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/98355992.jpg" alt="1"></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/62084844.jpg" alt="2"></p><ul><li>开启gh-pages功能  </li></ul><p>点击github主页点击头像下面的profile,找到新建立的username.github.io文件打开，点击settings，往下拉动鼠标到GitHub Pages。<br>如果你看到上方出现以下警告：  </p><div class="note warning"><br>GitHub Pages is currently disabled. You must first add content to your repository before you can publish a GitHub Pages site<br></div><p>不用管他，点击选择<code>choose a theme</code>，随便选择一个，（之后我们要更改这些丑陋的模板），然后select theme保存就行了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/79896859.jpg" alt="3"></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/1269326.jpg" alt="5"> </p><p>接下来的几个步骤参考<a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">教程1</a>即可。  </p><p>主要步骤包括：  </p><ul><li>git创建SSH密钥  </li><li>在GitHub账户中添加你的公钥  </li><li>测试成功并设置用户信息  </li><li>将本地的Hexo文件更新到Github库中  </li><li>hexo部署更新博客  </li></ul><p>经过以上几步的操作，顺利的话，你的博客可以发布到网上，其他人也可以通过你的网址<code>username.github.io</code>（我的是<code>makcyun.github.io</code>）<br>访问到你的博客。  </p><h3 id="4-赶紧新建个博客试试"><a href="#4-赶紧新建个博客试试" class="headerlink" title="4 赶紧新建个博客试试"></a>4 赶紧新建个博客试试</h3><p>接下来你可以自己新建一个文档来写下你的第一篇博客并在网页上测试。</p><p>同样在根目录<code>D:\blog</code>中运行下面命令：  </p><pre><code>hexo new 第一篇博客*注：第一篇博客名称可以随便修改*</code></pre><p>然后打开<code>D:\blog\source\_posts</code>文件夹，就可以看到一个<code>第一篇博客.md</code>的文件。用支持markdown语法的软件打开该文件进行编辑即可。</p><p>编辑好以后，运行下述命令：</p><pre><code>hexo cleanhexo d -g</code></pre><p>然后，在网址中输入<code>username.github.io</code>即可看到你的博客上，出现<strong>第一篇博客</strong>这篇新的文章。</p><p><strong>至此，你的个人博客初步搭建过程就完成了。</strong></p><p><br></p><p>但是，现在还存在两个问题你可能想解决：</p><ul><li>markdown语法是什么，如何用软件编写博客？</li><li>网址是<code>username.github.io</code>，感觉很奇怪，而我的博客网址怎么是<strong>www</strong>开头的？</li></ul><p>好，下面来讲解一下。</p><p><br></p><p><strong>第一个问题</strong></p><p>关于markdown语法介绍：<br><a href="https://www.jianshu.com/p/1e402922ee32/" target="_blank" rel="noopener">markdown——入门指南</a></p><p>当你大致了解markdown语法后，如何用markdown写博客呢？不妨参考这两篇详细教程：  </p><blockquote><p><a href="https://markdown.tw/" target="_blank" rel="noopener">Markdown語法說明</a><br><a href="https://www.ofind.cn/archives/" target="_blank" rel="noopener">HEXO下的Markdown语法(GFM)写博客</a></p></blockquote><p>接下来你要一个可以写markdown语法的软件，这里推荐两款软件。  </p><p>Windows下使用<a href="http://markdownpad.com/" target="_blank" rel="noopener">Markdown Pad2</a>, Mac下使用<a href="http://25.io/mou/" target="_blank" rel="noopener">Mou</a>。 </p><p>我用的是Markdown Pad2，这款软件是付费软件，但网上有很多破解版，我这里将软件上传到了百度网盘，如需请取。<br>MarkdownPad2： <a href="https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA" target="_blank" rel="noopener">https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA</a>   密码：y9zh</p><p>安装好后，就可以打开刚才的<code>第一篇博客.md</code>，开始尝试写你的第一篇博客了。  </p><p>比如这是我用markdownpad写的博客原稿。<br><img src="http://pbscl931v.bkt.clouddn.com/FpU5NFP6QFdTwqjStlUrBNk2GPDK" alt=""></p><p>可以看到文档里面都是字符，没有图片这些。所以只需要用键盘专注于打字就行了，不需要像word那么复杂，还要用鼠标插入标题样式、图片这些操作。</p><p><br></p><p><strong>第二个问题</strong></p><p>我的网址不是默认的<code>username.github.io</code>，是因为我购买了一个域名，然后和<code>username.github.io</code>进行了关联，这样我的博客网址变成了我的域名。</p><p>在哪里购买域名呢？<br>首推去 <a href="https://wanwang.aliyun.com/domain/?spm=5176.383338.1907008.1.LWIFhw" target="_blank" rel="noopener">阿里云官网</a> 购买。</p><p>你可以随意起你喜欢的名字，然后在该网站进行搜索，没有人占用的话你就可以购买该域名。不同的后缀价格不同。可以看到<strong>.com</strong>、 <strong>.net</strong>等会比较贵，最便宜的这两年新出的<strong>.top域名</strong>，只要4块钱一年，我购买的就是这种。</p><p>购买完域名以后，需要做以下几个步骤：  </p><ul><li>实名认证</li><li>修改DNS</li><li>域名解析</li><li>新建CNAME文件</li></ul><p><strong>1 实名认证</strong><br>在修改DNS之前，必须要阿里云官网实名认证成功，用淘宝账户登录然后填写相关信息即可。</p><p><strong>2 修改DNS</strong><br>实名认证成功后，进入管理界面，依次点击：<br><img src="http://pbscl931v.bkt.clouddn.com/Ft9CnDVTNm1WFZMegkca8SaOokfW" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/FtOks38CUKdJga4Q-rlSXjcdezPs" alt=""></p><p>修改DNS为：<br><strong>f1g1ns1.dnspod.net<br>f1g1ns2.dnspod.net</strong></p><p><strong>3 域名解析</strong><br>DNS修改好以后，到<strong>DNSPOD</strong>这个网站去解析你的域名。  </p><p>首先，微信登录并注册 <a href="https://www.dnspod.cn/" target="_blank" rel="noopener">https://www.dnspod.cn/</a>，点击域名解析，添加上你的域名。<br><img src="http://pbscl931v.bkt.clouddn.com/FuS2HLF9F9v9wvYiloqF8kpWMh8V" alt=""></p><p>接着，添加以下两条记录即可。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FojJP59gDAOuk41RMtCvEkuKijo2" alt=""></p><p>注意：<strong>makcyun.github.io.</strong>需换成你自己的名称，另外最后有一个<strong>“.”</strong></p><p><strong>4 新建CNAME文件</strong><br>在博客根目录文件夹下,例如我的<code>D:\blog\source</code>，新建名为<strong>CNAME</strong>的记事本文件，去掉后缀。<br>在里面输入你的域名，例如我的：<strong><a href="http://www.makcyun.top">www.makcyun.top</a></strong>即可，保存并关闭。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FmmCR8YMx-heNEiN9RFb4MkEbFA0" alt=""></p><p><strong>注意： </strong><br>这里填不填写<strong>www</strong>前缀都是可以的，区别在于填写www，那么博客网址就会以www开头，例如：<strong><a href="http://www.makcyun.top">www.makcyun.top</a></strong>；如果不填写，博客网址是：<strong>makcyun.top</strong>，二者都可以，看你喜欢。</p><p>完成以上4步之后，根目录下再次运行：  </p><pre><code>hexo d -g  </code></pre><p>这时，输入你在记事本里的域名网址，即可打开你的博客。<br>至此，你的博客就换成了你想要的网址，别人也可以通过这个网址访问到你的博客。 </p><p><br></p><p>到这里，博客的初步搭建就算完成了，顺利的话不到1小时就能完成，如果中间出现差错，保持耐心多试几次应该就没问题。  </p><p>此时，还有一个比较重要的问题就是，你可能会觉得你的博客不够美观，不如我博客里前面提到的那几个博客，甚至也没有我的好看。  </p><p>如果你还愿意折腾的话，下一篇文章，我会以我的博客为例，讲一讲如何进行博客的美化。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。&lt;/em&gt;&lt;/b&gt;&lt;br&gt;之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客" scheme="https://www.makcyun.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://www.makcyun.top/tags/hexo/"/>
    
      <category term="个人博客" scheme="https://www.makcyun.top/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github" scheme="https://www.makcyun.top/tags/github/"/>
    
  </entry>
  
</feed>
