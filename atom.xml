<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>高级农民工</title>
  
  <subtitle>Beginner&#39;s Mind</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.makcyun.top/"/>
  <updated>2018-09-09T01:02:19.216Z</updated>
  <id>https://www.makcyun.top/</id>
  
  <author>
    <name>高级农民工</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于PDF阅读处理软件，你需要的都在这里了</title>
    <link href="https://www.makcyun.top/fuli01.html"/>
    <id>https://www.makcyun.top/fuli01.html</id>
    <published>2018-09-08T12:16:37.349Z</published>
    <updated>2018-09-09T01:02:19.216Z</updated>
    
    <content type="html"><![CDATA[<p>分享6款软件：包括ABBYY FineReader、Small PDF。</p><a id="more"></a>  <p><strong>摘要：</strong> 我们办公中经常要和pdf打交道，包括同各种软件的转换、提取分割、压缩等处理操作，但你可能常常苦于找不到合适的软件。这里和你分享6款最棒的软件，满足你对pdf所有的需求。</p><p>先推荐两款pc和手机上比较好用的pdf阅读软件，这两款都是难得的国产良心之作。</p><h2 id="1-Foxit-Reader"><a href="#1-Foxit-Reader" class="headerlink" title="1. Foxit Reader"></a>1. Foxit Reader</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/96139143.jpg" alt=""><br>一句话概括：这款国产软件足以媲美大名鼎鼎的Adobe Acrobat。</p><h2 id="2-WPS-Office"><a href="#2-WPS-Office" class="headerlink" title="2. WPS Office"></a>2. WPS Office</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/90611339.jpg" alt=""><br>一句话概括：手机版的和电脑版的WPS可谓”天壤之别”，功能足够强大。</p><h2 id="3-ABBYY-FineReader"><a href="#3-ABBYY-FineReader" class="headerlink" title="3. ABBYY FineReader"></a>3. ABBYY FineReader</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/50294768.jpg" alt=""><br>一句话概括：知乎上PDF话题中最多赞文章中的推荐。官方售价500+RMB。<br><a href="https://www.zhihu.com/topic/19556393/top-answers" target="_blank" rel="noopener">https://www.zhihu.com/topic/19556393/top-answers</a><br>它最实用最强大的的功能：当你的pdf是扫描或者图片形式的，那么你无法进行复制但你又想复制，此时，这款软件能帮你搞定一切。<br>不仅是pdf，手机拍的照片文字也可以识别然后转化成word。</p><h2 id="4-Small-PDF"><a href="#4-Small-PDF" class="headerlink" title="4. Small PDF"></a>4. Small PDF</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/89711681.jpg" alt=""><br>一句话概括：网页版很好用，功能齐全，但是有次数限制，专业版费用是每年300+RMB。</p><h2 id="5-Solid-PDF-Converter"><a href="#5-Solid-PDF-Converter" class="headerlink" title="5. Solid PDF Converter"></a>5. Solid PDF Converter</h2><p><a href="https://www.soliddocuments.com/zh/features.htm?product=SolidConverterPDF" target="_blank" rel="noopener">https://www.soliddocuments.com/zh/features.htm?product=SolidConverterPDF</a><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/15794080.jpg" alt=""><br>一句话概括：当pdf是可复制的前提下，转换成word，excel和ppt的效果最好。官方售价100美元。</p><h2 id="6-PDFdo"><a href="#6-PDFdo" class="headerlink" title="6. PDFdo"></a>6. PDFdo</h2><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-8/19958488.jpg" alt=""><br>一句话概括：它最实用最强大的的功能是合并、分割、提取等功能。官方售价99RMB。  </p><p>以上这6款软件足以解决日常遇到的大部分问题。如需，公众号后台回复<strong>pdf</strong>即可得到。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分享6款软件：包括ABBYY FineReader、Small PDF。&lt;/p&gt;
    
    </summary>
    
      <category term="python爬虫" scheme="https://www.makcyun.top/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="pdf" scheme="https://www.makcyun.top/tags/pdf/"/>
    
  </entry>
  
  <entry>
    <title>Pyhton可视化(1): 中国大学学术排行榜</title>
    <link href="https://www.makcyun.top/Python_analysis1.html"/>
    <id>https://www.makcyun.top/Python_analysis1.html</id>
    <published>2018-09-05T05:27:22.000Z</published>
    <updated>2018-09-06T09:28:15.720Z</updated>
    
    <content type="html"><![CDATA[<p>Python爬虫近十年中国大学Top20强并结合D3.js做动态数据可视化表。</p><a id="more"></a>  <p><strong>摘要：</strong>：最近在朋友圈看到一个很酷炫的动态数据可视化表，介绍了新中国成立后各省GDP的发展历程，非常惊叹竟然还有这种操作，也想试试。于是，照葫芦画瓢虎，在网上爬取了历年中国大学学术排行榜，制作了一个中国大学排名Top20强动态表。</p><h2 id="1-作品介绍"><a href="#1-作品介绍" class="headerlink" title="1. 作品介绍"></a>1. 作品介绍</h2><p>这里先放一下这个动态表是什么样的：<br><a href="https://www.bilibili.com/video/av24503002" target="_blank" rel="noopener">https://www.bilibili.com/video/av24503002</a></p><p>不知道你看完是什么感觉，至少我是挺震惊的，想看看作者是怎么做出来的，于是追到了作者的B站主页，发现了更多有意思的动态视频：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/84261830.jpg" alt="">  </p><p>这些作品的作者是：@Jannchie见齐，他的主页：<br><a href="https://space.bilibili.com/1850091/#/video" target="_blank" rel="noopener">https://space.bilibili.com/1850091/#/video</a></p><p>这些会动的图表是如何做出来的呢？他用到的是一个动态图形显示数据的JavaScript库：<strong>D3.js</strong>，一种前端技术。难怪不是一般地酷炫。<br>那么，如果不会D3.js是不是就做不出来了呢？当然不是，Jannchie非常Open地给出了一个手把手简单教程：<br><a href="https://www.bilibili.com/video/av28087807" target="_blank" rel="noopener">https://www.bilibili.com/video/av28087807</a> </p><p>他同时还开放了程序源码，你只需要做2步就能够实现：  </p><ul><li><p>到他的Github主页下载源码到本地电脑：<br><a href="https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js" target="_blank" rel="noopener">https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</a><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/38399216.jpg" alt=""></p></li><li><p>打开<code>dist</code>文件夹里面的<code>exampe.csv</code>文件，放进你想要展示的数据，再用浏览器打开<code>bargraph.html</code>网页，就可以实现动态效果了。</p></li></ul><p>下面，我们稍微再说详细一点，实现这种效果的关键点。<br>最重要的是要有数据。观察一下上面的作品可以看到，横向柱状图中的数据要满足两个条件：一是要有多个对比的对象，二是要在时间上连续。这样才可以做出动态效果来。</p><p>看完后我立马就有了一个想法：<strong>想看看近十年中国的各个大学排名是个什么情况</strong>。下面我们就通过实实例来操作下。</p><h2 id="2-案例操作：中国大学Top20强"><a href="#2-案例操作：中国大学Top20强" class="headerlink" title="2. 案例操作：中国大学Top20强"></a>2. 案例操作：中国大学Top20强</h2><h3 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1. 数据来源"></a>2.1. 数据来源</h3><p>世界上最权威的大学排名有4类，分别是：</p><ul><li>原上海交通大学的ARWU<br> <a href="http://www.shanghairanking.com/ARWU2018.html" target="_blank" rel="noopener">http://www.shanghairanking.com/ARWU2018.html</a></li><li>英国教育组织的QS<br><a href="https://www.topuniversities.com/university-rankings/world-university-rankings/2018" target="_blank" rel="noopener">https://www.topuniversities.com/university-rankings/world-university-rankings/2018</a></li><li>泰晤士的THE<br><a href="https://www.timeshighereducation.com/world-university-rankings" target="_blank" rel="noopener">https://www.timeshighereducation.com/world-university-rankings</a>  </li><li>美国的usnews<br><a href="https://www.usnews.com/best-colleges/rankings" target="_blank" rel="noopener">https://www.usnews.com/best-colleges/rankings</a></li></ul><p>关于，这四类排名的更多介绍，可以看这个：<br><a href="https://www.zhihu.com/question/20825030/answer/71336291" target="_blank" rel="noopener">https://www.zhihu.com/question/20825030/answer/71336291</a></p><p>这里，我们选取相对比较权威也比较符合国情的第一个ARWU的排名结果。打开官网，可以看到有英文版和中文版排名，这里选取中文版。<br>排名非常齐全，从2003年到最新的2018年都有，非常好。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/79756091.jpg" alt=""></p><p>同时，可以看到这是世界500强的大学排名，而我们需要的是中国（包括港澳台）的大学排名。怎么办呢？ 当然不能一年年地复制然后再从500条数据里一条条筛选出中国的，这里就要用爬虫来实现了。可以参考不久前的一篇爬取表格的文章：<br><a href="https://www.makcyun.top/web_scraping_withpython2.html">https://www.makcyun.top/web_scraping_withpython2.html</a></p><h3 id="2-2-抓取数据"><a href="#2-2-抓取数据" class="headerlink" title="2.2. 抓取数据"></a>2.2. 抓取数据</h3><h4 id="2-2-1-分析url"><a href="#2-2-1-分析url" class="headerlink" title="2.2.1. 分析url"></a>2.2.1. 分析url</h4><p>首先，分析一下URL:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http://www.zuihaodaxue.com/ARWU2018.html</span><br><span class="line">http://www.zuihaodaxue.com/ARWU2017.html</span><br><span class="line">...</span><br><span class="line">http://www.zuihaodaxue.com/ARWU2009.html</span><br></pre></td></tr></table></figure><p>可以看到，url非常有规律，只有年份数字在变，很简单就能构造出for循环。<br>格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br></pre></td></tr></table></figure><p>下面就可以开始写爬虫了。  </p><h4 id="2-2-2-获取网页内容"><a href="#2-2-2-获取网页内容" class="headerlink" title="2.2.2. 获取网页内容"></a>2.2.2. 获取网页内容</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="comment"># 2009-2015用'gbk'，2016-2018用'utf-8'</span></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="comment"># return response.text  # text会乱码，content没有问题</span></span><br><span class="line">        <span class="keyword">return</span> response.content</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br></pre></td></tr></table></figure><p>上面需要注意的是，不同年份网页采用的编码不同，返回response.test会乱码，返回response.content则不会。关于编码乱码的问题，以后单独写一篇文章。</p><h4 id="2-2-3-解析表格"><a href="#2-2-3-解析表格" class="headerlink" title="2.2.3. 解析表格"></a>2.2.3. 解析表格</h4><p>用read_html函数一行代码来抓取表格，然后输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line">print(tb)</span><br></pre></td></tr></table></figure></p><p>可以看到，很顺利地表格就被抓取了下来：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/80641562.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/80641562.jpg"><br>但是表格需要进行处理，比如删除掉不需要的评分列，增加年份列等，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 重命名表格列，不需要的列用数字表示</span></span><br><span class="line">tb.columns = [<span class="string">'world rank'</span>,<span class="string">'university'</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="string">'score'</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">tb.drop([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],axis = <span class="number">1</span>,inplace = <span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 删除后面不需要的评分列</span></span><br><span class="line"><span class="comment"># rank列100名后是区间，需需唯一化，增加一列index作为排名</span></span><br><span class="line">tb[<span class="string">'index_rank'</span>] = tb.index</span><br><span class="line">tb[<span class="string">'index_rank'</span>] = tb[<span class="string">'index_rank'</span>].astype(int) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一列年份列</span></span><br><span class="line">tb[<span class="string">'year'</span>] = i</span><br><span class="line"><span class="comment"># read_html没有爬取country，需定义函数单独爬取</span></span><br><span class="line">tb[<span class="string">'country'</span>] = get_country(html)</span><br><span class="line"><span class="keyword">return</span> tb</span><br></pre></td></tr></table></figure><p>需要注意的是，国家没有被抓取下来，因为国家是用的图片表示的，定位到国家代码位置：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/39145641.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/39145641.jpg"></p><p>可以看到美国是用英文的USA表示的，那么我们可以单独提取出src属性，然后用正则提取出国家名称就可以了，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取国家名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_country</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    countries = soup.select(<span class="string">'td &gt; a &gt; img'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> countries:</span><br><span class="line">        src = i[<span class="string">'src'</span>]</span><br><span class="line">        pattern = re.compile(<span class="string">'flag.*\/(.*?).png'</span>)</span><br><span class="line">        country = re.findall(pattern,src)[<span class="number">0</span>]</span><br><span class="line">        lst.append(country)</span><br><span class="line">    <span class="keyword">return</span> lst</span><br></pre></td></tr></table></figure><p>然后，我们就可以输出一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">    world rank    university  score  index_rank  year      country</span><br><span class="line"><span class="number">0</span>            <span class="number">1</span>          哈佛大学  <span class="number">100.0</span>           <span class="number">1</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">1</span>            <span class="number">2</span>         斯坦福大学   <span class="number">75.6</span>           <span class="number">2</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">2</span>            <span class="number">3</span>          剑桥大学   <span class="number">71.8</span>           <span class="number">3</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">3</span>            <span class="number">4</span>        麻省理工学院   <span class="number">69.9</span>           <span class="number">4</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">4</span>            <span class="number">5</span>      加州大学-伯克利   <span class="number">68.3</span>           <span class="number">5</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">5</span>            <span class="number">6</span>        普林斯顿大学   <span class="number">61.0</span>           <span class="number">6</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">6</span>            <span class="number">7</span>          牛津大学   <span class="number">60.0</span>           <span class="number">7</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">7</span>            <span class="number">8</span>        哥伦比亚大学   <span class="number">58.2</span>           <span class="number">8</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">8</span>            <span class="number">9</span>        加州理工学院   <span class="number">57.4</span>           <span class="number">9</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">9</span>           <span class="number">10</span>         芝加哥大学   <span class="number">55.5</span>          <span class="number">10</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">10</span>          <span class="number">11</span>      加州大学-洛杉矶   <span class="number">51.2</span>          <span class="number">11</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">11</span>          <span class="number">12</span>         康奈尔大学   <span class="number">50.7</span>          <span class="number">12</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">12</span>          <span class="number">12</span>          耶鲁大学   <span class="number">50.7</span>          <span class="number">13</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">13</span>          <span class="number">14</span>     华盛顿大学-西雅图   <span class="number">50.0</span>          <span class="number">14</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">14</span>          <span class="number">15</span>     加州大学-圣地亚哥   <span class="number">47.8</span>          <span class="number">15</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">15</span>          <span class="number">16</span>       宾夕法尼亚大学   <span class="number">46.4</span>          <span class="number">16</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">16</span>          <span class="number">17</span>        伦敦大学学院   <span class="number">46.1</span>          <span class="number">17</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">17</span>          <span class="number">18</span>      约翰霍普金斯大学   <span class="number">45.4</span>          <span class="number">18</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">18</span>          <span class="number">19</span>     苏黎世联邦理工学院   <span class="number">43.9</span>          <span class="number">19</span>  <span class="number">2018</span>  Switzerland</span><br><span class="line"><span class="number">19</span>          <span class="number">20</span>    华盛顿大学-圣路易斯   <span class="number">42.1</span>          <span class="number">20</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">20</span>          <span class="number">21</span>      加州大学-旧金山   <span class="number">41.9</span>          <span class="number">21</span>  <span class="number">2018</span>          USA</span><br></pre></td></tr></table></figure><p>数据很完美，接下来就可以按照D3.js模板中的example.csv文件的格式作进一步的处理了。  </p><h3 id="2-3-数据处理"><a href="#2-3-数据处理" class="headerlink" title="2.3. 数据处理"></a>2.3. 数据处理</h3><p>这里先将数据输出为<code>university.csv</code>文件，结果见下表：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/44505347.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/44505347.jpg"></p><p>10年一共5011行×6列数据。接着，读入该表作进一步数据处理，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'university.csv'</span>)</span><br><span class="line"><span class="comment"># 包含港澳台</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只包括内地</span></span><br><span class="line">df = df.query(<span class="string">"(country == 'China')"</span>)</span><br><span class="line">df[<span class="string">'index_rank_score'</span>] = df[<span class="string">'index_rank'</span>]</span><br><span class="line"><span class="comment"># 将index_rank列转为整形</span></span><br><span class="line">df[<span class="string">'index_rank'</span>] = df[<span class="string">'index_rank'</span>].astype(int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 美国</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'UnitedStates')|(country == 'USA')")</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#求topn名</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(df)</span>:</span></span><br><span class="line">    top = df.sort_values([<span class="string">'year'</span>,<span class="string">'index_rank'</span>],ascending = <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> top[:<span class="number">20</span>].reset_index()</span><br><span class="line">df = df.groupby(by =[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改列顺序</span></span><br><span class="line">df = df[[<span class="string">'university'</span>,<span class="string">'index_rank_score'</span>,<span class="string">'index_rank'</span>,<span class="string">'year'</span>]]</span><br><span class="line"><span class="comment"># 重命名列</span></span><br><span class="line">df.rename (columns = &#123;<span class="string">'university'</span>:<span class="string">'name'</span>,<span class="string">'index_rank_score'</span>:<span class="string">'type'</span>,<span class="string">'index_rank'</span>:<span class="string">'value'</span>,<span class="string">'year'</span>:<span class="string">'date'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">df.to_csv(<span class="string">'university_ranking.csv'</span>,mode =<span class="string">'w'</span>,encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># index可以设置</span></span><br></pre></td></tr></table></figure><p>上面需要注意两点：</p><ul><li>可以提取包含港澳台在内的大中华区所有的大学，也可以只提取内地的大学，还可以提取世界、美国等各种排名。</li><li>定义了一个求Topn的函数，能够按年份分别求出各年的前20名大学名单。</li></ul><p>打开输出的<code>university_ranking.csv</code>文件：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/64400003.jpg" alt="http://pbscl931v.bkt.clouddn.com/18-9-6/64400003.jpg"></p><p>结果非常好，可以直接作为D3.js的导入文件了。</p><h4 id="2-3-1-完整代码"><a href="#2-3-1-完整代码" class="headerlink" title="2.3.1. 完整代码"></a>2.3.1. 完整代码</h4><p>将代码再稍微完善一下，完整地代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"><span class="comment"># 获取网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(year)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            headers = &#123;</span><br><span class="line">                <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment"># 英文版</span></span><br><span class="line">            <span class="comment"># url = 'http://www.shanghairanking.com/ARWU%s.html' % (str(year))</span></span><br><span class="line">            <span class="comment"># 中文版</span></span><br><span class="line">            url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br><span class="line">            response = requests.get(url,headers = headers)</span><br><span class="line">            <span class="comment"># 2009-2015用'gbk'，2016-2018用'utf-8'</span></span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="comment"># return response.text  # text会乱码，content没有问题</span></span><br><span class="line">                <span class="comment"># https://stackoverflow.com/questions/17011357/what-is-the-difference-between-content-and-text</span></span><br><span class="line">                <span class="keyword">return</span> response.content</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">except</span> RequestException:</span><br><span class="line">            print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html,i)</span>:</span></span><br><span class="line">        tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 重命名表格列，不需要的列用数字表示</span></span><br><span class="line">        tb.columns = [<span class="string">'world rank'</span>,<span class="string">'university'</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="string">'score'</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">        tb.drop([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],axis = <span class="number">1</span>,inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 删除后面不需要的评分列</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># rank列100名后是区间，需需唯一化，增加一列index作为排名</span></span><br><span class="line">        tb[<span class="string">'index_rank'</span>] = tb.index</span><br><span class="line">        tb[<span class="string">'index_rank'</span>] = tb[<span class="string">'index_rank'</span>].astype(int) + <span class="number">1</span></span><br><span class="line">        <span class="comment"># 增加一列年份列</span></span><br><span class="line">        tb[<span class="string">'year'</span>] = i</span><br><span class="line">        <span class="comment"># read_html没有爬取country，需定义函数单独爬取</span></span><br><span class="line">        tb[<span class="string">'country'</span>] = get_country(html)</span><br><span class="line">        <span class="comment"># print(tb) # 测试表格ok</span></span><br><span class="line">        <span class="keyword">return</span> tb</span><br><span class="line">        <span class="comment"># print(tb.info()) # 查看表信息</span></span><br><span class="line">        <span class="comment"># print(tb.columns.values) # 查看列表名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_country</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    countries = soup.select(<span class="string">'td &gt; a &gt; img'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> countries:</span><br><span class="line">        src = i[<span class="string">'src'</span>]</span><br><span class="line">        pattern = re.compile(<span class="string">'flag.*\/(.*?).png'</span>)</span><br><span class="line">        country = re.findall(pattern,src)[<span class="number">0</span>]</span><br><span class="line">        lst.append(country)</span><br><span class="line">    <span class="keyword">return</span> lst</span><br><span class="line">    <span class="comment"># print(lst) # 测试提取国家是否成功ok</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存表格为csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_csv</span><span class="params">(tb)</span>:</span></span><br><span class="line">    tb.to_csv(<span class="string">r'university.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    endtime = time.time()-start_time</span><br><span class="line">    <span class="comment"># print('程序运行了%.2f秒' %endtime)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis</span><span class="params">()</span>:</span></span><br><span class="line">    df = pd.read_csv(<span class="string">'university.csv'</span>)</span><br><span class="line">    <span class="comment"># 包含港澳台</span></span><br><span class="line">    <span class="comment"># df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]</span></span><br><span class="line">    <span class="comment"># 只包括内地</span></span><br><span class="line">    df = df.query(<span class="string">"(country == 'China')"</span>)</span><br><span class="line"></span><br><span class="line">    df[<span class="string">'index_rank_score'</span>] = df[<span class="string">'index_rank'</span>]</span><br><span class="line">    <span class="comment"># 将index_rank列转为整形</span></span><br><span class="line">    df[<span class="string">'index_rank'</span>] = df[<span class="string">'index_rank'</span>].astype(int)</span><br><span class="line">    <span class="comment"># 美国</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'UnitedStates')|(country == 'USA')")</span></span><br><span class="line">    <span class="comment">#求topn名</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(df)</span>:</span></span><br><span class="line">        top = df.sort_values([<span class="string">'year'</span>,<span class="string">'index_rank'</span>],ascending = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> top[:<span class="number">20</span>].reset_index()</span><br><span class="line">    df = df.groupby(by =[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line">    <span class="comment"># 更改列顺序</span></span><br><span class="line">    df = df[[<span class="string">'university'</span>,<span class="string">'index_rank_score'</span>,<span class="string">'index_rank'</span>,<span class="string">'year'</span>]]</span><br><span class="line">    <span class="comment"># 重命名列</span></span><br><span class="line">    df.rename (columns = &#123;<span class="string">'university'</span>:<span class="string">'name'</span>,<span class="string">'index_rank_score'</span>:<span class="string">'type'</span>,<span class="string">'index_rank'</span>:<span class="string">'value'</span>,<span class="string">'year'</span>:<span class="string">'date'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    df.to_csv(<span class="string">'university_ranking.csv'</span>,mode =<span class="string">'w'</span>,encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># index可以设置</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(year)</span>:</span></span><br><span class="line">    <span class="comment"># generate_mysql()</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2009</span>,year):  <span class="comment">#抓取10年</span></span><br><span class="line">        <span class="comment"># get_one_page(i)</span></span><br><span class="line">        html = get_one_page(i)</span><br><span class="line">        <span class="comment"># parse_one_page(html,i)  # 测试表格ok</span></span><br><span class="line">        tb = parse_one_page(html,i)</span><br><span class="line">        save_csv(tb)</span><br><span class="line">        print(i,<span class="string">'年排名提取完成完成'</span>)</span><br><span class="line">        analysis()</span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(<span class="number">2019</span>)</span><br><span class="line">    <span class="comment"># 2016-2018采用gb2312编码，2009-2015采用utf-8编码</span></span><br></pre></td></tr></table></figure><p>至此，我们已经有<code>university_ranking.csv</code>基础数据，下面就可以进行可视化呈现了。</p><h3 id="2-4-可视化呈现"><a href="#2-4-可视化呈现" class="headerlink" title="2.4. 可视化呈现"></a>2.4. 可视化呈现</h3><p>首先，到见齐的github主页：<br><a href="https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js" target="_blank" rel="noopener">https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</a> </p><h4 id="2-4-1-克隆仓库文件"><a href="#2-4-1-克隆仓库文件" class="headerlink" title="2.4.1. 克隆仓库文件"></a>2.4.1. 克隆仓库文件</h4><p>如果你平常使用github或者Git软件的话，那么就找个合适文件存放目录，然后直接在 GitBash里分别输入下面3条命令就搭建好环境了：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 克隆项目仓库</span><br><span class="line">git clone https:<span class="comment">//github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</span></span><br><span class="line"># 切换到项目根目录</span><br><span class="line">cd Historical-ranking-data-visualization-based-on-d3.js</span><br><span class="line"># 安装依赖</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>如果你此前没有用过上面的软件，你可以直接点击<code>Download Zip</code>下载下来然后解压即可，不过还是强烈建议使用第一种方法，因为后面如果要自定义可视化效果的话，需要修改代码然后执行<code>npm run build</code>命令才能够看到效果。</p><h4 id="2-4-2-效果呈现"><a href="#2-4-2-效果呈现" class="headerlink" title="2.4.2. 效果呈现"></a>2.4.2. 效果呈现</h4><p>好，所有基本准备都已完成，下面就可以试试看效果了。<br>任意浏览器打开<code>bargraph.html</code>网页，点击选择文件，然后选择：前面输出的<code>university_ranking.csv</code>文件，看下效果吧：<br><a href="https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0" target="_blank" rel="noopener">https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0</a></p><p>可以看到，有了大致的可视化效果，但还存在很多瑕疵，比如：表顺序颠倒了、字体不合适、配色太花哨等。可不可以修改呢？  </p><p>当然是可以的，只需要分别修改文件夹中这几个文件的参数就可以了：  </p><ul><li>config.js 全局设置各项功能的开关，比如配色、字体、文字名称、反转图表等等功能；</li><li>color.css 修改柱形图的配色；</li><li>stylesheet.css 具体修改配色、字体、文字名称等的css样式；</li><li>visual.js 更进一步的修改，比如图表的透明度等。</li></ul><p>知道在哪里修改了以后，那么，如何修改呢？很简单，只需要简单的几步就可以实现：  </p><ul><li><p>打开网页，<code>右键-检查</code>，箭头指向想要修改的元素，然后在右侧的css样式表里，双击各项参数修改参数，修改完元素就会发生变化，可以不断微调，直至满意为止。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-6/63524613.jpg" alt=""></p></li><li><p>把参数复制到四个文件中对应的文件里并保存。</p></li><li>Git Bash不断重复运行<code>npm run build</code>，之后刷新网页就可以看到优化后的效果。</li></ul><p>最后，再添加一个合适的BGM就可以了。以下是我优化之后的效果：<br><a href="https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0" target="_blank" rel="noopener">https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0</a><br><em>BGM：ツナ覚醒</em></p><p>如果你不太会调整，没有关系，我会分享优化后的配置文件。</p><p>以上，就是实现动态可视化表的步骤。 同样地，只要更改数据源可以很方便地做出世界、美国等大学的动态效果，可以看看：<br>中国（含港澳台）大学排名：<br><a href="http://pc1lljdwb.bkt.clouddn.com/Greater_China_uni_ranking.mp4" target="_blank" rel="noopener">http://pc1lljdwb.bkt.clouddn.com/Greater_China_uni_ranking.mp4</a><br>美国大学排名：<br><a href="http://pc1lljdwb.bkt.clouddn.com/USA_uni_ranking.mp4" target="_blank" rel="noopener">http://pc1lljdwb.bkt.clouddn.com/USA_uni_ranking.mp4</a></p><p>文章所有的素材，在公众号后台回复<strong>大学排名</strong>就可以得到，或者到我的github下载：<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a><br>感兴趣的话就动手试试吧。</p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python爬虫近十年中国大学Top20强并结合D3.js做动态数据可视化表。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://www.makcyun.top/categories/Python/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(4)：图片批量下载-以澎湃网信息图为例</title>
    <link href="https://www.makcyun.top/web_scraping_withpython4.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython4.html</id>
    <published>2018-09-02T00:00:57.468Z</published>
    <updated>2018-09-05T05:31:50.448Z</updated>
    
    <content type="html"><![CDATA[<p>澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。</p><a id="more"></a>  <p><strong>摘要：</strong> 上一篇文章介绍了单页图片的爬取，但是当爬取多页时，难度会增加。同时，前几篇爬虫文章中的网站有一个明显的特点是：可以通过点击鼠标实现网页的翻页，并且url会发生相应的变化。除了此类网站以外，还有一类非常常见的网站特点是：没有”下一页”这样的按钮，而是”加载更多”或者会不断自动刷新从而呈现出更多的内容，同时网页url也不发生变化。这种类型的网页通常采用的是Ajax技术，要抓取其中的网页内容需要采取一定的技巧。本文以信息图做得非常棒的澎湃”美数课”为例，抓取该栏目至今所有文章的图片。<br>栏目网址：<a href="https://www.thepaper.cn/list_25635" target="_blank" rel="noopener">https://www.thepaper.cn/list_25635</a></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/30853746.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/51553778.jpg" alt=""></p><p><strong>本文知识点：</strong>  </p><ul><li>Ajax知识</li><li>多页图片爬取</li></ul><h2 id="1-Ajax知识"><a href="#1-Ajax知识" class="headerlink" title="1. Ajax知识"></a>1. Ajax知识</h2><p>在该主页上尝试不断下拉，会发现网页不断地加载出新的文章内容来，而并不需要通过点击”下一页”来实现，而且网址url也保持不变。也就是说在同一个网页中通过下拉源源不断地刷新出了网页内容。这种形式的网页在今天非常常见，它们普遍是采用了<strong>Ajax</strong>技术。   </p><blockquote><p>Ajax 全称是 Asynchronous JavaScript and XML（异步 JavaScript 和 XML）。<br>它不是一门编程语言，而是利用 JavaScript 在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。</p></blockquote><p>Ajax更多参考：<br><a href="https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html</a></p><p>采用了Ajax的网页和普通的网页有一定的区别，普通网页的爬虫代码放在这种类型的网页上就行不通了，必须另辟出路。下面我们就来尝试一下如何爬取网易”数读”所有的文章。 </p><p>主页<code>右键-检查</code>，然后按<code>f5</code>刷新，会弹出很多链接文件。鼠标上拉回到第一个文件：<strong>list_25635</strong>，在右侧按<code>ctrl+f</code>搜索一下第一篇文章的标题：”娃娃机生意经”，可以看到在html网页中找到了对应的源代码。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/67533549.jpg" alt=""></p><p>接着，我们拖动下拉鼠标，显示出更多文章。然后再次搜索一篇文章的标题：”金砖峰会”，会发现搜不到相应的内容了。是不是感觉很奇怪？</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/72910700.jpg" alt=""></p><p>其实，这里就是用了Ajax的技术，和普通网页翻页是刷新整个网页不同，这种类型网页可以再保持url不变的前提下只刷新部分内容。这就为我们进行爬虫带来了麻烦。因为，我们通过解析网页的url：<code>https://www.thepaper.cn/list_25635</code>只能爬取前面部分的内容而后面通过下拉刷新出来的内容是爬取不到的。这显然不完美，那么怎么才能够爬取到后面不断刷新出来的网页内容呢？   </p><h2 id="2-url分析"><a href="#2-url分析" class="headerlink" title="2. url分析"></a>2. url分析</h2><p>我们把右侧的选项卡从<code>ALL</code>切换到<code>Network</code>，然后按再次按<code>f5</code>刷新，可以发现<code>Name</code>列有4个结果。选择第3个链接打开并点击<code>Response</code>，通过滑动可以看到一些文本内容和网页中的文章标题是一一对应的。比如第一个是：<strong>娃娃机生意经｜有没有好奇过抓娃娃机怎么又重新火起来了？</strong>，一直往下拖拽可以看到有很多篇文章。此时，再切换到headers选项卡，复制<code>Request URL</code>后面的链接并打开，会显示一部分文章的标题和图片内容。数一下的话，可以发现一共有20个文章标题，也就是对应着20篇文章。  </p><p>这个链接其实和上面的<strong>list_25635</strong>链接的内容是一致的。这样看来，好像发现不了什么东西，不过不要着急。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/40705043.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/642179.jpg" alt=""></p><p>接下来，回到<code>Name</code>列，尝试滚动下拉鼠标，会发现弹出好几个新的开头为<code>load_index</code>的链接来。选中第一个<code>load_index</code>的链接，点击<code>Response</code>查看一下html源代码，尝试在网页中搜索一下：<code>十年金砖峰</code>这个文章的标题，惊奇地发现，在网页中找到了对于的文章标题。而前面，我们搜索这个词时，是没有搜索到的。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/83879931.jpg" alt=""></p><p>这说明了什么呢？说明<code>十年金砖峰</code>这篇文章的内容不在第一个<strong>list_25635</strong>链接中，而在这个<code>load_index</code>的链接里。鼠标点击<code>headers</code>，复制<code>Request URL</code>后面的链接并打开，就可以再次看到包括这篇文章在内的新的20篇文章。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/4302921.jpg" alt=""></p><p>是不是发现了点了什么？接着，我们继续下拉，会发现弹出更多的<code>load_index</code>的链接。再搜索一个标题：<code>地图湃｜海外港口热</code>，可以发现在网页中也同样找到了文章标题。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/68563785.jpg" alt=""></p><p>回到我们的初衷：<strong>下载所有网页的图片内容</strong>。那么现在就有解决办法礼：一个个地把出现的这些url网址中图片下载下来就大功告成了。</p><p>好，我们先来分析一下这些url，看看有没有相似性，如果有很明显的相似性，那么就可以像普通网页那样，通过构造翻页页数的url，实现for循环就可以批量下载所有网页的图片了。复制前3个链接如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=2&amp;isList=true&amp;lastTime=1533169319712  </span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=3&amp;isList=true&amp;lastTime=1528625875167  </span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=4&amp;isList=true&amp;lastTime=1525499007926</span><br></pre></td></tr></table></figure></p><p>发现<code>pageidx</code>键的值呈现规律的数字递增变化，看起来是个好消息。但同时发现后面的lastTime键的值看起来是随机变化的，这个有没有影响呢？ 来测试一下，复制第一个链接，删掉<code>&amp;lastTime=1533169319712</code>这一串字符，会发现网页一样能够正常打开，就说明着一对参数不影响网页内容，那就太好了。我们可以删除掉，这样所有url的区别只剩<code>pageidx</code>的值了，这时就可以构造url来实现for循环了。构造的url形式如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=2</span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=3</span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=4</span><br></pre></td></tr></table></figure></p><p>同时，尝试把数字2改成1并打开链接看看会有什么变化，发现呈现的内容就是第1页的内容。这样，我们就可以从第一页开始构造url循环了。<br><code>https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=1</code>  </p><p>既然确定了首页，那么也要相应地确定一下尾页。很简单，我们把数字改大然后打开链接看是否有内容即可。比如改为<strong>10 </strong>，打开发现有内容显示，很好。接着，再改为30，发现没有内容了。说明该栏目的页数介于这两个数之间，尝试几次后，发现<code>25</code>是最后一个有内容的网页，也意味着能够爬取的页数一共是25页。  </p><p>确定了首页和尾页后，下面我们就可以开始构造链接，先爬取第一篇文章网页里的图片（这个爬取过程，我们上一篇爬取网易”数读”已经尝试过了），然后爬取这一整页的图片，最后循环25页，爬取所有图片，下面开始吧。</p><h2 id="3-程序代码"><a href="#3-程序代码" class="headerlink" title="3. 程序代码"></a>3. 程序代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 获取索引界面网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_index</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment"># 下载1页</span></span><br><span class="line">    <span class="comment"># url = 'https://www.thepaper.cn/newsDetail_forward_2370041'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2下载多页，构造url</span></span><br><span class="line">    paras = &#123;</span><br><span class="line">        <span class="string">'nodeids'</span>: <span class="number">25635</span>,</span><br><span class="line">        <span class="string">'pageidx'</span>: i</span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'https://www.thepaper.cn/load_index.jsp?'</span> + urlencode(paras)</span><br><span class="line"></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="comment"># print(response.text)  # 测试网页内容是否提取成功ok</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 解析索引界面网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_index</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每页文章数</span></span><br><span class="line">    num = soup.find_all(name = <span class="string">'div'</span>,class_=<span class="string">'news_li'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)):</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="comment"># 获取title</span></span><br><span class="line">        <span class="string">'title'</span>:soup.select(<span class="string">'h2 a'</span>)[i].get_text(),</span><br><span class="line">        <span class="comment"># 获取图片url，需加前缀</span></span><br><span class="line">        <span class="string">'url'</span>:<span class="string">'https://www.thepaper.cn/'</span> + soup.select(<span class="string">'h2 a'</span>)[i].attrs[<span class="string">'href'</span>]</span><br><span class="line">        <span class="comment"># print(url)  # 测试图片链接</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 获取每条文章的详情页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_detail</span><span class="params">(item)</span>:</span></span><br><span class="line">    url = item.get(<span class="string">'url'</span>)</span><br><span class="line">    <span class="comment"># 增加异常捕获语句</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,headers = headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="comment"># print(response.text)  # 测试网页内容是否提取成功</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        print(<span class="string">'网页请求失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 4 解析每条文章的详情页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_detail</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># 获取title</span></span><br><span class="line">    <span class="keyword">if</span> soup.h1:  <span class="comment">#有的网页没有h1节点，因此必须要增加判断，否则会报错</span></span><br><span class="line">        title = soup.h1.string</span><br><span class="line">        <span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">        items = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>,<span class="string">'600'</span>])</span><br><span class="line">        <span class="comment"># 有的图片节点用width='100%'表示，有的用600表示，因此用list合并选择</span></span><br><span class="line">        <span class="comment"># https://blog.csdn.net/w_xuechun/article/details/76093950</span></span><br><span class="line">        <span class="comment"># print(items) # 测试返回的img节点ok</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(items)):</span><br><span class="line">            pic = items[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">            <span class="comment"># print(pic) #测试图片链接ok</span></span><br><span class="line">            <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'title'</span>:title,</span><br><span class="line">            <span class="string">'pic'</span>:pic,</span><br><span class="line">            <span class="string">'num'</span>:i  <span class="comment"># 图片添加编号顺序</span></span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment"># 5 下载图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pic</span><span class="params">(pic)</span>:</span></span><br><span class="line">    title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">    <span class="comment"># 标题规范命名：去掉符号非法字符| 等</span></span><br><span class="line">     title = re.sub(<span class="string">'[\/:*?"&lt;&gt;|]'</span>,<span class="string">'-'</span>,title).strip()</span><br><span class="line">    url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line">    <span class="comment"># 设置图片编号顺序</span></span><br><span class="line">    num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">        os.mkdir(title)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片url网页信息</span></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 建立图片存放地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line">            <span class="comment"># 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest()</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="comment"># 开始下载图片</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">                    print(<span class="string">'文章"&#123;0&#125;"的第&#123;1&#125;张图片下载完成'</span> .format(title,num))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'该图片%s 已下载'</span> %title)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e,<span class="string">'图片获取失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment"># get_page_index(i) # 测试索引界面网页内容是否获取成功ok</span></span><br><span class="line"></span><br><span class="line">    html = get_page_index(i)</span><br><span class="line">    data = parse_page_index(html)  <span class="comment"># 测试索引界面url是否获取成功ok</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># print(item)  #测试返回的dict</span></span><br><span class="line">        html = get_page_detail(item)</span><br><span class="line">        data = parse_page_detail(html)</span><br><span class="line">        <span class="keyword">for</span> pic <span class="keyword">in</span> data:</span><br><span class="line">            save_pic(pic)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>):</span><br><span class="line">        main(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    pool.map(main,[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>)])</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/46673865.jpg" alt="">  </p><p><img src="http://pbscl931v.bkt.clouddn.com/18-9-3/4372426.jpg" alt="">  </p><p>文章代码和栏目从2015年至今437篇文章共1509张图片资源，可在下方链接中得到。<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫(3)：单页图片下载-网易&quot;数读&quot;信息图</title>
    <link href="https://www.makcyun.top/web_scraping_withpython3.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython3.html</id>
    <published>2018-09-01T06:49:30.854Z</published>
    <updated>2018-09-03T05:00:59.054Z</updated>
    
    <content type="html"><![CDATA[<p>下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。</p><a id="more"></a>  <p><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/65507772.jpg" alt="">  </p><p><strong>本文知识点：</strong>  </p><ul><li>单张图片下载</li><li>单页图片下载  </li><li>Ajax技术介绍</li></ul><h2 id="1-单张图片下载"><a href="#1-单张图片下载" class="headerlink" title="1. 单张图片下载"></a>1. 单张图片下载</h2><p>以一篇最近比较热的涨房价的文章为例：<a href="http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html" target="_blank" rel="noopener">暴涨的房租，正在摧毁中国年轻人的生活</a>，从文章里随意挑选一张<a href="http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png" target="_blank" rel="noopener">北京房租地图图片</a>，通过<strong>Requests的content属性</strong>来实现单张图片的下载。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/14528855.jpg" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'北京房租地图.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure></p><p>5行代码就能将这张图片下载到电脑上。不只是该张图片，任意图片都可以下载，只要替换图片的url即可。<br>这里用到了<strong>Requests的content属性</strong>，将图片存储为二进制数据。至于，图片为什么可以用二进制数据进行存储，可以参考这个教程：<br><a href="https://www.zhihu.com/question/36269548/answer/66734582" target="_blank" rel="noopener">https://www.zhihu.com/question/36269548/answer/66734582</a></p><p>5行代码看起来很短，但如果只是下载一张图片显然没有必要写代码，”右键另存为”更快。现在，我们放大一下范围，去下载这篇文章中的所有图片。粗略数一下，网页里有超过15张图片，这时，如果再用”右键另存为”的方法，显然就比较繁琐了。下面，我们用代码来实现下载该网页中的所有图片。</p><h2 id="2-单页图片下载"><a href="#2-单页图片下载" class="headerlink" title="2. 单页图片下载"></a>2. 单页图片下载</h2><h3 id="2-1-Requests获取网页内容"><a href="#2-1-Requests获取网页内容" class="headerlink" title="2.1. Requests获取网页内容"></a>2.1. Requests获取网页内容</h3><p>首先，用堪称python”爬虫利器”的<strong>Requests库</strong>来获取该篇文章的html内容。<br>Requests库可以说是一款python爬虫的利器，它的更多用法，可参考下面的教程：<br><a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/index.html</a><br><a href="https://cuiqingcai.com/2556.html" target="_blank" rel="noopener">https://cuiqingcai.com/2556.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">url = <span class="string">'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'</span></span><br><span class="line"></span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="comment"># return response.text</span></span><br><span class="line">    print(response.text)  <span class="comment"># 测试网页内容是否提取成功ok</span></span><br></pre></td></tr></table></figure><h3 id="2-2-解析网页内容"><a href="#2-2-解析网页内容" class="headerlink" title="2.2. 解析网页内容"></a>2.2. 解析网页内容</h3><p>通过上面方法可以获取到html内容，接下来解析html字符串内容，从中提取出网页内的图片url。解析和提取url的方法有很多种，常见的有5种，分别是：正则表达式、Xpath、BeautifulSoup、CSS、PyQuery。任选一种即可，这里为了再次加强练习，5种方法全部尝试一遍。<br>首先，在网页中定位到图片url所在的位置，如下图所示：<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/91340067.jpg" alt=""></p><p>从外到内定位url的位置：<code>&lt;p&gt;节点-&lt;a&gt;节点-&lt;img&gt;节点里的src属性值</code>。   </p><h4 id="2-2-1-正则表达式"><a href="#2-2-1-正则表达式" class="headerlink" title="2.2.1. 正则表达式"></a>2.2.1. 正则表达式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern =re.compile(<span class="string">'&lt;p&gt;.*?&lt;img alt="房租".*?src="(.*?)".*?style'</span>,re.S)</span><br><span class="line">    items = re.findall(pattern,html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行结果如下:<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/425eca61322a4f99837988bb78a001ac.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/d6cb58a6bb014b8683b232f3c00f0e39.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/88d2e535765a4ed09e03877238647aa5.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/09/01/98d2f9579e9e49aeb76ad6155e8fc4ea.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7410ed4041a94cab8f30e8de53aaaaa1.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/49a0c80a140b4f1aa03724654c5a39af.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3070964278bf4637ba3d92b6bb771cea.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/812b7a51475246a9b57f467940626c5c.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/8bcbc7d180f74397addc74e47eaa1f63.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/e593efca849744489096a77aafd10d3e.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7653feecbfd94758a8a0ff599915d435.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/edbaa24a17dc4cca9430761bfc557ffb.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/f768d440d9f14b8bb58e3c425345b97e.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3430043fd305411782f43d3d8635d632.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/111ba73d11084c68b8db85cdd6d474a7.png'&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-Xpath语法"><a href="#2-2-2-Xpath语法" class="headerlink" title="2.2.2. Xpath语法"></a>2.2.2. Xpath语法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'*//p//img[@alt = "房租"]/@src'</span>)</span><br><span class="line">    print(items)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>结果同上。</p><h4 id="2-2-3-CSS选择器"><a href="#2-2-3-CSS选择器" class="headerlink" title="2.2.3. CSS选择器"></a>2.2.3. CSS选择器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = soup.select(<span class="string">'p &gt; a &gt; img'</span>) <span class="comment">#&gt;表示下级绝对节点</span></span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item[<span class="string">'src'</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="2-2-4-BeautifulSoup-find-all方法"><a href="#2-2-4-BeautifulSoup-find-all方法" class="headerlink" title="2.2.4. BeautifulSoup find_all方法"></a>2.2.4. BeautifulSoup find_all方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"><span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">item = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(item)):</span><br><span class="line">    url = item[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'url'</span>:url</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># print(pic) #测试图片链接ok</span></span><br></pre></td></tr></table></figure><h4 id="2-2-5-PyQuery"><a href="#2-2-5-PyQuery" class="headerlink" title="2.2.5. PyQuery"></a>2.2.5. PyQuery</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">data = pq(html)</span><br><span class="line">data2 = data(<span class="string">'p &gt; a &gt; img'</span>)</span><br><span class="line"><span class="comment"># print(items)</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> data2.items():   <span class="comment">#注意这里和BeautifulSoup 的css用法不同</span></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'url'</span>:item.attr(<span class="string">'src'</span>)</span><br><span class="line">    <span class="comment"># 或者'url':item.attr.src</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>以上用了5种方法提取出了该网页的url地址，任选一种即可。这里假设选择了第4种方法，接下来就可以下载图片了。提取出的网址是一个<strong>dict字典</strong>，通过dict的get方法调用里面的键和值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line"><span class="comment"># 设置图片编号顺序</span></span><br><span class="line">num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">    os.mkdir(title)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图片url网页信息</span></span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立图片存放地址</span></span><br><span class="line">file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line"><span class="comment"># 文件名采用编号方便按顺序查看</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始下载图片</span></span><br><span class="line"><span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">   f.write(response.content)</span><br><span class="line">   print(<span class="string">'该图片已下载完成'</span>,title)</span><br></pre></td></tr></table></figure></p><p>很快，15张图片就按着文章的顺序下载下来了。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/94009879.jpg" alt=""></p><p>将上述代码整理一下，增加一点异常处理和图片的标题、编号的代码以让爬虫更健壮，完整的代码如下所示：</p><h3 id="2-3-全部代码"><a href="#2-3-全部代码" class="headerlink" title="2.3. 全部代码"></a>2.3. 全部代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 下载1页</span></span><br><span class="line">    url = <span class="string">'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加异常捕获语句</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,headers = headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="comment"># print(response.text)  # 测试网页内容是否提取成功</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        print(<span class="string">'网页请求失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># 获取title</span></span><br><span class="line">    title = soup.h1.string</span><br><span class="line">    <span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">    item = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>])</span><br><span class="line">    <span class="comment"># print(item) # 测试</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(item)):</span><br><span class="line">        pic = item[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'title'</span>:title,</span><br><span class="line">        <span class="string">'pic'</span>:pic,</span><br><span class="line">        <span class="string">'num'</span>:i  <span class="comment"># 图片添加编号顺序</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># print(pic) #测试图片链接ok</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pic</span><span class="params">(pic)</span>:</span></span><br><span class="line">    title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">    url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line">    <span class="comment"># 设置图片编号顺序</span></span><br><span class="line">    num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">        os.mkdir(title)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片url网页信息</span></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 建立图片存放地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line">            <span class="comment"># 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest()</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="comment"># 开始下载图片</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">                    print(<span class="string">'该图片已下载完成'</span>,title)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'该图片%s 已下载'</span> %title)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e,<span class="string">'图片获取失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># get_page() # 测试网页内容是获取成功ok</span></span><br><span class="line">    html = get_page()</span><br><span class="line">    <span class="comment"># parse_page(html) # 测试网页内容是否解析成功ok</span></span><br><span class="line"></span><br><span class="line">    data = parse_page(html)</span><br><span class="line">    <span class="keyword">for</span> pic <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># print(pic) #测试dict</span></span><br><span class="line">        save_pic(pic)</span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>小结</strong><br>上面通过爬虫实现下载一张图片延伸到下载一页图片，相比于手动操作，爬虫的优势逐渐显现。那么，能否实现多页循环批量下载更多的图片呢，当然可以，下一篇文章将进行介绍。  </p><p>你也可以尝试一下，这里先放上”福利”：网易”数度”栏目从2012年至今350篇文章的全部图片已下载完成。<br><img src="http://pbscl931v.bkt.clouddn.com/18-9-2/90267464.jpg" alt=""></p><p>如果你需要，可以到我的github下载。<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a>  </p><p>本文完。  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(2)：表格型数据抓取</title>
    <link href="https://www.makcyun.top/web_scraping_withpython2.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython2.html</id>
    <published>2018-08-27T00:26:57.000Z</published>
    <updated>2018-09-02T00:14:20.328Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>python爬虫第2篇</em></b><br>利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。</p><a id="more"></a>  <p><strong>摘要：</strong> 我们平常在浏览网页中会遇到一些表格型的数据信息，除了表格本身体现的内容以外，你可能想透过表格再更进一步地进行汇总、筛选、处理分析等操作从而得到更多有价值的信息，这时可用python爬虫来实现。本文采用pandas库中的read_html方法来快速准确地抓取表格数据。</p><p><strong>本文知识点：</strong>  </p><ul><li>Table型表格抓取</li><li>DataFrame.read_html函数使用  </li><li>爬虫数据存储到mysql数据库</li><li>Navicat数据库的使用</li></ul><h2 id="1-table型表格"><a href="#1-table型表格" class="headerlink" title="1. table型表格"></a>1. table型表格</h2><p>我们在网页上会经常看到这样一些表格，比如：<br><a href="http://ranking.promisingedu.com/qs" target="_blank" rel="noopener">QS2018世界大学排名</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/59439970.jpg" alt=""></p><p><a href="http://www.fortunechina.com/fortune500/c/2018-07/19/content_311046.htm" target="_blank" rel="noopener">财富世界500强企业排名</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/66712901.jpg" alt=""></p><p><a href="https://www.boxofficemojo.com/" target="_blank" rel="noopener">IMDB世界电影票房排行榜</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/76002510.jpg" alt=""></p><p><a href="http://pbscl931v.bkt.clouddn.com/18-8-27/78659021.jpg" target="_blank" rel="noopener">中国上市公司信息</a>：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/78659021.jpg" alt=""></p><p>他们除了都是表格以外，还一个共同点就是当你点击右键-定位时，可以看到他们都是table类型的表格形式。<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/87245193.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/54573575.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/21054545.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/30765316.jpg" alt=""></p><p>从中可以看到table类型的表格网页结构大致如下：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">class</span>=<span class="string">"..."</span> <span class="attr">id</span>=<span class="string">"..."</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">th</span>&gt;</span>...<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tbody</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>...<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">tbody</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>先来简单解释一下上文出现的几种标签含义：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>: 定义表格</span><br><span class="line"><span class="tag">&lt;<span class="name">thead</span>&gt;</span>: 定义表格的页眉</span><br><span class="line"><span class="tag">&lt;<span class="name">tbody</span>&gt;</span>: 定义表格的主体</span><br><span class="line"><span class="tag">&lt;<span class="name">tr</span>&gt;</span>: 定义表格的行</span><br><span class="line"><span class="tag">&lt;<span class="name">th</span>&gt;</span>: 定义表格的表头</span><br><span class="line"><span class="tag">&lt;<span class="name">td</span>&gt;</span>: 定义表格单元</span><br></pre></td></tr></table></figure></p><p>这样的表格数据，就可以利用pandas模块里的read_html函数方便快捷地抓取下来。下面我们就来操作一下。</p><h2 id="2-快速抓取"><a href="#2-快速抓取" class="headerlink" title="2. 快速抓取"></a>2. 快速抓取</h2><p>下面以中国上市公司信息这个网页中的表格为例，感受一下read_html函数的强大之处。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">178</span>):  <span class="comment"># 爬取全部177页数据</span></span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s'</span> % (str(i))</span><br><span class="line">tb = pd.read_html(url)[<span class="number">3</span>] <span class="comment">#经观察发现所需表格是网页中第4个表格，故为[3]</span></span><br><span class="line">tb.to_csv(<span class="string">r'1.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="number">1</span>, index=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'第'</span>+str(i)+<span class="string">'页抓取完成'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-27/96662344.jpg" alt=""><br>只需不到十行代码，1分钟左右就可以将全部178页共3536家A股上市公司的信息干净整齐地抓取下来。比采用正则表达式、xpath这类常规方法要省心省力地多。如果采取人工一页页地复制粘贴到excel中，就得操作到猴年马月去了。<br>上述代码除了能爬上市公司表格以外，其他几个网页的表格都可以爬，只需做简单的修改即可。因此，可作为一个简单通用的代码模板。但是，为了让代码更健壮更通用一些，接下来，以爬取177页的A股上市公司信息为目标，讲解一下详细的代码实现步骤。</p><h2 id="3-详细代码实现"><a href="#3-详细代码实现" class="headerlink" title="3. 详细代码实现"></a>3. 详细代码实现</h2><h3 id="3-1-read-html函数"><a href="#3-1-read-html函数" class="headerlink" title="3.1. read_html函数"></a>3.1. read_html函数</h3><p>先来了解一下<strong>read_html</strong>函数的api:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pandas.read_html(io, match=<span class="string">'.+'</span>, flavor=<span class="keyword">None</span>, header=<span class="keyword">None</span>, index_col=<span class="keyword">None</span>, skiprows=<span class="keyword">None</span>, attrs=<span class="keyword">None</span>, parse_dates=<span class="keyword">False</span>, tupleize_cols=<span class="keyword">None</span>, thousands=<span class="string">', '</span>, encoding=<span class="keyword">None</span>, decimal=<span class="string">'.'</span>, converters=<span class="keyword">None</span>, na_values=<span class="keyword">None</span>, keep_default_na=<span class="keyword">True</span>, displayed_only=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">常用的参数：</span><br><span class="line">io:可以是url、html文本、本地文件等；</span><br><span class="line">flavor：解析器；</span><br><span class="line">header：标题行；</span><br><span class="line">skiprows：跳过的行；</span><br><span class="line">attrs：属性，比如 attrs = &#123;<span class="string">'id'</span>: <span class="string">'table'</span>&#125;；</span><br><span class="line">parse_dates：解析日期</span><br><span class="line"></span><br><span class="line">注意：返回的结果是**DataFrame**组成的**list**。</span><br></pre></td></tr></table></figure></p><p>参考：</p><blockquote><p>1 <a href="http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html</a><br>2 <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html</a></p></blockquote><h3 id="3-2-分析网页url"><a href="#3-2-分析网页url" class="headerlink" title="3.2. 分析网页url"></a>3.2. 分析网页url</h3><p>首先，观察一下中商情报网第1页和第2页的网址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=1#QueryCondition</span><br><span class="line">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=2#QueryCondition</span><br></pre></td></tr></table></figure></p><p>可以发现，只有<strong>pageNum</strong>的值随着翻页而变化，所以基本可以断定pageNum=1代表第1页，pageNum=10代表第10页，以此类推。这样比较容易用for循环构造爬取的网址。<br>试着把<strong>#QueryCondition</strong>删除，看网页是否同样能够打开，经尝试发现网页依然能正常打开，因此在构造url时，可以使用这样的格式：<br><strong><a href="http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i" target="_blank" rel="noopener">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i</a></strong><br>再注意一下其他参数：<br><strong>a</strong>：表示A股，把a替换为<strong>h</strong>，表示<strong>港股</strong>；把a替换为<strong>xsb</strong>，则表示<strong>新三板</strong>。那么，在网址分页for循环外部再加一个for循环，就可以爬取这三个股市的股票了。  </p><h3 id="3-3-定义函数"><a href="#3-3-定义函数" class="headerlink" title="3.3. 定义函数"></a>3.3. 定义函数</h3><p>将整个爬取分为网页提取、内容解析、数据存储等步骤，依次建立相应的函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网页提取函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,   </span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># beatutiful soup解析然后提取表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line"></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(tbl)</span><br><span class="line"><span class="comment"># return tbl</span></span><br><span class="line"><span class="comment"># rename将表格15列的中文名改为英文名，便于存储到mysql及后期进行数据分析</span></span><br><span class="line"><span class="comment"># tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):   <span class="comment"># page表示提取页数</span></span><br><span class="line">html = get_one_page(i)</span><br><span class="line">parse_one_page(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)   <span class="comment">#共提取n页</span></span><br></pre></td></tr></table></figure></p><p>上面两个函数相比于快速抓取的方法代码要多一些，如果需要抓的表格很少或只需要抓一次，那么推荐快速抓取法。如果页数比较多，这种方法就更保险一些。解析函数用了BeautifulSoup和css选择器，这种方法定位提取表格所在的<strong>id为#myTable04</strong>的table代码段，更为准确。</p><h3 id="3-4-存储到MySQL"><a href="#3-4-存储到MySQL" class="headerlink" title="3.4. 存储到MySQL"></a>3.4. 存储到MySQL</h3><p>接下来，我们可以将结果保存到本地csv文件，也可以保存到MySQL数据库中。这里为了练习一下MySQL，因此选择保存到MySQL中。</p><p>首先，需要先在数据库建立存放数据的表格，这里命名为<strong>listed_company</strong>。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,   <span class="comment"># 本地服务器</span></span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,  <span class="comment"># 你的数据库密码</span></span><br><span class="line">port=<span class="number">3306</span>,          <span class="comment"># 默认端口</span></span><br><span class="line">charset = <span class="string">'utf8'</span>,</span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company2 (serial_number INT(30) NOT NULL,stock_code INT(30) ,stock_abbre VARCHAR(30) ,company_name VARCHAR(30) ,province VARCHAR(30) ,city VARCHAR(30) ,main_bussiness_income VARCHAR(30) ,net_profit VARCHAR(30) ,employees INT(30) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(30) ,financial_report VARCHAR(30) , industry_classification VARCHAR(255) ,industry_type VARCHAR(255) ,main_business VARCHAR(255) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">    <span class="comment"># listed_company是要在wade数据库中建立的表，用于存放数据</span></span><br><span class="line">    </span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line">generate_mysql()</span><br></pre></td></tr></table></figure></p><p>上述代码定义了generate_mysql()函数，用于在MySQL中wade数据库下生成一个listed_company的表。表格包含15个列字段。根据每列字段的属性，分别设置为INT整形（长度为30）、VARCHAR字符型(长度为30) 、DATETIME(0) 日期型等。<br>在Navicat中查看建立好之后的表格：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/33076915.jpg" alt=""><br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/97554452.jpg" alt=""></p><p>接下来就可以往这个表中写入数据，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="comment"># db = 'wade'表示存储到wade这个数据库中,root后面的*是密码</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># 因为要循环网页不断数据库写入内容，所以if_exists选择append，同时该表要有表头，parse_one_page（）方法中df.rename已设置</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure></p><p>以上就完成了单个页面的表格爬取和存储工作，接下来只要在main()函数进行for循环，就可以完成所有总共178页表格的爬取和存储，完整代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode  <span class="comment"># 编码 URL 字符串</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,</span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(tbl)</span></span><br><span class="line"><span class="keyword">return</span> tbl</span><br><span class="line"><span class="comment"># rename将中文名改为英文名，便于存储到mysql及后期进行数据分析</span></span><br><span class="line"><span class="comment"># tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,</span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,</span><br><span class="line">port=<span class="number">3306</span>,</span><br><span class="line">charset = <span class="string">'utf8'</span>,  </span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">    <span class="comment"># listed_company是要在wade数据库中建立的表，用于存放数据</span></span><br><span class="line">    </span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># df = pd.read_csv(df)</span></span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company2'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># append表示在原有表基础上增加，但该表要有表头</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    generate_mysql()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):  </span><br><span class="line">html = get_one_page(i)</span><br><span class="line">tbl = parse_one_page(html)</span><br><span class="line">write_to_sql(tbl)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)</span><br><span class="line"></span><br><span class="line">endtime = time.time()-start_time</span><br><span class="line">print(<span class="string">'程序运行了%.2f秒'</span> %endtime)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="comment"># from multiprocessing import Pool</span></span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment"># pool = Pool(4)</span></span><br><span class="line"><span class="comment"># pool.map(main, [i for i in range(1,178)])  #共有178页</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># endtime = time.time()-start_time</span></span><br><span class="line"><span class="comment"># print('程序运行了%.2f秒' %(time.time()-start_time))</span></span><br></pre></td></tr></table></figure></p><p>最终，A股所有3535家企业的信息已经爬取到mysql中，如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/63973864.jpg" alt=""></p><p>最后，需说明不是所有表格都可以用这种方法爬取，比如这个网站中的表格，表面是看起来是表格，但在html中不是前面的table格式，而是list列表格式。这种表格则不适用read_html爬取。得用其他的方法，比如selenium，以后再进行介绍。<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-28/3402980.jpg" alt=""></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;python爬虫第2篇&lt;/em&gt;&lt;/b&gt;&lt;br&gt;利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="pandas" scheme="https://www.makcyun.top/tags/pandas/"/>
    
      <category term="数据抓取" scheme="https://www.makcyun.top/tags/%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(1):多种方法爬取猫眼top100电影</title>
    <link href="https://www.makcyun.top/web_scraping_withpython1.html"/>
    <id>https://www.makcyun.top/web_scraping_withpython1.html</id>
    <published>2018-08-20T11:18:14.973Z</published>
    <updated>2018-09-01T07:32:56.056Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>python爬虫第1篇</em></b><br>利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。</p><a id="more"></a>  <p><strong>摘要：</strong> 作为小白，<strong>爬虫可以说是入门python最快和最容易获得成就感的途径</strong>。因为初级爬虫的套路相对固定，常见的方法只有几种，比较好上手。最近，跟着崔庆才大佬的书：<em>python3网络爬虫开发实战</em> 学习爬虫。选取网页结构较为简单的猫眼top100电影为案例进行练习。 <strong>重点是用上述所说的4种方法提取出关键内容</strong>。一个问题采用不同的解决方法有助于拓展思维，通过不断练习就能够灵活运用。</p><blockquote><p><strong>本文知识点：</strong><br>Requsts 请求库的使用<br>beautiful+lxml两大解析库使用<br>正则表达式 、xpath、css选择器的使用  </p></blockquote><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/49818413.jpg" alt=""></p><h2 id="1-为什么爬取该网页？"><a href="#1-为什么爬取该网页？" class="headerlink" title="1. 为什么爬取该网页？"></a>1. 为什么爬取该网页？</h2><ul><li>比较懒，不想一页页地去翻100部电影的介绍，<strong>想在一个页面内进行总体浏览</strong>（比如在excel表格中）；</li></ul><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/28479553.jpg" alt=""></p><ul><li>想<strong>深入了解一些比较有意思的信息</strong>，比如：哪部电影的评分最高？哪位演员的作品数量最多？哪个国家/地区上榜的电影数量最多？哪一年上榜的电影作品最多等。这些信息在网页上是不那么容易能直接获得的，所以需要爬虫。</li></ul><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/66062822.jpg" alt=""></p><h2 id="2-爬虫目标"><a href="#2-爬虫目标" class="headerlink" title="2. 爬虫目标"></a>2. 爬虫目标</h2><ul><li>从网页中提取出top100电影的电影名称、封面图片、排名、评分、演员、上映国家/地区、评分等信息，并保存为csv文本文件。</li><li>根据爬取结果，进行简单的可视化分析。</li></ul><p>平台：windows7 + SublimeText3</p><h2 id="3-爬取步骤"><a href="#3-爬取步骤" class="headerlink" title="3. 爬取步骤"></a>3. 爬取步骤</h2><h3 id="3-1-网址URL分析"><a href="#3-1-网址URL分析" class="headerlink" title="3.1. 网址URL分析"></a>3.1. 网址URL分析</h3><p>首先，打开猫眼Top100的url网址： <a href="http://maoyan.com/board/4?offset=0" target="_blank" rel="noopener"><strong>http://maoyan.com/board/4?offset=0</strong></a>。页面非常简单，所包含的信息就是上述所说的爬虫目标。下拉页面到底部，点击第2页可以看到网址变为：<strong><a href="http://maoyan.com/board/4?offset=10" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10</a></strong>。因此，可以推断出url的变化规律：offset表示偏移，10代表一个页面的电影偏移数量，即：第一页电影是从0-10，第二页电影是从11-20。因此，获取全部100部电影，只需要构造出10个url，然后依次获取网页内容，再用不同的方法提取出所需内容就可以了。<br>下面，用requests方法获取第一个页面。</p><h3 id="3-2-Requests获取首页数据"><a href="#3-2-Requests获取首页数据" class="headerlink" title="3.2. Requests获取首页数据"></a>3.2. Requests获取首页数据</h3><p>先定义一个获取单个页面的函数：get_one_page()，传入url参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line">        <span class="comment"># 不加headers爬不了</span></span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># try-except语句捕获异常</span></span><br></pre></td></tr></table></figure><p>接下来在main()函数中设置url。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    print(html)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>运行上述程序后，首页的源代码就被爬取下来了。如下图所示：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/18415362.jpg" alt=""></p><p>接下来就需要从整个网页中提取出几项我们需要的内容，用到的方法就是上述所说的四种方法，下面分别进行说明。</p><h3 id="3-3-4种内容解析提取方法"><a href="#3-3-4种内容解析提取方法" class="headerlink" title="3.3. 4种内容解析提取方法"></a>3.3. 4种内容解析提取方法</h3><h4 id="3-3-1-正则表达式提取"><a href="#3-3-1-正则表达式提取" class="headerlink" title="3.3.1. 正则表达式提取"></a>3.3.1. 正则表达式提取</h4><p>第一种是利用<strong>正则表达式</strong>提取。<br>什么是正则表达式？ 下面这串看起来乱七八糟的符号就是正则表达式的语法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&apos;</span><br></pre></td></tr></table></figure><p>它是一种强大的字符串处理工具。之所以叫正则表达式，是因为它们可以识别正则字符串（regular string）。可以这么定义：“ 如果你给我的字符串符合规则，我就返回它”；“如果字符串不符合规则，我就忽略它”。通过requests抓取下来的网页是一堆大量的字符串，用它处理后便可提取出我们想要的内容。</p><p>如果还不了解它，可以参考下面的教程：</p><blockquote><p><a href="http://www.runoob.com/regexp/regexp-syntax.html" target="_blank" rel="noopener">http://www.runoob.com/regexp/regexp-syntax.html</a><br><a href="https://www.w3cschool.cn/regexp/zoxa1pq7.html" target="_blank" rel="noopener">https://www.w3cschool.cn/regexp/zoxa1pq7.html</a></p></blockquote><p><strong>正则表达式常用语法：</strong></p><style>table th:nth-of-type(1) {    width: 60px;}</style><table><thead><tr><th style="text-align:center">模式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">\w</td><td style="text-align:center">匹配字母数字及下划线</td></tr><tr><td style="text-align:center">\W</td><td style="text-align:center">匹配非字母数字及下划线</td></tr><tr><td style="text-align:center">\s</td><td style="text-align:center">匹配任意空白字符，等价于 [\t\n\r\f]</td></tr><tr><td style="text-align:center">\S</td><td style="text-align:center">匹配任意非空字符</td></tr><tr><td style="text-align:center">\d</td><td style="text-align:center">匹配任意数字，等价于 [0-9]</td></tr><tr><td style="text-align:center">\D</td><td style="text-align:center">匹配任意非数字</td></tr><tr><td style="text-align:center">\n</td><td style="text-align:center">匹配一个换行符</td></tr><tr><td style="text-align:center">\t</td><td style="text-align:center">匹配一个制表符</td></tr><tr><td style="text-align:center">^</td><td style="text-align:center">匹配字符串开始位置的字符</td></tr><tr><td style="text-align:center">$</td><td style="text-align:center">匹配字符串的末尾</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">匹配任意字符，除了换行符</td></tr><tr><td style="text-align:center">[…]</td><td style="text-align:center">用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’</td></tr><tr><td style="text-align:center">[^…]</td><td style="text-align:center">不在 [ ] 中的字符</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">匹配前面的字符、子表达式或括号里的字符 0 次或多次</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">同上，匹配至少一次</td></tr><tr><td style="text-align:center">?</td><td style="text-align:center">同上，匹配0到1次</td></tr><tr><td style="text-align:center">{n}</td><td style="text-align:center">匹配前面的字符、子表达式或括号里的字符 n 次</td></tr><tr><td style="text-align:center">{n, m}</td><td style="text-align:center">同上，匹配 m 到n 次（包含 m 或 n）</td></tr><tr><td style="text-align:center">( )</td><td style="text-align:center">匹配括号内的表达式，也表示一个组</td></tr></tbody></table><p>下面，开始提取关键内容。右键网页-检查-Network选项，选中左边第一个文件然后定位到电影信息的相应位置，如下图：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/84055565.jpg" alt=""></p><p>可以看到每部电影的相关信息都在<strong>dd</strong>这个节点之中。所以就可以从该节点运用正则进行提取。<br>第1个要提取的内容是电影的排名。它位于class=”board-index”的<strong>i</strong>节点内。不需要提取的内容用’.*?’替代，需要提取的数字排名用（）括起来，（）里面的数字表示为（\d+）。正则表达式可以写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;&apos;</span><br></pre></td></tr></table></figure><p>接着，第2个需要提取的是封面图片，图片网址位于img节点的’data-src’属性中，正则表达式可写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;data-src=&quot;(.*?)&quot;.*?&apos;</span><br></pre></td></tr></table></figure><p>第1和第2个正则之间的代码是不需要的，用’.*?’替代，所以这两部分合起来写就是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;</span><br></pre></td></tr></table></figure><p>同理，可以依次用正则写下主演、上映时间和评分等内容,完整的正则表达式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;</span><br></pre></td></tr></table></figure><p>正则表达式写好以后，可以定义一个页面解析提取方法：parse_one_page（），用来提取内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(</span><br><span class="line">        <span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>, re.S)</span><br><span class="line">    <span class="comment"># re.S表示匹配任意字符，如果不加，则无法匹配换行符</span></span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'index'</span>: item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(item[<span class="number">1</span>]),  <span class="comment"># 定义get_thumb()方法进一步处理网址</span></span><br><span class="line">            <span class="string">'name'</span>: item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'star'</span>: item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="comment"># 'time': item[4].strip()[5:],</span></span><br><span class="line">            <span class="comment"># 用两个方法分别提取time里的日期和地区</span></span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: item[<span class="number">5</span>].strip() + item[<span class="number">6</span>].strip()</span><br><span class="line">            <span class="comment"># 评分score由整数+小数两部分组成</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>re.S:</strong>匹配任意字符，如果不加，则无法匹配换行符；<br><strong>yield:</strong>使用yield的好处是作为生成器，可以遍历迭代，并且将数据整理形成字典，输出结果美观。具体用法可参考：<a href="https://blog.csdn.net/zhangpinghao/article/details/18716275；" target="_blank" rel="noopener">https://blog.csdn.net/zhangpinghao/article/details/18716275；</a><br><strong>.strip():</strong>用于去掉字符串中的空格。</p></blockquote><p>上面程序为了便于提取内容，又定义了3个方法：get_thumb（）、get_release_time（）和 get_release_area（）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取封面大图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_thumb</span><span class="params">(url)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)@.*?'</span>)</span><br><span class="line">    thumb = re.search(pattern, url)</span><br><span class="line">    <span class="keyword">return</span> thumb.group(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c</span></span><br><span class="line"><span class="comment"># 去掉@160w_220h_1e_1c就是大图    </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取上映时间函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_time</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)(\(|$)'</span>)</span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)  <span class="comment"># 返回匹配到的第一个括号(.*?)中结果即时间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家/地区函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_area</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'.*\((.*)\)'</span>)</span><br><span class="line">    <span class="comment"># $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?)</span></span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>‘r’：</strong>正则前面加上’r’ 是为了告诉编译器这个string是个raw string，不要转意’\’。当一个字符串使用了正则表达式后，最好在前面加上’r’；<br><strong>‘|’ ‘$’：</strong>  正则’|’表示或’，’$’表示匹配一行字符串的结尾；<br><strong>.group(1)</strong>：意思是返回search匹配的第一个括号中的结果，即(.*?)，gropup()则返回所有结果2013-12-18(，group(1)返回’（’。</p></blockquote><p>接下来，修改main()函数来输出爬取的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        print(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>if <strong> name</strong> == ‘_ _main__’:</strong>当.py文件被直接运行时，if <strong> name</strong> == ‘_ <em>main__’之下的代码块将被运行；当.py文件以模块形式被导入时，if <strong> name</strong> == ‘</em> _main__’之下的代码块不被运行。<br>参考：<a href="https://blog.csdn.net/yjk13703623757/article/details/77918633。" target="_blank" rel="noopener">https://blog.csdn.net/yjk13703623757/article/details/77918633。</a></p></blockquote><p>运行程序，就可成功地提取出所需内容，结果如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'1'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg'</span>, <span class="string">'name'</span>: <span class="string">'霸王别姬'</span>, <span class="string">'star'</span>: <span class="string">'张国荣,张丰毅,巩俐'</span>, <span class="string">'time'</span>: <span class="string">'1993-01-01'</span>, <span class="string">'area'</span>: <span class="string">'中国香港'</span>, <span class="string">'score'</span>: <span class="string">'9.6'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'2'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/54617769d96807e4d81804284ffe2a27239007.jpg'</span>, <span class="string">'name'</span>: <span class="string">'罗马假日'</span>, <span class="string">'star'</span>: <span class="string">'格利高里·派克,奥黛丽·赫本,埃迪·艾伯特'</span>, <span class="string">'time'</span>: <span class="string">'1953-09-02'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.1'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'3'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg'</span>, <span class="string">'name'</span>: <span class="string">'肖申克的救赎'</span>, <span class="string">'star'</span>: <span class="string">'蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿'</span>, <span class="string">'time'</span>: <span class="string">'1994-10-14'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.5'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'4'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/e55ec5d18ccc83ba7db68caae54f165f95924.jpg'</span>, <span class="string">'name'</span>: <span class="string">'这个杀手不太冷'</span>, <span class="string">'star'</span>: <span class="string">'让·雷诺,加里·奥德曼,娜塔莉·波特曼'</span>, <span class="string">'time'</span>: <span class="string">'1994-09-14'</span>, <span class="string">'area'</span>: <span class="string">'法国'</span>, <span class="string">'score'</span>: <span class="string">'9.5'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'5'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p1.meituan.net/movie/f5a924f362f050881f2b8f82e852747c118515.jpg'</span>, <span class="string">'name'</span>: <span class="string">'教父'</span>, <span class="string">'star'</span>: <span class="string">'马龙·白兰度,阿尔·帕西诺,詹姆斯·肯恩'</span>, <span class="string">'time'</span>: <span class="string">'1972-03-24'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.3'</span>&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">[Finished <span class="keyword">in</span> <span class="number">1.9</span>s]</span><br></pre></td></tr></table></figure><p>以上是第1种提取方法，如果还不习惯正则表达式这种复杂的语法，可以试试下面的第2种方法。</p><h4 id="3-3-2-lxml结合xpath提取"><a href="#3-3-2-lxml结合xpath提取" class="headerlink" title="3.3.2. lxml结合xpath提取"></a>3.3.2. lxml结合xpath提取</h4><p>该方法需要用到<strong>lxml</strong>这款解析利器，同时搭配xpath语法，利用它的的路径选择表达式，来高效提取所需内容。lxml包为第三方包，需要自行安装。如果对xpath的语法还不太熟悉，可参考下面的教程：<br><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_syntax.asp</a></p><p><strong>xpath常用的规则</strong>    </p><table><thead><tr><th style="text-align:center">表达式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">nodename</td><td style="text-align:center">选取此节点的所有子节点</td></tr><tr><td style="text-align:center">/</td><td style="text-align:center">从当前节点选取直接子节点</td></tr><tr><td style="text-align:center">//</td><td style="text-align:center">从当前节点选取子孙节点</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">选取当前节点</td></tr><tr><td style="text-align:center">..</td><td style="text-align:center">选取当前节点的父节点</td></tr><tr><td style="text-align:center">@</td><td style="text-align:center">选取属性</td></tr></tbody></table><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span> <span class="attr">id</span>=<span class="string">"app"</span> <span class="attr">class</span>=<span class="string">"page-board/index"</span> &gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"main"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"update-time"</span>&gt;</span>2018-08-18<span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"has-fresh-text"</span>&gt;</span>已更新<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"board-content"</span>&gt;</span>榜单规则：将猫眼电影库中的经典影片，按照评分和评分人数从高到低综合排序取前100名，每天上午10点更新。相关数据来源于“猫眼电影库”。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dl</span> <span class="attr">class</span>=<span class="string">"board-wrapper"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">dd</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"board-index board-index-1"</span>&gt;</span>1<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/films/1203"</span> <span class="attr">title</span>=<span class="string">"霸王别姬"</span> <span class="attr">class</span>=<span class="string">"image-link"</span> <span class="attr">data-act</span>=<span class="string">"boarditem-click"</span> <span class="attr">data-val</span>=<span class="string">"&#123;movieId:1203&#125;"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"//ms0.meituan.net/mywww/image/loading_2.e3d934bf.png"</span> <span class="attr">alt</span>=<span class="string">""</span> <span class="attr">class</span>=<span class="string">"poster-default"</span> /&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-src</span>=<span class="string">"http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c"</span> <span class="attr">alt</span>=<span class="string">"霸王别姬"</span> <span class="attr">class</span>=<span class="string">"board-img"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"board-item-main"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"board-item-content"</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"movie-item-info"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"name"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/films/1203"</span> <span class="attr">title</span>=<span class="string">"霸王别姬"</span> <span class="attr">data-act</span>=<span class="string">"boarditem-click"</span> <span class="attr">data-val</span>=<span class="string">"&#123;movieId:1203&#125;"</span>&gt;</span>霸王别姬<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"star"</span>&gt;</span></span><br><span class="line">                主演：张国荣,张丰毅,巩俐</span><br><span class="line">        <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"releasetime"</span>&gt;</span>上映时间：1993-01-01(中国香港)<span class="tag">&lt;/<span class="name">p</span>&gt;</span>    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"movie-item-number score-num"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"score"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"integer"</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fraction"</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                <span class="tag">&lt;/<span class="name">dd</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">dd</span>&gt;</span></span><br></pre></td></tr></table></figure><p>根据截取的部分html网页，先来提取第1个电影排名信息，有两种方法。<br><strong>第一种：</strong>直接复制。<br>右键-Copy-Copy Xpath，得到xpath路径为：<strong>//*[@id=”app”]/div/div/div[1]/dl/dd[1]/i</strong>,为了能够提取到页面所有的排名信息，需进一步修改为：<strong>//*[@id=”app”]/div/div/div[1]/dl/dd/i/text()</strong>，如果想要再精简一点，可以省去中间部分绝对路径’/‘然后用相对路径’//‘代替，最后进一步修改为：<strong>//*[@id=”app”]//div//dd/i/text()</strong>。<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/840042.jpg" alt=""></p><p><strong>第二种：</strong>观察网页结构自己写。<br>首先注意到<strong>id = app</strong>的div节点，因为在整个网页结构id是唯一的不会有第二个相同的，所有可以将该div节点作为xpath语法的起点，然后往下观察分别是3级div节点，可以省略写为：<strong>//div</strong>,再往下分别是是两个并列的<strong>p</strong>节点、<strong>dl</strong>节点、<strong>dd</strong>节点和最后的<strong>i</strong>节点文本。中间可以随意省略，只要保证该路径能够选择到唯一的文本值<strong>‘1’</strong>即可，例如省去p和dl节点，只保留后面的节点。这样，完整路径可以为：<strong>//*[@id=”app”]//div//dd/i/text()</strong>，和上式一样。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/87341266.jpg" alt=""></p><p>根据上述思路，可以写下其他内容的xpath路径。观察到路径的前一部分：<strong>//*[@id=”app”]//div//dd</strong>都是一样的，从后面才开始不同，因此为了能够精简代码，将前部分路径赋值为一个变量items，最终提取的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2 用lxml结合xpath提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page2</span><span class="params">(html)</span>:</span></span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'//*[@id="app"]//div//dd'</span>)</span><br><span class="line">    <span class="comment"># 完整的是//*[@id="app"]/div/div/div[1]/dl/dd</span></span><br><span class="line">    <span class="comment"># print(type(items))</span></span><br><span class="line">    <span class="comment"># *代表匹配所有节点，@表示属性</span></span><br><span class="line">    <span class="comment"># 第一个电影是dd[1],要提取页面所有电影则去掉[1]</span></span><br><span class="line">    <span class="comment"># xpath://*[@id="app"]/div/div/div[1]/dl/dd[1]    </span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>: item.xpath(<span class="string">'./i/text()'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="comment">#./i/text()前面的点表示从items节点开始</span></span><br><span class="line">            <span class="comment">#/text()提取文本</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(str(item.xpath(<span class="string">'./a/img[2]/@data-src'</span>)[<span class="number">0</span>].strip())),</span><br><span class="line">            <span class="comment"># 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。</span></span><br><span class="line">            <span class="string">'name'</span>: item.xpath(<span class="string">'./a/@title'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'star'</span>: item.xpath(<span class="string">'.//p[@class = "star"]/text()'</span>)[<span class="number">0</span>].strip(),</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span> : item.xpath(<span class="string">'.//p[@class = "score"]/i[1]/text()'</span>)[<span class="number">0</span>] + \</span><br><span class="line">            item.xpath(<span class="string">'.//p[@class = "score"]/i[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>[0]：</strong>xpath后面添加了[0]是因为返回的是只有1个字符串的list，添加[0]是将list提取为字符串，使其简洁；<br><strong>Network：</strong>要在最原始的Network选项卡中定位，而不是Elements中，不然提取不到相关内容；<br><strong>class属性：</strong>p[@class = “star”]/text()表示提取class属性为”star”的p节点的文本值；<br><strong>提取属性值：</strong>img[2]/@data-src’：提取img节点的data-src属性值，属性值后面无需添加’/text()’</p></blockquote><p>运行程序，就可成功地提取出所需内容，结果和第一种方法一样。</p><p>以上是第2种提取方法，如果也不太习惯xpath语法，可以试试下面的第3种方法。</p><h4 id="3-3-3-Beautiful-Soup-css选择器"><a href="#3-3-3-Beautiful-Soup-css选择器" class="headerlink" title="3.3.3. Beautiful Soup + css选择器"></a>3.3.3. Beautiful Soup + css选择器</h4><p>Beautiful Soup 同lxml一样，是一个非常强大的python解析库，可以从HTML或XML文件中提取效率非常高。关于它的用法，可参考下面的教程：<br><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></p><p>css选择器选是一种模式，用于选择需要添加样式的元素，使用它的语法同样能够快速定位到所需节点，然后提取相应内容。使用方法可参考下面的教程：<br><a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a></p><p><strong>css选择器常用的规则</strong>  </p><style>table th:nth-of-type(1) {    width: 30%;}</style><table><thead><tr><th style="text-align:center">选择器</th><th style="text-align:center">例子</th><th style="text-align:center">例子描述</th></tr></thead><tbody><tr><td style="text-align:center">.class</td><td style="text-align:center">.intro</td><td style="text-align:center">选择 class=”intro” 的所有元素。</td></tr><tr><td style="text-align:center">#id</td><td style="text-align:center">#firstname</td><td style="text-align:center">选择 id=”firstname” 的所有元素。</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">*</td><td style="text-align:center">选择所有元素。</td></tr><tr><td style="text-align:center">element</td><td style="text-align:center">p</td><td style="text-align:center">选择所有p元素。</td></tr><tr><td style="text-align:center">element,element</td><td style="text-align:center">div,p</td><td style="text-align:center">选择所有div元素和所有p元素。</td></tr><tr><td style="text-align:center">element?element</td><td style="text-align:center">div p</td><td style="text-align:center">选择div元素内部的所有p元素。</td></tr><tr><td style="text-align:center">element&gt;element</td><td style="text-align:center">div&gt;p</td><td style="text-align:center">选择父元素为div元素的所有p元素。</td></tr><tr><td style="text-align:center">element+element</td><td style="text-align:center">div+p</td><td style="text-align:center">选择紧接在div元素之后的所有p元素。</td></tr><tr><td style="text-align:center">[attribute]</td><td style="text-align:center">[target]</td><td style="text-align:center">选择带有 target 属性所有元素。</td></tr><tr><td style="text-align:center">[attribute=value]</td><td style="text-align:center">[target=_blank]</td><td style="text-align:center">选择 target=”_blank” 的所有元素。</td></tr></tbody></table><p>下面就利用这种方法进行提取：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3 用beautifulsoup + css选择器提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page3</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    <span class="comment"># print(type(content))</span></span><br><span class="line">    <span class="comment"># print('------------')</span></span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.select(<span class="string">'dd i.board-index'</span>)[item].string,</span><br><span class="line">            <span class="comment"># iclass节点完整地为'board-index board-index-1',写board-index即可</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(soup.select(<span class="string">'a &gt; img.board-img'</span>)[item][<span class="string">"data-src"</span>]),</span><br><span class="line">            <span class="comment"># 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值</span></span><br><span class="line"></span><br><span class="line">            <span class="string">'name'</span>: soup.select(<span class="string">'.name a'</span>)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.select(<span class="string">'.star'</span>)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: soup.select(<span class="string">'.integer'</span>)[item].string + soup.select(<span class="string">'.fraction'</span>)[item].string</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></p><p>运行上述程序，结果同第1种方法一样。</p><h4 id="3-3-4-Beautiful-Soup-find-all函数提取"><a href="#3-3-4-Beautiful-Soup-find-all函数提取" class="headerlink" title="3.3.4. Beautiful Soup + find_all函数提取"></a>3.3.4. Beautiful Soup + find_all函数提取</h4><p>Beautifulsoup除了和css选择器搭配，还可以直接用它自带的find_all函数进行提取。<br><strong>find_all</strong>，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。<br>它的API如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find_all(name , attrs , recursive , text , **kwargs)</span><br></pre></td></tr></table></figure><blockquote><p><strong>常用的语法规则如下：</strong><br>soup.find_all(name=’ul’)： 查找所有<strong>ul</strong>节点，ul节点内还可以嵌套；<br>li.string和li.get_text()：都是获取<strong>li</strong>节点的文本，但推荐使用后者；<br>soup.find_all(attrs={‘id’: ‘list-1’}))：传入 attrs 参数，参数的类型是字典类型，表示查询 <strong>id</strong> 为 <strong>list-1</strong> 的节点；<br>常用的属性比如 id、class 等，可以省略attrs采用更简洁的形式，例如：<br>soup.find_all(id=’list-1’)<br>soup.find_all(class_=’element’)</p></blockquote><p>根据上述常用语法，可以提取网页中所需内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page4</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.find_all(class_=<span class="string">'board-index'</span>)[item].string,</span><br><span class="line">            <span class="string">'thumb'</span>: soup.find_all(class_ = <span class="string">'board-img'</span>)[item].attrs[<span class="string">'data-src'</span>],</span><br><span class="line">            <span class="comment"># 用.get('data-src')获取图片src链接，或者用attrs['data-src']</span></span><br><span class="line">            <span class="string">'name'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span> : <span class="string">'name'</span>&#125;)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'star'</span>&#125;)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>:soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'integer'</span>&#125;)[item].string.strip() + soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'fraction'</span>&#125;)[item].string.strip()</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>以上就是4种不同的内容提取方法。</p><h3 id="3-4-数据存储"><a href="#3-4-数据存储" class="headerlink" title="3.4. 数据存储"></a>3.4. 数据存储</h3><p>上述输出的结果为字典格式，可利用csv包的DictWriter函数将字典格式数据存储到csv文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储到csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file3</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼top100.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 'a'为追加模式（添加）</span></span><br><span class="line">        <span class="comment"># utf_8_sig格式导出csv不乱码 </span></span><br><span class="line">        fieldnames = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]</span><br><span class="line">        w = csv.DictWriter(f,fieldnames = fieldnames)</span><br><span class="line">        <span class="comment"># w.writeheader()</span></span><br><span class="line">        w.writerow(item)</span><br></pre></td></tr></table></figure><p>然后修改一下main()方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line">        write_to_csv(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/66910595.jpg" alt=""></p><p>再将封面的图片下载下来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_thumb</span><span class="params">(name, url,num)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'封面图/'</span> + name + <span class="string">'.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">            print(<span class="string">'第%s部电影封面下载完毕'</span> %num)</span><br><span class="line">            print(<span class="string">'------'</span>)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     <span class="comment"># 不能是w，否则会报错，因为图片是二进制数据所以要用wb</span></span><br></pre></td></tr></table></figure><h3 id="3-5-分页爬取"><a href="#3-5-分页爬取" class="headerlink" title="3.5. 分页爬取"></a>3.5. 分页爬取</h3><p>上面完成了一页电影数据的提取，接下来还需提取剩下9页共90部电影的数据。对网址进行遍历，给网址传入一个offset参数即可，修改如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset='</span> + str(offset)</span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line">        write_to_csv(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        main(offset = i*<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>这样就完成了所有电影的爬取。结果如下：</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/28479553.jpg" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-19/34117279.jpg" alt=""></p><h2 id="4-可视化分析"><a href="#4-可视化分析" class="headerlink" title="4. 可视化分析"></a>4. 可视化分析</h2><p>俗话说“文不如表，表不如图”。下面根据excel的数据结果，进行简单的数据可视化分析，并用图表呈现。</p><h3 id="4-1-电影评分最高top10"><a href="#4-1-电影评分最高top10" class="headerlink" title="4.1. 电影评分最高top10"></a>4.1. 电影评分最高top10</h3><p>首先，想看一看评分最高的前10部电影是哪些？</p><p>程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment">#用于修改x轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)   <span class="comment">#默认绘图风格很难看，替换为好看的ggplot风格</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))   <span class="comment">#设置图片大小</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment">#设置图表title、text标注的颜色</span></span><br><span class="line"></span><br><span class="line">columns = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]  <span class="comment">#设置表头</span></span><br><span class="line">df = pd.read_csv(<span class="string">'maoyan_top100.csv'</span>,encoding = <span class="string">"utf-8"</span>,header = <span class="keyword">None</span>,names =columns,index_col = <span class="string">'index'</span>)  <span class="comment">#打开表格</span></span><br><span class="line"><span class="comment"># index_col = 'index' 将索引设为index</span></span><br><span class="line"></span><br><span class="line">df_score = df.sort_values(<span class="string">'score'</span>,ascending = <span class="keyword">False</span>)  <span class="comment">#按得分降序排列</span></span><br><span class="line"></span><br><span class="line">name1 = df_score.name[:<span class="number">10</span>]      <span class="comment">#x轴坐标</span></span><br><span class="line">score1 = df_score.score[:<span class="number">10</span>]    <span class="comment">#y轴坐标  </span></span><br><span class="line">plt.bar(range(<span class="number">10</span>),score1,tick_label = name1)  <span class="comment">#绘制条形图，用range()能搞保持x轴正确顺序</span></span><br><span class="line">plt.ylim ((<span class="number">9</span>,<span class="number">9.8</span>))  <span class="comment">#设置纵坐标轴范围</span></span><br><span class="line">plt.title(<span class="string">'电影评分最高top10'</span>,color = colors1) <span class="comment">#标题</span></span><br><span class="line">plt.xlabel(<span class="string">'电影名称'</span>)      <span class="comment">#x轴标题</span></span><br><span class="line">plt.ylabel(<span class="string">'评分'</span>)          <span class="comment">#y轴标题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个条形图添加数值标签</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(score1)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.01</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line"></span><br><span class="line">pl.xticks(rotation=<span class="number">270</span>)   <span class="comment">#x轴名称太长发生重叠，旋转为纵向显示</span></span><br><span class="line">plt.tight_layout()    <span class="comment">#自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line"><span class="comment"># plt.savefig('电影评分最高top10.png')   #保存图片</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/80083042.jpg" alt=""><br>可以看到：排名最高的分别是两部国产片”霸王别姬”和”大话西游”，其他还包括”肖申克的救赎”、”教父”等。<br>嗯，还好基本上都看过。</p><h3 id="4-2-各国家的电影数量比较"><a href="#4-2-各国家的电影数量比较" class="headerlink" title="4.2. 各国家的电影数量比较"></a>4.2. 各国家的电影数量比较</h3><p>然后，想看看100部电影都是来自哪些国家？<br>程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">area_count = df.groupby(by = <span class="string">'area'</span>).area.count().sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图方法1</span></span><br><span class="line">area_count.plot.bar(color = <span class="string">'#4652B1'</span>)  <span class="comment">#设置为蓝紫色</span></span><br><span class="line">pl.xticks(rotation=<span class="number">0</span>)   <span class="comment">#x轴名称太长重叠，旋转为纵向</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图方法2</span></span><br><span class="line"><span class="comment"># plt.bar(range(11),area_count.values,tick_label = area_count.index)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(area_count.values)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.5</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line">plt.title(<span class="string">'各国/地区电影数量排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'国家/地区'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('各国(地区)电影数量排名.png')</span></span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-21/57684234.jpg" alt=""><br>可以看到，除去网站自身没有显示国家的电影以外，上榜电影被10个国家/地区”承包”了。其中，美国以30部电影的绝对优势占据第1名，其次是8部的日本，韩国第3，居然有7部上榜。<br>不得不说的是香港有5部，而内地一部都没有。。。</p><h3 id="4-3-电影作品数量集中的年份"><a href="#4-3-电影作品数量集中的年份" class="headerlink" title="4.3. 电影作品数量集中的年份"></a>4.3. 电影作品数量集中的年份</h3><p>接下来站在漫长的百年电影史的时间角度上，分析一下哪些年份”贡献了”最多的电影数量，也可以说是”电影大年”。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从日期中提取年份</span></span><br><span class="line">df[<span class="string">'year'</span>] = df[<span class="string">'time'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">'/'</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># print(df.info())</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计各年上映的电影数量</span></span><br><span class="line">grouped_year = df.groupby(<span class="string">'year'</span>)</span><br><span class="line">grouped_year_amount = grouped_year.year.count()</span><br><span class="line">top_year = grouped_year_amount.sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">top_year.plot(kind = <span class="string">'bar'</span>,color = <span class="string">'orangered'</span>) <span class="comment">#颜色设置为橙红色</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(top_year.values)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.1</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line">plt.title(<span class="string">'电影数量年份排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'年份(年)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line"><span class="comment"># plt.savefig('电影数量年份排名.png')</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/32342735.jpg" alt=""></p><p>可以看到，100部电影来自37个年份。其中2011年上榜电影数量最多，达到9部；其次是前一年的7部。回忆一下，那会儿正是上大学的头两年，可怎么感觉除了阿凡达之外，没有什么其他有印象的电影了。。。<br>另外，网上传的号称”电影史奇迹年”的1994年仅排名第6。这让我进一步对猫眼榜单的权威性产生了质疑。<br>再往后看，发现遥远的1939和1940年也有电影上榜。那会儿应该还是黑白电影时代吧，看来电影的口碑好坏跟外在的技术没有绝对的关系，质量才是王道。</p><h3 id="4-4-拥有电影作品数量最多的演员"><a href="#4-4-拥有电影作品数量最多的演员" class="headerlink" title="4.4 拥有电影作品数量最多的演员"></a>4.4 拥有电影作品数量最多的演员</h3><p>最后，看看前100部电影中哪些演员的作品数量最多。<br>程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中</span></span><br><span class="line">starlist = []</span><br><span class="line">star_total = df.star</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df.star.str.replace(<span class="string">' '</span>,<span class="string">''</span>).str.split(<span class="string">','</span>):</span><br><span class="line">    starlist.extend(i)  </span><br><span class="line"><span class="comment"># print(starlist)</span></span><br><span class="line"><span class="comment"># print(len(starlist))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set去除重复的演员名</span></span><br><span class="line">starall = set(starlist)</span><br><span class="line"><span class="comment"># print(starall)</span></span><br><span class="line"><span class="comment"># print(len(starall))</span></span><br><span class="line"></span><br><span class="line">starall2 = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> starall:</span><br><span class="line">    <span class="keyword">if</span> starlist.count(i)&gt;<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 筛选出电影数量超过1部的演员</span></span><br><span class="line">        starall2[i] = starlist.count(i)</span><br><span class="line"></span><br><span class="line">starall2 = sorted(starall2.items(),key = <span class="keyword">lambda</span> starlist:starlist[<span class="number">1</span>] ,reverse = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">starall2 = dict(starall2[:<span class="number">10</span>])  <span class="comment">#将元组转为字典格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">x_star = list(starall2.keys())      <span class="comment">#x轴坐标</span></span><br><span class="line">y_star = list(starall2.values())    <span class="comment">#y轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.bar(range(<span class="number">10</span>),y_star,tick_label = x_star)</span><br><span class="line">pl.xticks(rotation = <span class="number">270</span>)</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(y_star):</span><br><span class="line">    plt.text(x,y+<span class="number">0.1</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'演员电影作品数量排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'演员'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()    </span><br><span class="line"><span class="comment"># plt.savefig('演员电影作品数量排名.png')</span></span><br></pre></td></tr></table></figure></p><p>结果如下图：<br><img src="http://pbscl931v.bkt.clouddn.com/18-8-20/66062822.jpg" alt=""></p><p>张国荣排在了第一位，这是之前没有猜到的。其次是梁朝伟和星爷，再之后是布拉德·皮特。惊奇地发现，前十名影星中，香港影星居然占了6位。有点严重怀疑这是不是香港版的top100电影。。。</p><p>对张国荣以7部影片的巨大优势雄霸榜单第一位感到好奇，想看看是哪7部电影。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'star1'</span>] = df[<span class="string">'star'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">','</span>)[<span class="number">0</span>])  <span class="comment">#提取1号演员</span></span><br><span class="line">df[<span class="string">'star2'</span>] = df[<span class="string">'star'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">','</span>)[<span class="number">1</span>])  <span class="comment">#提取2号演员</span></span><br><span class="line">star_most = df[(df.star1 == <span class="string">'张国荣'</span>) | (df.star2 == <span class="string">'张国荣'</span>)][[<span class="string">'star'</span>,<span class="string">'name'</span>]].reset_index(<span class="string">'index'</span>)</span><br><span class="line"><span class="comment"># |表示两个条件或查询，之后重置索引</span></span><br><span class="line">print(star_most)</span><br></pre></td></tr></table></figure></p><p>可以看到包括排名第1的”霸王别姬”、第17名的”春光乍泄”、第27名的”射雕英雄传之东成西就”等。<br>突然发现，好像只看过”英雄本色”。。。有时间，去看看他其他的作品。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">     index        star              name</span><br><span class="line">0      1   张国荣,张丰毅,巩俐        霸王别姬</span><br><span class="line">1     17   张国荣,梁朝伟,张震        春光乍泄</span><br><span class="line">2     27  张国荣,梁朝伟,张学友  射雕英雄传之东成西就</span><br><span class="line">3     37  张国荣,梁朝伟,刘嘉玲        东邪西毒</span><br><span class="line">4     70   张国荣,王祖贤,午马        倩女幽魂</span><br><span class="line">5     99  张国荣,张曼玉,刘德华        阿飞正传</span><br><span class="line">6    100   狄龙,张国荣,周润发        英雄本色</span><br></pre></td></tr></table></figure><p>由于数据量有限，故仅作了上述简要的分析。</p><h2 id="5-完整程序"><a href="#5-完整程序" class="headerlink" title="5. 完整程序"></a>5. 完整程序</h2><p>最后，将前面爬虫的所有代码整理一下，完整的代码如下：<br>一、爬虫部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line">        <span class="comment"># 不加headers爬不了</span></span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 用正则提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(</span><br><span class="line">        <span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>, re.S)</span><br><span class="line">    <span class="comment"># re.S表示匹配任意字符，如果不加.无法匹配换行符</span></span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'index'</span>: item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(item[<span class="number">1</span>]),</span><br><span class="line">            <span class="string">'name'</span>: item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'star'</span>: item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="comment"># 'time': item[4].strip()[5:],</span></span><br><span class="line">            <span class="comment"># 用函数分别提取time里的日期和地区</span></span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: item[<span class="number">5</span>].strip() + item[<span class="number">6</span>].strip()</span><br><span class="line">        &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 2 用lxml结合xpath提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page2</span><span class="params">(html)</span>:</span></span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'//*[@id="app"]//div//dd'</span>)</span><br><span class="line">    <span class="comment"># 完整的是//*[@id="app"]/div/div/div[1]/dl/dd</span></span><br><span class="line">    <span class="comment"># print(type(items))</span></span><br><span class="line">    <span class="comment"># *代表匹配所有节点，@表示属性</span></span><br><span class="line">    <span class="comment"># 第一个电影是dd[1],要提取页面所有电影则去掉[1]</span></span><br><span class="line">    <span class="comment"># xpath://*[@id="app"]/div/div/div[1]/dl/dd[1]</span></span><br><span class="line">    <span class="comment"># lst = []</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>: item.xpath(<span class="string">'./i/text()'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="comment">#./i/text()前面的点表示从items节点开始</span></span><br><span class="line">            <span class="comment">#/text()提取文本</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(str(item.xpath(<span class="string">'./a/img[2]/@data-src'</span>)[<span class="number">0</span>].strip())),</span><br><span class="line">            <span class="comment"># 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。</span></span><br><span class="line">            <span class="string">'name'</span>: item.xpath(<span class="string">'./a/@title'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'star'</span>: item.xpath(<span class="string">'.//p[@class = "star"]/text()'</span>)[<span class="number">0</span>].strip(),</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span> : item.xpath(<span class="string">'.//p[@class = "score"]/i[1]/text()'</span>)[<span class="number">0</span>] + \</span><br><span class="line">            item.xpath(<span class="string">'.//p[@class = "score"]/i[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 3 用beautifulsoup + css选择器提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page3</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    <span class="comment"># print(type(content))</span></span><br><span class="line">    <span class="comment"># print('------------')</span></span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.select(<span class="string">'dd i.board-index'</span>)[item].string,</span><br><span class="line">            <span class="comment"># iclass节点完整地为'board-index board-index-1',写board-inde即可</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(soup.select(<span class="string">'a &gt; img.board-img'</span>)[item][<span class="string">"data-src"</span>]),</span><br><span class="line">            <span class="comment"># 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值</span></span><br><span class="line"></span><br><span class="line">            <span class="string">'name'</span>: soup.select(<span class="string">'.name a'</span>)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.select(<span class="string">'.star'</span>)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: soup.select(<span class="string">'.integer'</span>)[item].string + soup.select(<span class="string">'.fraction'</span>)[item].string</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 用beautifulsoup + find_all提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page4</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.find_all(class_=<span class="string">'board-index'</span>)[item].string,</span><br><span class="line">            <span class="string">'thumb'</span>: soup.find_all(class_ = <span class="string">'board-img'</span>)[item].attrs[<span class="string">'data-src'</span>],</span><br><span class="line">            <span class="comment"># 用.get('data-src')获取图片src链接，或者用attrs['data-src']</span></span><br><span class="line">            <span class="string">'name'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span> : <span class="string">'name'</span>&#125;)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'star'</span>&#125;)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>:soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'integer'</span>&#125;)[item].string.strip() + soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'fraction'</span>&#125;)[item].string.strip()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取时间函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_time</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)(\(|$)'</span>)</span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)  <span class="comment"># 返回匹配到的第一个括号(.*?)中结果即时间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家/地区函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_area</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'.*\((.*)\)'</span>)</span><br><span class="line">    <span class="comment"># $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?)</span></span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取封面大图</span></span><br><span class="line"><span class="comment"># http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c</span></span><br><span class="line"><span class="comment"># 去掉@160w_220h_1e_1c就是大图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_thumb</span><span class="params">(url)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)@.*?'</span>)</span><br><span class="line">    thumb = re.search(pattern, url)</span><br><span class="line">    <span class="keyword">return</span> thumb.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据存储到csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file3</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼top100.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 'a'为追加模式（添加）</span></span><br><span class="line">        <span class="comment"># utf_8_sig格式导出csv不乱码 </span></span><br><span class="line">        fieldnames = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]</span><br><span class="line">        w = csv.DictWriter(f,fieldnames = fieldnames)</span><br><span class="line">        <span class="comment"># w.writeheader()</span></span><br><span class="line">        w.writerow(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 封面下载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_thumb</span><span class="params">(name, url,num)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'封面图/'</span> + name + <span class="string">'.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">            print(<span class="string">'第%s部电影封面下载完毕'</span> %num)</span><br><span class="line">            print(<span class="string">'------'</span>)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     <span class="comment"># 存储格式是wb,因为图片是二进制数格式，不能用w，否则会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset='</span> + str(offset)</span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    <span class="comment"># print(html)</span></span><br><span class="line">    <span class="comment"># parse_one_page2(html)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  <span class="comment"># 切换内容提取方法</span></span><br><span class="line">        print(item)</span><br><span class="line">        write_to_file(item)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下载封面图</span></span><br><span class="line">        download_thumb(item[<span class="string">'name'</span>], item[<span class="string">'thumb'</span>],item[<span class="string">'index'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment">#     for i in range(10):</span></span><br><span class="line"><span class="comment">#         main(i * 10)</span></span><br><span class="line">        <span class="comment"># time.sleep(0.5)</span></span><br><span class="line">        <span class="comment"># 猫眼增加了反爬虫，设置0.5s的延迟时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 使用多进程提升抓取效率</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    pool.map(main, [i * <span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure></p><p>二、可视化部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 可视化分析</span></span><br><span class="line"><span class="comment"># -------------------------------</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment"># 用于修改x轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)  <span class="comment"># 默认绘图风格很难看，替换为好看的ggplot风格</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))  <span class="comment"># 设置图片大小</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment"># 设置图表title、text标注的颜色</span></span><br><span class="line">columns = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]  <span class="comment"># 设置表头</span></span><br><span class="line">df = pd.read_csv(<span class="string">'maoyan_top100.csv'</span>, encoding=<span class="string">"utf-8"</span>,</span><br><span class="line">                 header=<span class="keyword">None</span>, names=columns, index_col=<span class="string">'index'</span>)  <span class="comment"># 打开表格</span></span><br><span class="line"><span class="comment"># index_col = 'index' 将索引设为index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1电影评分最高top10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_1</span><span class="params">()</span>:</span></span><br><span class="line">    df_score = df.sort_values(<span class="string">'score'</span>, ascending=<span class="keyword">False</span>)  <span class="comment"># 按得分降序排列</span></span><br><span class="line"></span><br><span class="line">    name1 = df_score.name[:<span class="number">10</span>]  <span class="comment"># x轴坐标</span></span><br><span class="line">    score1 = df_score.score[:<span class="number">10</span>]  <span class="comment"># y轴坐标</span></span><br><span class="line">    plt.bar(range(<span class="number">10</span>), score1, tick_label=name1)  <span class="comment"># 绘制条形图，用range()能搞保持x轴正确顺序</span></span><br><span class="line">    plt.ylim((<span class="number">9</span>, <span class="number">9.8</span>))  <span class="comment"># 设置纵坐标轴范围</span></span><br><span class="line">    plt.title(<span class="string">'电影评分最高top10'</span>, color=colors1)  <span class="comment"># 标题</span></span><br><span class="line">    plt.xlabel(<span class="string">'电影名称'</span>)  <span class="comment"># x轴标题</span></span><br><span class="line">    plt.ylabel(<span class="string">'评分'</span>)  <span class="comment"># y轴标题</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为每个条形图添加数值标签</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(score1)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.01</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line"></span><br><span class="line">    pl.xticks(rotation=<span class="number">270</span>)  <span class="comment"># x轴名称太长发生重叠，旋转为纵向显示</span></span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line">    <span class="comment"># plt.savefig('电影评分最高top10.png')   #保存图片</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 2各国家的电影数量比较</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_2</span><span class="params">()</span>:</span></span><br><span class="line">    area_count = df.groupby(</span><br><span class="line">        by=<span class="string">'area'</span>).area.count().sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图方法1</span></span><br><span class="line">    area_count.plot.bar(color=<span class="string">'#4652B1'</span>)  <span class="comment"># 设置为蓝紫色</span></span><br><span class="line">    pl.xticks(rotation=<span class="number">0</span>)  <span class="comment"># x轴名称太长重叠，旋转为纵向</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图方法2</span></span><br><span class="line">    <span class="comment"># plt.bar(range(11),area_count.values,tick_label = area_count.index,color</span></span><br><span class="line">    <span class="comment"># = '#4652B1')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(area_count.values)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.5</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line">    plt.title(<span class="string">'各国/地区电影数量排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'国家/地区'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('各国(地区)电影数量排名.png')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 3电影作品数量集中的年份</span></span><br><span class="line"><span class="comment"># 从日期中提取年份</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_3</span><span class="params">()</span>:</span></span><br><span class="line">    df[<span class="string">'year'</span>] = df[<span class="string">'time'</span>].map(<span class="keyword">lambda</span> x: x.split(<span class="string">'/'</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(df.info())</span></span><br><span class="line">    <span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计各年上映的电影数量</span></span><br><span class="line">    grouped_year = df.groupby(<span class="string">'year'</span>)</span><br><span class="line">    grouped_year_amount = grouped_year.year.count()</span><br><span class="line">    top_year = grouped_year_amount.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    top_year.plot(kind=<span class="string">'bar'</span>, color=<span class="string">'orangered'</span>)  <span class="comment"># 颜色设置为橙红色</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(top_year.values)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.1</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line">    plt.title(<span class="string">'电影数量年份排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'年份(年)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment"># plt.savefig('电影数量年份排名.png')</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 4拥有电影作品数量最多的演员</span></span><br><span class="line"><span class="comment"># 表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_4</span><span class="params">()</span>:</span></span><br><span class="line">    starlist = []</span><br><span class="line">    star_total = df.star</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> df.star.str.replace(<span class="string">' '</span>, <span class="string">''</span>).str.split(<span class="string">','</span>):</span><br><span class="line">        starlist.extend(i)</span><br><span class="line">    <span class="comment"># print(starlist)</span></span><br><span class="line">    <span class="comment"># print(len(starlist))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set去除重复的演员名</span></span><br><span class="line">    starall = set(starlist)</span><br><span class="line">    <span class="comment"># print(starall)</span></span><br><span class="line">    <span class="comment"># print(len(starall))</span></span><br><span class="line"></span><br><span class="line">    starall2 = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> starall:</span><br><span class="line">        <span class="keyword">if</span> starlist.count(i) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 筛选出电影数量超过1部的演员</span></span><br><span class="line">            starall2[i] = starlist.count(i)</span><br><span class="line"></span><br><span class="line">    starall2 = sorted(starall2.items(),</span><br><span class="line">                      key=<span class="keyword">lambda</span> starlist: starlist[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    starall2 = dict(starall2[:<span class="number">10</span>])  <span class="comment"># 将元组转为字典格式</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    x_star = list(starall2.keys())  <span class="comment"># x轴坐标</span></span><br><span class="line">    y_star = list(starall2.values())  <span class="comment"># y轴坐标</span></span><br><span class="line"></span><br><span class="line">    plt.bar(range(<span class="number">10</span>), y_star, tick_label=x_star)</span><br><span class="line">    pl.xticks(rotation=<span class="number">270</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(y_star):</span><br><span class="line">        plt.text(x, y + <span class="number">0.1</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'演员电影作品数量排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'演员'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('演员电影作品数量排名.png')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    annalysis_1()</span><br><span class="line">    annalysis_2()</span><br><span class="line">    annalysis_3()</span><br><span class="line">    annalysis_4()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;python爬虫第1篇&lt;/em&gt;&lt;/b&gt;&lt;br&gt;利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="requests" scheme="https://www.makcyun.top/tags/requests/"/>
    
      <category term="正则表达式" scheme="https://www.makcyun.top/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
      <category term="beautifulsoup" scheme="https://www.makcyun.top/tags/beautifulsoup/"/>
    
      <category term="css" scheme="https://www.makcyun.top/tags/css/"/>
    
      <category term="xpath" scheme="https://www.makcyun.top/tags/xpath/"/>
    
      <category term="lxml" scheme="https://www.makcyun.top/tags/lxml/"/>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>4块钱,用Github+Hexo搭建你的个人博客：美化篇</title>
    <link href="https://www.makcyun.top/hexo02.html"/>
    <id>https://www.makcyun.top/hexo02.html</id>
    <published>2018-07-17T10:17:10.000Z</published>
    <updated>2018-08-22T06:18:21.193Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。</p><a id="more"></a><p><strong>摘要：</strong>搭建博客相对简单，而美化博客则要复杂一些，因为涉及到修改和增删源代码，对于没有前端基础的人来说，会比较费时间精力。为了尽可能在最短的时间里，打造一个总体看得过去的博客，本文以我的博客为例，介绍一些比较实用的博客美化操作和技巧。</p><h2 id="1-选择新的模板"><a href="#1-选择新的模板" class="headerlink" title="1. 选择新的模板"></a>1. 选择新的模板</h2><p>首先，是要更换非常难看的初始的博客界面。重新挑选一个好看的主题模板，然后在此基础上进行美化。<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/90405263.jpg" alt=""></p><p>主题寻找：<br><a href="https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories" target="_blank" rel="noopener">https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories</a></p><p>该网站按照模板的受欢迎程度进行排名，可以看到遥遥领先的第一名是一款叫作：<strong>next</strong>的主题，选用这款即可。进入到这个主题，可以阅读<strong>README.md</strong>模板使用说明，还可以查看模板示例网站。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FsgZW6JnrTdKpylVLyEORZbKJr9f" alt=""></p><p>模板使用：<br>打开博客根目录下的<strong>themes文件夹</strong>(注：后文所说的根目录指：<code>D:\blog</code>)，右键<strong>Git Bash</strong>运行下述命令：<br><code>git clone https://github.com/iissnan/hexo-theme-next themes/next</code><br>就可以把这款主题的安装文件下载到电脑中。接着，打开D:\blog_config.yml文件，找到 theme字段，修改参数为：theme: hexo-theme-next，然后根目录运行下述命令：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo s -g</span><br></pre></td></tr></table></figure></p><p>这样，便成功应用新的<strong>next</strong>主题，浏览器访问 :<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>，查看一下新的博客页面。<br><img src="http://pbscl931v.bkt.clouddn.com/FpHLTzWWl-JiakApNlw4nTH4hiin" alt=""><br>可以看到，博客变得非常清爽了，（可能和你实际看到的，略有不同，没有关系）。<br>这款主题包含4种风格，默认的是<strong>Muse</strong>，也可以尝试其他风格。具体操作：<br>打开<code>D:\blog\_config.yml</code>，定位到Schemes，想要哪款主题就取消前面的<strong>#</strong>，我的博客使用的是<strong>Pisces</strong>风格。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br></pre></td></tr></table></figure></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/78786445.jpg" alt=""></p><h2 id="2-模板美化"><a href="#2-模板美化" class="headerlink" title="2. 模板美化"></a>2. 模板美化</h2><p>接下来进行模板的美化。<br>根据网页的结构布局，将从以下几个部分进行针对性地美化：</p><ul><li>总体</li><li>侧边栏</li><li>页脚</li><li>文章</li></ul><p><strong>重要的文件</strong><br>美化需要主要是对几个模板文件进行修改和增删。为了便于后续进行操作，先列出文件名和所在的位置： </p><ul><li>站点文件。位于站点文件夹根目录内：<br><strong>D:/blog/_config.yml</strong></li><li>主题文件。位于主题文件夹根目录内：<br><strong>D:/blog/themes/next/_config.yml</strong></li><li>自定义样式文件。位于主题文件夹内：<br><strong>D:\blog\themes\hexo-theme-next\source\css_custom\custom.styl</strong></li></ul><h3 id="2-1-总体布置"><a href="#2-1-总体布置" class="headerlink" title="2.1. 总体布置"></a>2.1. 总体布置</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/38494028.jpg" alt=""></p><h4 id="2-1-1-设置中文界面"><a href="#2-1-1-设置中文界面" class="headerlink" title="2.1.1. 设置中文界面"></a>2.1.1. 设置中文界面</h4><p><strong>站点文件:</strong> language: zh-Hans<br>如果中文乱码，记事本另存为utf-8，最好不要用记事本编辑，用notepad。</p><h4 id="2-1-2-动态背景"><a href="#2-1-2-动态背景" class="headerlink" title="2.1.2. 动态背景"></a>2.1.2. 动态背景</h4><p><strong>主题文件：</strong> canvas_nest: true<br>背景的几何线条是采用的nest效果，一个基于html5 canvas绘制的网页背景效果，非常赞！来自github的开源项目canvas-nest：<a href="https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md" target="_blank" rel="noopener">https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md</a></p><p>如果感觉默认的线条太多的话，可以这么设置：<br>打开 <code>next/layout/_layout.swig</code>，在 &lt; /body&gt;之前添加代码(注意不要放在&lt; /head&gt;的后面)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.canvas_nest %&#125;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;</span><br><span class="line">color=&quot;233,233,233&quot; opacity=&apos;0.9&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>说明：<br>color ：线条颜色, 默认: ‘0,0,0’；三个数字分别为(R,G,B)<br>opacity: 线条透明度（0~1）, 默认: 0.5<br>count: 线条的总数量, 默认: 150<br>zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1</p><h3 id="2-2-侧边栏美化"><a href="#2-2-侧边栏美化" class="headerlink" title="2.2. 侧边栏美化"></a>2.2. 侧边栏美化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/97716423.jpg" alt=""></p><h4 id="2-2-1-添加博客名字和slogan"><a href="#2-2-1-添加博客名字和slogan" class="headerlink" title="2.2.1. 添加博客名字和slogan"></a>2.2.1. 添加博客名字和slogan</h4><p>修改<strong>站点文件</strong>如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Hexo Configuration</span><br><span class="line">## Docs: https://hexo.io/docs/configuration.html</span><br><span class="line">## Source: https://github.com/hexojs/hexo/</span><br><span class="line"></span><br><span class="line"># Site</span><br><span class="line">title: 高级农民工            # 更改为你自己的</span><br><span class="line">subtitle: Beginner<span class="string">'s Mind   </span></span><br><span class="line"><span class="string">description:</span></span><br><span class="line"><span class="string">keywords: python,hexo,神器,软件</span></span><br><span class="line"><span class="string">author: 高级农民工</span></span><br><span class="line"><span class="string">language: zh-Hans</span></span><br><span class="line"><span class="string">timezone:</span></span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-菜单设置"><a href="#2-2-2-菜单设置" class="headerlink" title="2.2.2. 菜单设置"></a>2.2.2. 菜单设置</h4><p>文件路径：<code>D:\blog\themes\hexo-theme-next\languages\zh-Hans.yml</code><br>修改如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: 首&amp;emsp;&amp;emsp;页</span><br><span class="line">  archives: 归&amp;emsp;&amp;emsp;档</span><br><span class="line">  categories: 分&amp;emsp;&amp;emsp;类</span><br><span class="line">  tags: 标&amp;emsp;&amp;emsp;签</span><br><span class="line">  about: 关于博主</span><br><span class="line">  search: 站内搜索</span><br><span class="line">  top: 最受欢迎</span><br><span class="line">  schedule: 日程表</span><br><span class="line">  sitemap: 站点地图</span><br><span class="line">  # commonweal: 公益404</span><br></pre></td></tr></table></figure></p><p>注意：两字的中间添加<code>&amp;emsp;&amp;emsp;</code>可实现列对齐。</p><h4 id="2-2-3-新建标签、分类、关于页面"><a href="#2-2-3-新建标签、分类、关于页面" class="headerlink" title="2.2.3. 新建标签、分类、关于页面"></a>2.2.3. 新建标签、分类、关于页面</h4><p>分别运行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;tags&quot; </span><br><span class="line">hexo new page &quot;categories&quot;  </span><br><span class="line">hexo new page &quot;about&quot;</span><br></pre></td></tr></table></figure></p><p>然后，打开<code>D:\blog\source</code>就可以看到上述三个文件夹。<br>要添加关于博主的介绍，只需要在<code>/about/index.md</code>文件中，用markdown书写内容即可，写完后运行：<code>hexo d -g</code>，便可看到效果。</p><h4 id="2-2-4-侧栏社交链接图标设置"><a href="#2-2-4-侧栏社交链接图标设置" class="headerlink" title="2.2.4. 侧栏社交链接图标设置"></a>2.2.4. 侧栏社交链接图标设置</h4><p>可以添加你的github、Email、知乎、简书等社交网站账号。<br><strong>主题文件：</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># ---------------------------------------------------------------</span><br><span class="line"># Sidebar Settings 侧栏社交链接图标设置</span><br><span class="line"># ---------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># Social Links.</span><br><span class="line"># Usage: `Key: permalink || icon`</span><br><span class="line"># Key is the link label showing to end users.</span><br><span class="line"># Value before `||` delimeter is the target permalink.</span><br><span class="line"># Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.</span><br><span class="line">social:</span><br><span class="line">  GitHub: https:<span class="comment">//github.com/makcyun || github</span></span><br><span class="line">  E-Mail: mailto:johnny824lee@gmail.com || envelope</span><br><span class="line">  #Google: https://plus.google.com/yourname || google</span><br><span class="line">  #Twitter: https://twitter.com/yourname || twitter</span><br><span class="line">  #FB Page: https://www.facebook.com/yourname || facebook</span><br><span class="line">  #VK Group: https://vk.com/yourname || vk</span><br><span class="line">  #StackOverflow: https://stackoverflow.com/yourname || stack-overflow</span><br><span class="line">  #YouTube: https://youtube.com/yourname || youtube</span><br><span class="line">  #Instagram: https://instagram.com/yourname || instagram</span><br><span class="line">  #Skype: skype:yourname?call|chat || skype</span><br><span class="line"></span><br><span class="line">social_icons:</span><br><span class="line">  enable: <span class="literal">true</span></span><br><span class="line">  icons_only: <span class="literal">false</span></span><br><span class="line">  transition: <span class="literal">false</span></span><br></pre></td></tr></table></figure></p><h4 id="2-2-5-添加头像并美化"><a href="#2-2-5-添加头像并美化" class="headerlink" title="2.2.5. 添加头像并美化"></a>2.2.5. 添加头像并美化</h4><p>博客添加头像有两种方法：第一种是放在本地文件夹中：D:\blog\public\uploads，并且命名为<strong>avatar.jpg</strong>。第二种是将图片放在七牛云中，然后传入链接。推荐这种方式，可以加快网页打开速度。<br><strong>站点文件</strong>任意行添加下面代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 添加头像</span><br><span class="line"># avatar: /uploads/avatar.jpg   #方法1本地图片</span><br><span class="line">avatar: http:<span class="comment">//pbscl931v.bkt.clouddn.com/18-8-3/40685653.jpg  # 方法2网络图片</span></span><br><span class="line"></span><br><span class="line">注意：uppoads文件夹是在主题里的文件夹，没有则新建</span><br><span class="line">D:\blog\themes\hexo-theme-next\source\uploads\avatar.jpg</span><br></pre></td></tr></table></figure></p><p><strong>头像变圆形</strong><br>可参考：<br><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html" target="_blank" rel="noopener">http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html</a><br><code>D:\blog\themes\next\source\css\_common\components\sidebar\sidebar-author.styl</code>，在里面添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">.site-author-image &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  margin: <span class="number">0</span> auto;</span><br><span class="line">  padding: $site-author-image-padding;</span><br><span class="line">  max-width: $site-author-image-width;</span><br><span class="line">  height: $site-author-image-height;</span><br><span class="line">  border: $site-author-image-border-width solid $site-author-image-border-color;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 头像圆形 */</span></span><br><span class="line">  border-radius: <span class="number">80</span>px;</span><br><span class="line">  -webkit-border-radius: <span class="number">80</span>px;</span><br><span class="line">  -moz-border-radius: <span class="number">80</span>px;</span><br><span class="line">  box-shadow: inset 0 -1px 0 #333sf;</span><br><span class="line">  <span class="comment">/* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 </span></span><br><span class="line"><span class="comment">    (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-transition: -webkit-transform <span class="number">1.0</span>s ease-out;</span><br><span class="line">  -moz-transition: -moz-transform <span class="number">1.0</span>s ease-out;</span><br><span class="line">  transition: transform <span class="number">1.0</span>s ease-out;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*再进一步想点击产生旋转效果，就继续在该文件下方添加代码：*/</span></span><br><span class="line"></span><br><span class="line">img:hover &#123;</span><br><span class="line">  <span class="comment">/* 鼠标经过停止头像旋转 </span></span><br><span class="line"><span class="comment">  -webkit-animation-play-state:paused;</span></span><br><span class="line"><span class="comment">  animation-play-state:paused;*/</span></span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">  -moz-transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">  transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* Z 轴旋转动画 */</span></span><br><span class="line">@-webkit-keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@-moz-keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    -moz-transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    -moz-transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="2-3-页脚美化"><a href="#2-3-页脚美化" class="headerlink" title="2.3. 页脚美化"></a>2.3. 页脚美化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/89825453.jpg" alt=""><br><strong>建站时间设置</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site's feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss:</span><br><span class="line"></span><br><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  # 建站年份</span><br><span class="line">  since: 2018           #根据实际情况修改</span><br></pre></td></tr></table></figure></p><h4 id="2-3-1-隐藏powered-By-Hexo-主题"><a href="#2-3-1-隐藏powered-By-Hexo-主题" class="headerlink" title="2.3.1. 隐藏powered By Hexo/主题"></a>2.3.1. 隐藏powered By Hexo/主题</h4><p>文件路径： D:\blog\themes\hexo-theme-next\layout_partials\ <strong>footer.swig</strong><br>更改该文件下面的代码：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="theme-info"&gt;&#123;#</span><br><span class="line">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line">  #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span><br><span class="line">#&#125;&lt;/div&gt;</span><br></pre></td></tr></table></figure></p><p>用<!--  -->注释两行如下语句，也可以直接删除掉这段代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--<span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"theme-info"</span>&gt;</span>&#123;#</span></span><br><span class="line"><span class="xml">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span></span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line"><span class="xml">  #&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span>&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span></span><br><span class="line"><span class="xml">#&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span>--&gt;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-2-next版本隐藏"><a href="#2-3-2-next版本隐藏" class="headerlink" title="2.3.2. next版本隐藏"></a>2.3.2. next版本隐藏</h4><p>继续在上面文件中修改代码如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 用&lt;!--注释语句--&gt;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.footer.theme.enable %&#125;</span><br><span class="line">  &lt;!--<span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"theme-info"</span>&gt;</span>&#123;#</span></span><br><span class="line"><span class="xml">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span></span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line"><span class="xml">  #&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span>&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span></span><br><span class="line"><span class="xml">#&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span>--&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-3-时间和用户名之间添加心形"><a href="#2-3-3-时间和用户名之间添加心形" class="headerlink" title="2.3.3. 时间和用户名之间添加心形"></a>2.3.3. 时间和用户名之间添加心形</h4><p><strong>主题文件：</strong>建站时间下面修改<code>icon: heart</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  # 建站年份</span><br><span class="line">  since: 2018</span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  # 年份后面的图标，为 Font Awesome 图标</span><br><span class="line">  # 自己去纠结 http://fontawesome.io/icons/</span><br><span class="line">  # 然后更改名字就行，下面的有关图标的设置都一样</span><br><span class="line"></span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  #icon: user</span><br><span class="line">  icon: heart</span><br></pre></td></tr></table></figure></p><p>如果还想让心变成跳动的红心，则继续在:上面的<strong>footer.swig</strong>文件中修改：<br><code>&lt;span class=&quot;with-love&quot;&gt;</code>为 <code>&lt;span class=&quot;with-love&quot; id=&quot;heart&quot;&gt;</code>  #一定要加id=”heart”<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="copyright"&gt;&#123;#</span><br><span class="line">#&#125;&#123;% set current = date(Date.now(), "YYYY") %&#125;&#123;#</span><br><span class="line">#&#125;&amp;copy; &#123;% if theme.footer.since and theme.footer.since != current %&#125;&#123;&#123; theme.footer.since &#125;&#125; &amp;mdash; &#123;% endif %&#125;&#123;#</span><br><span class="line">#&#125;&lt;span itemprop="copyrightYear"&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt;</span><br><span class="line">  &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"with-love"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-&#123;&#123; theme.footer.icon &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">  &lt;span class="author" itemprop="copyrightHolder"&gt;&#123;&#123; theme.footer.copyright || config.author &#125;&#125;&lt;/</span>span&gt;</span><br></pre></td></tr></table></figure></p><p>在自定义文件中添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 页脚加闪烁红心</span></span><br><span class="line"><span class="comment">// 自定义页脚跳动的心样式</span></span><br><span class="line">@keyframes heartAnimate &#123;</span><br><span class="line">    <span class="number">0</span>%,<span class="number">100</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    <span class="number">10</span>%,<span class="number">30</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    <span class="number">20</span>%,<span class="number">40</span>%,<span class="number">60</span>%,<span class="number">80</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    <span class="number">50</span>%,<span class="number">70</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line">#heart &#123;</span><br><span class="line">    animation: heartAnimate <span class="number">1.33</span>s ease-<span class="keyword">in</span>-out infinite;</span><br><span class="line">&#125;</span><br><span class="line">.with-love &#123;</span><br><span class="line">    color: rgb(<span class="number">192</span>, <span class="number">0</span>, <span class="number">39</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>接着在自定义<code>custom.styl</code>文件中，添加以下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 页脚加闪烁红心</span></span><br><span class="line"><span class="comment">// 自定义页脚跳动的心样式</span></span><br><span class="line">@keyframes heartAnimate &#123;</span><br><span class="line">    <span class="number">0</span>%,<span class="number">100</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    <span class="number">10</span>%,<span class="number">30</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    <span class="number">20</span>%,<span class="number">40</span>%,<span class="number">60</span>%,<span class="number">80</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    <span class="number">50</span>%,<span class="number">70</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line">#heart &#123;</span><br><span class="line">    animation: heartAnimate <span class="number">1.33</span>s ease-<span class="keyword">in</span>-out infinite;</span><br><span class="line">&#125;</span><br><span class="line">.with-love &#123;</span><br><span class="line">    color: rgb(192, 0, 39);   # rgb可随意修改</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-4-页脚显示总访客数和总浏览量"><a href="#2-3-4-页脚显示总访客数和总浏览量" class="headerlink" title="2.3.4. 页脚显示总访客数和总浏览量"></a>2.3.4. 页脚显示总访客数和总浏览量</h4><p>首先，在上述<code>footer.swig</code>文件首行添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;script <span class="keyword">async</span> src=<span class="string">"https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">#接着修改相应代码：</span><br><span class="line"># 添加总访客量</span><br><span class="line">&lt;span id=<span class="string">"busuanzi_container_site_uv"</span>&gt;</span><br><span class="line">  访客数:<span class="xml"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"busuanzi_value_site_uv"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span>人次</span><br><span class="line">&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&#123;% if theme.footer.powered %&#125;</span></span><br><span class="line"><span class="regexp">  &lt;!--&lt;div class="powered-by"&gt;&#123;#</span></span><br><span class="line"><span class="regexp">  #&#125;&#123;&#123; __('footer.powered', '&lt;a class="theme-link" target="_blank" href="https:/</span><span class="regexp">/hexo.io"&gt;Hexo&lt;/</span>a&gt;<span class="string">') &#125;&#125;&#123;#</span></span><br><span class="line"><span class="string">#&#125;&lt;/div&gt;--&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加'</span>|<span class="string">'符号</span></span><br><span class="line"><span class="string">&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125;</span></span><br><span class="line"><span class="string">  &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;% if theme.footer.custom_text %&#125;</span></span><br><span class="line"><span class="string">  &lt;div class="footer-custom"&gt;&#123;#</span></span><br><span class="line"><span class="string">  #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;#</span></span><br><span class="line"><span class="string">#&#125;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加总访问量</span></span><br><span class="line"><span class="string">&lt;span id="busuanzi_container_site_pv"&gt;</span></span><br><span class="line"><span class="string">   总访问量:&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次</span></span><br><span class="line"><span class="string">&lt;/span&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加'</span>|<span class="string">'符号</span></span><br><span class="line"><span class="string">&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125;</span></span><br><span class="line"><span class="string">  &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加博客全站共：</span></span><br><span class="line"><span class="string">&lt;div class="theme-info"&gt;</span></span><br><span class="line"><span class="string">  &lt;div class="powered-by"&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">  &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br></pre></td></tr></table></figure></p><h3 id="2-4-文章美化"><a href="#2-4-文章美化" class="headerlink" title="2.4. 文章美化"></a>2.4. 文章美化</h3><p><img src="http://pbscl931v.bkt.clouddn.com/18-8-22/74366995.jpg" alt=""></p><h4 id="2-4-1-显示统计字数和估计阅读时长"><a href="#2-4-1-显示统计字数和估计阅读时长" class="headerlink" title="2.4.1. 显示统计字数和估计阅读时长"></a>2.4.1. 显示统计字数和估计阅读时长</h4><p>修改<strong>主题文件：</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Post wordcount display settings</span><br><span class="line"># Dependencies: https://github.com/willin/hexo-wordcount</span><br><span class="line"># 显示统计字数和估计阅读时长</span><br><span class="line"># 注意：这个要安装插件，先进入站点文件夹根目录</span><br><span class="line"># 然后：npm install hexo-wordcount --save</span><br><span class="line">post_wordcount:</span><br><span class="line">  item_text: <span class="literal">true</span></span><br><span class="line">  wordcount: <span class="literal">true</span></span><br><span class="line">  min2read: <span class="literal">true</span></span><br><span class="line">  totalcount: <span class="literal">false</span></span><br><span class="line">  separated_meta: <span class="literal">false</span></span><br></pre></td></tr></table></figure></p><p>注意，做了以上修改后，发现字数只显示了数字并没有带相应的单位:<strong>字</strong>和<strong>分钟</strong>。因此，还需做如下修改：<br>打开<code>D:\blog\themes\hexo-theme-next\layout\_macro\ **post.swig**</code>文件，添加单位：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> theme.post_wordcount.wordcount or theme.post_wordcount.min2read %&#125;</span><br><span class="line">            &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-wordcount"</span>&gt;</span><br><span class="line">              &#123;% <span class="keyword">if</span> theme.post_wordcount.wordcount %&#125;</span><br><span class="line">                &#123;% <span class="keyword">if</span> not theme.post_wordcount.separated_meta %&#125;</span><br><span class="line">                  &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-divider"</span>&gt;|<span class="xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-item-icon"</span>&gt;</span><br><span class="line">                  &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-file-word-o"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">                &#123;% if theme.post_wordcount.item_text %&#125;</span></span><br><span class="line"><span class="regexp">                  &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.wordcount') &#125;&#125;&amp;#58;&lt;/</span>span&gt;</span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span title=<span class="string">"&#123;&#123; __('post.wordcount') &#125;&#125;"</span>&gt;</span><br><span class="line">                  &#123;&#123; wordcount(post.content) &#125;&#125; 字</span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">              &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">              &#123;% if theme.post_wordcount.wordcount and theme.post_wordcount.min2read %&#125;</span></span><br><span class="line"><span class="regexp">                &lt;span class="post-meta-divider"&gt;|&lt;/</span>span&gt;</span><br><span class="line">              &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">              &#123;% <span class="keyword">if</span> theme.post_wordcount.min2read %&#125;</span><br><span class="line">                &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-item-icon"</span>&gt;</span><br><span class="line">                  &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-clock-o"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">                &#123;% if theme.post_wordcount.item_text %&#125;</span></span><br><span class="line"><span class="regexp">                  &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.min2read') &#125;&#125; &amp;asymp;&lt;/</span>span&gt;</span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span title=<span class="string">"&#123;&#123; __('post.min2read') &#125;&#125;"</span>&gt;</span><br><span class="line">                  &#123;&#123; min2read(post.content) &#125;&#125; 分钟 </span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">              &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;/</span>div&gt;</span><br><span class="line">          &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">          &#123;% <span class="keyword">if</span> post.description and (not theme.excerpt_description or not is_index) %&#125;</span><br><span class="line">              &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-description"</span>&gt;</span><br><span class="line">                  &#123;&#123; post.description &#125;&#125;</span><br><span class="line">              &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">          &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">      &lt;<span class="regexp">/header&gt;</span></span><br><span class="line"><span class="regexp">    &#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure></p><h4 id="2-4-2-添加阅读全文"><a href="#2-4-2-添加阅读全文" class="headerlink" title="2.4.2. 添加阅读全文"></a>2.4.2. 添加阅读全文</h4><p>实现在主页只展示部分文字，其他文字隐藏起来，通过点击’阅读更多’来阅读全文。<br>方法就是写每一篇文章的时候，在必要的地方添加<code>&lt;!-- more --&gt;</code>即可。<br>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 4块钱,用Github+Hexo搭建你的个人博客：搭建篇</span><br><span class="line">id: hexo01</span><br><span class="line">images: http://pbscl931v.bkt.clouddn.com/18-8-3/89578286.jpg</span><br><span class="line">categories: hexo博客</span><br><span class="line">tags: [hexo,个人博客,github]</span><br><span class="line">keywords: hexo,搭建博客,github pages,next</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。</span><br><span class="line">  </span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line"></span><br><span class="line">摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。</span><br></pre></td></tr></table></figure></p><h4 id="2-4-3-显示每篇文章的阅读量"><a href="#2-4-3-显示每篇文章的阅读量" class="headerlink" title="2.4.3. 显示每篇文章的阅读量"></a>2.4.3. 显示每篇文章的阅读量</h4><p>参考这个教程即可：<br><a href="http://www.jeyzhang.com/hexo-next-add-post-views.html" target="_blank" rel="noopener">http://www.jeyzhang.com/hexo-next-add-post-views.html</a></p><p>在这个过程中发现了一个问题：pc端正常显示阅读量，但是移动端没有显示具体的阅读量。解决办法：<br>在leancloud网站上，进入安全中心，检查web安全域名列表中是否添加了<strong>http：开头</strong>的域名，如果没有，则添加上应该就能解决，例如，我的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://makcyun.top/</span><br></pre></td></tr></table></figure></p><h4 id="2-4-4-文章摘要配图"><a href="#2-4-4-文章摘要配图" class="headerlink" title="2.4.4. 文章摘要配图"></a>2.4.4. 文章摘要配图</h4><p>参考这个教程即可：<br><a href="http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/" target="_blank" rel="noopener">http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/</a></p><p>附上我的设置：<br>在自定义文件中添加如下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// img.img-topic &#123;</span></span><br><span class="line"><span class="comment">//    width: 100%;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//图片外部的容器方框</span></span><br><span class="line">.out-img-topic &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  max-height:<span class="number">350</span>px;      <span class="comment">//图片显示高度，如果不设置则每篇文章的图片高度会不一样，看起来不协调</span></span><br><span class="line">  margin-bottom: <span class="number">24</span>px;</span><br><span class="line">  overflow: hidden;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//图片</span></span><br><span class="line">img.img-topic &#123;</span><br><span class="line">  display: block ;</span><br><span class="line">  margin-left: <span class="number">.7</span>em;</span><br><span class="line">  margin-right: <span class="number">.7</span>em;</span><br><span class="line">  padding: <span class="number">0</span>;</span><br><span class="line">  float: right;</span><br><span class="line">  clear: right;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 去掉图片边框</span></span><br><span class="line">.posts-expand .post-body img &#123;</span><br><span class="line">    border: none;</span><br><span class="line">    padding: <span class="number">0</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-4-5-添加打赏功能"><a href="#2-4-5-添加打赏功能" class="headerlink" title="2.4.5. 添加打赏功能"></a>2.4.5. 添加打赏功能</h4><p>参考下面的教程：<br><a href="https://www.cnblogs.com/mrwuzs/p/7943337.html" target="_blank" rel="noopener">https://www.cnblogs.com/mrwuzs/p/7943337.html</a><br><a href="https://blog.csdn.net/lcyaiym/article/details/76796545" target="_blank" rel="noopener">https://blog.csdn.net/lcyaiym/article/details/76796545</a></p><p>以上，包括了博客美化的大部分操作。<br>如果，你觉得还不够，想做得更精致一些，那么推荐一个非常详细的美化教程：<br><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl" target="_blank" rel="noopener">https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl</a></p><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客" scheme="https://www.makcyun.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://www.makcyun.top/tags/hexo/"/>
    
      <category term="个人博客" scheme="https://www.makcyun.top/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github" scheme="https://www.makcyun.top/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>4块钱,用Github+Hexo搭建你的个人博客：搭建篇</title>
    <link href="https://www.makcyun.top/hexo01.html"/>
    <id>https://www.makcyun.top/hexo01.html</id>
    <published>2018-07-06T08:44:19.881Z</published>
    <updated>2018-08-16T09:27:15.222Z</updated>
    
    <content type="html"><![CDATA[<p><b><em>4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。</em></b><br>之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。</p><a id="more"></a>  <p><strong>【更新于2018/7/14】</strong></p><p><strong>摘要：</strong> 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。好在参考了很多大佬的教程后顺利搭建完成，但过程中还是踩了一些坑。这里及时进行总结，作为博客的第一篇文章。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><h3 id="1-网上有很多现成的博客不用，为什么要自己搭建"><a href="#1-网上有很多现成的博客不用，为什么要自己搭建" class="headerlink" title="1 网上有很多现成的博客不用，为什么要自己搭建?"></a>1 网上有很多现成的博客不用，为什么要自己搭建?</h3><p>可能有人会说：很多网站都能写博客，而且博客现在其实都有点过时了，为什么还要自己去搞？  </p><p>这里我说一下我想自己搭建的两点原因：<br><strong>一、</strong>网上的多数博客大家都共用一套相同的模板界面，没有特点、界面也杂乱充斥着很多不相关的东西，不论是自己写还是给人看，体验都不好。<br><strong>二、</strong>拥有一个你自己可以起名字的博客网站，里面的任何内容完全由你自己决定，这感觉是件很酷的事。 </p><p>这些普通的网站博客和一些个人博客，哪个好看，高下立判吧。<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/94423333.jpg" alt="新浪博客">  </p><p><center><strong>vs</strong></center><br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/12736339.jpg" alt="个人博客"></p><p>&nbsp;<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/26966.jpg" alt="CSDN博客">  </p><p><center><strong>vs</strong></center><br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/73613801.jpg" alt="个人博客"></p><p>更多个人博客：<br><strong>litten</strong> &nbsp; <a href="http://litten.me/" target="_blank" rel="noopener">http://litten.me/</a><br><strong>Ryan</strong> &nbsp; <a href="http://ryane.top/" target="_blank" rel="noopener">http://ryane.top/</a><br><strong>liyin</strong> &nbsp; <a href="https://liyin.date/" target="_blank" rel="noopener">https://liyin.date/</a><br><strong>reuixiy</strong> &nbsp; <a href="https://reuixiy.github.io/" target="_blank" rel="noopener">https://reuixiy.github.io/</a><br><strong>Tranquilpeak</strong> &nbsp; <a href="https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/" target="_blank" rel="noopener">https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/</a></p><p><br></p><h3 id="2-搭建博客难不难？"><a href="#2-搭建博客难不难？" class="headerlink" title="2 搭建博客难不难？"></a>2 搭建博客难不难？</h3><p>我之前认为搭建博客是一件只有能程序猿才能做出来的高大上的活。其实，只要跟着网上的教程一步步做下去，一个小时不到就可以搭建好你自己的个人博客。所以，搭建博客其实很简单。只不过，如果你想把博客做得好看一些的话，才会花费一些精力。</p><p><br></p><h2 id="二、开始搭建博客"><a href="#二、开始搭建博客" class="headerlink" title="二、开始搭建博客"></a>二、开始搭建博客</h2><p><strong>如果看到上面那些精美的博客，你已经心动了，那就开始动手吧。下面正式开始博客搭建步骤。</strong>  </p><p><strong>搭建教程参考</strong><br>搭建博客的教程网上一搜一大堆，为了节省你的搜索时间，这里我筛选出了下面几篇很棒的教程，我基本都是跟着一步步做下来的。</p><ol><li><a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">小白独立搭建博客</a>   </li><li><a href="http://ryane.top/2018/01/10/2018%EF%BC%8C%E4%BD%A0%E8%AF%A5%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E4%BA%86%EF%BC%81/" target="_blank" rel="noopener">2018，你该搭建自己的博客了！</a></li><li><a href="https://blog.csdn.net/gdutxiaoxu/article/details/53576018" target="_blank" rel="noopener">手把手教你用Hexo+Github 搭建属于自己的博客</a></li></ol><p>操作平台:Win7 64位。</p><p><br></p><p><strong>相关名词解释：</strong><br><strong>Hexo：</strong>一种常用的博客框架，有了它建立博客非常简单。你可以认为它是一种博客模板，只不过它比普通网站的那种博客模板要好看地多，并且有很高度的自定义性，只要你愿意，你可以建立一个独一无二的博客来。<br>若想详细了解Hexo的使用，移步 <strong>Hexo官方网站</strong> <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/</a>。  </p><p><strong>Github：</strong>一个全世界程序猿聚集的知名网站。免费的远程仓库和开源协作社区。我们需要利用网站里的Github Pages功能来托管需发布到网上的博客的相关文件和代码。</p><p><strong>Git：</strong> 一种版本控制系统。我们在自己的本地电脑写博客，如何把博客同步到Github，然后发布到网上去？就需要用这个软件去写几行代码然后就能搞定，后期用的最多的就是它。</p><p><strong>Node.js：</strong> 提供JavaScript的开发环境，安装好以后就不用跟它再打交道，所以不用太关注它。</p><h3 id="1-软件安装配置"><a href="#1-软件安装配置" class="headerlink" title="1 软件安装配置"></a>1 软件安装配置</h3><p>搭建博客需要先下载2个软件：Git和Nodejs。<br>软件安装过程很简单，一直点击Next默认直到安装完成就行了。</p><p><strong>Git</strong><br>官网：<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">https://git-scm.com/download/win</a><br>安装完，打开cmd窗口运行下面命令，如果有返回版本信息说明安装成功。</p><pre><code>git –version </code></pre><p><strong>Nodejs</strong><br>官网：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a><br>同样，安装完有返回版本信息说明安装成功，见下图。</p><pre><code>node -v  npm -v  </code></pre><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/43008634.jpg" alt="cmd命令"></p><p>至此，软件安装步骤完成。</p><h3 id="2-安装Hexo博客框架"><a href="#2-安装Hexo博客框架" class="headerlink" title="2 安装Hexo博客框架"></a>2 安装Hexo博客框架</h3><ul><li>安装hexo  </li></ul><p>这里开始就要用到使用频率最高的Git软件了。</p><p>桌面右键点击<strong>git bash here</strong>选项，会打开Git软件界面，输入下面每行命令并回车：  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>第一句是安装hexo，第二句是安装hexo部署到git page的deployer。代码命令看不懂没关系，一是这些命令之后几乎不再用到，二是用多了你会慢慢记住。<br><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/18599032.jpg" alt="">  </p><ul><li>设置博客存放文件夹  </li></ul><p>你需要预想一下你要把博客文件新建在哪个盘下的文件夹里，除了c盘都可以，例如d盘根目录下的blog文件夹，紧接着在桌面输入下面命令并回车：  </p><pre><code>hexo init /d/blogcd /d/blognpm install*注：/d/bog可以更改为你自己的文件夹*</code></pre><p>有的教程是先新建博客文件夹，在该文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，再执行以下操作。但是我操作过程中出现过这样的失败提示：<code>hexo:conmand not found</code>，但我执行上面的命令时就没有出现该问题。</p><pre><code>hexo init npm install</code></pre><ul><li>查看博客效果  </li></ul><p>至此，博客初步搭建好，输入下面一行本地部署生成的命令：  </p><pre><code>hexo s -g </code></pre><p>然后打开浏览器在网址栏输入：<code>localhost:4000</code>就可以看到博客的样子，如果无法打开，则继续输入下面命令：    </p><pre><code>npm install hexo-deployer-git --savehexo cleanhexo s -g </code></pre><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/90405263.jpg" alt="">  </p><p>打开该网址，你可以看到第一篇默认的博客：<strong>Hello World</strong>。但看起来很难看，后续会通过重新选择模板来对博客进行美化。  </p><div class="note primary"><p>现在你就可以开始写博客了，但是博客只能在你自己的电脑上看到，别人无法在网上看到你的博客。接下来需要利用前面提到的的Github Pages功能进行设置，设置完成之后别人通过搜索就可以看到你的博客。</p></div><h3 id="3-把你的博客部署到Github-Pages上去"><a href="#3-把你的博客部署到Github-Pages上去" class="headerlink" title="3 把你的博客部署到Github Pages上去"></a>3 把你的博客部署到Github Pages上去</h3><p>这是搭建博客相对比较复杂也是容易出错的一部分。</p><p><strong>1. Github账号注册及配置</strong>  </p><p>如果你没有github帐号，就新建一个，然后去邮箱进行验证；如果你有帐号则直接登录。<br>官网：<a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a> </p><p>配置步骤：  </p><ul><li>建立new repository</li></ul><p>只填写username.github.io即可，然后点击<code>create repositrory</code>。<br>注意：<code>username.github.io</code> 的<code>username</code>要和用户名保持一致，不然后面会失败。以我的为例：  </p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/98355992.jpg" alt="1"></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/62084844.jpg" alt="2"></p><ul><li>开启gh-pages功能  </li></ul><p>点击github主页点击头像下面的profile,找到新建立的username.github.io文件打开，点击settings，往下拉动鼠标到GitHub Pages。<br>如果你看到上方出现以下警告：  </p><div class="note warning"><br>GitHub Pages is currently disabled. You must first add content to your repository before you can publish a GitHub Pages site<br></div><p>不用管他，点击选择<code>choose a theme</code>，随便选择一个，（之后我们要更改这些丑陋的模板），然后select theme保存就行了。</p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/79896859.jpg" alt="3"></p><p><img src="http://pbscl931v.bkt.clouddn.com/18-7-14/1269326.jpg" alt="5"> </p><p>接下来的几个步骤参考<a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">教程1</a>即可。  </p><p>主要步骤包括：  </p><ul><li>git创建SSH密钥  </li><li>在GitHub账户中添加你的公钥  </li><li>测试成功并设置用户信息  </li><li>将本地的Hexo文件更新到Github库中  </li><li>hexo部署更新博客  </li></ul><p>经过以上几步的操作，顺利的话，你的博客可以发布到网上，其他人也可以通过你的网址<code>username.github.io</code>（我的是<code>makcyun.github.io</code>）<br>访问到你的博客。  </p><h3 id="4-赶紧新建个博客试试"><a href="#4-赶紧新建个博客试试" class="headerlink" title="4 赶紧新建个博客试试"></a>4 赶紧新建个博客试试</h3><p>接下来你可以自己新建一个文档来写下你的第一篇博客并在网页上测试。</p><p>同样在根目录<code>D:\blog</code>中运行下面命令：  </p><pre><code>hexo new 第一篇博客*注：第一篇博客名称可以随便修改*</code></pre><p>然后打开<code>D:\blog\source\_posts</code>文件夹，就可以看到一个<code>第一篇博客.md</code>的文件。用支持markdown语法的软件打开该文件进行编辑即可。</p><p>编辑好以后，运行下述命令：</p><pre><code>hexo cleanhexo d -g</code></pre><p>然后，在网址中输入<code>username.github.io</code>即可看到你的博客上，出现<strong>第一篇博客</strong>这篇新的文章。</p><p><strong>至此，你的个人博客初步搭建过程就完成了。</strong></p><p><br></p><p>但是，现在还存在两个问题你可能想解决：</p><ul><li>markdown语法是什么，如何用软件编写博客？</li><li>网址是<code>username.github.io</code>，感觉很奇怪，而我的博客网址怎么是<strong>www</strong>开头的？</li></ul><p>好，下面来讲解一下。</p><p><br></p><p><strong>第一个问题</strong></p><p>关于markdown语法介绍：<br><a href="https://www.jianshu.com/p/1e402922ee32/" target="_blank" rel="noopener">markdown——入门指南</a></p><p>当你大致了解markdown语法后，如何用markdown写博客呢？不妨参考这两篇详细教程：  </p><blockquote><p><a href="https://markdown.tw/" target="_blank" rel="noopener">Markdown語法說明</a><br><a href="https://www.ofind.cn/archives/" target="_blank" rel="noopener">HEXO下的Markdown语法(GFM)写博客</a></p></blockquote><p>接下来你要一个可以写markdown语法的软件，这里推荐两款软件。  </p><p>Windows下使用<a href="http://markdownpad.com/" target="_blank" rel="noopener">Markdown Pad2</a>, Mac下使用<a href="http://25.io/mou/" target="_blank" rel="noopener">Mou</a>。 </p><p>我用的是Markdown Pad2，这款软件是付费软件，但网上有很多破解版，我这里将软件上传到了百度网盘，如需请取。<br>MarkdownPad2： <a href="https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA" target="_blank" rel="noopener">https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA</a>   密码：y9zh</p><p>安装好后，就可以打开刚才的<code>第一篇博客.md</code>，开始尝试写你的第一篇博客了。  </p><p>比如这是我用markdownpad写的博客原稿。<br><img src="http://pbscl931v.bkt.clouddn.com/FpU5NFP6QFdTwqjStlUrBNk2GPDK" alt=""></p><p>可以看到文档里面都是字符，没有图片这些。所以只需要用键盘专注于打字就行了，不需要像word那么复杂，还要用鼠标插入标题样式、图片这些操作。</p><p><br></p><p><strong>第二个问题</strong></p><p>我的网址不是默认的<code>username.github.io</code>，是因为我购买了一个域名，然后和<code>username.github.io</code>进行了关联，这样我的博客网址变成了我的域名。</p><p>在哪里购买域名呢？<br>首推去 <a href="https://wanwang.aliyun.com/domain/?spm=5176.383338.1907008.1.LWIFhw" target="_blank" rel="noopener">阿里云官网</a> 购买。</p><p>你可以随意起你喜欢的名字，然后在该网站进行搜索，没有人占用的话你就可以购买该域名。不同的后缀价格不同。可以看到<strong>.com</strong>、 <strong>.net</strong>等会比较贵，最便宜的这两年新出的<strong>.top域名</strong>，只要4块钱一年，我购买的就是这种。</p><p>购买完域名以后，需要做以下几个步骤：  </p><ul><li>实名认证</li><li>修改DNS</li><li>域名解析</li><li>新建CNAME文件</li></ul><p><strong>1 实名认证</strong><br>在修改DNS之前，必须要阿里云官网实名认证成功，用淘宝账户登录然后填写相关信息即可。</p><p><strong>2 修改DNS</strong><br>实名认证成功后，进入管理界面，依次点击：<br><img src="http://pbscl931v.bkt.clouddn.com/Ft9CnDVTNm1WFZMegkca8SaOokfW" alt=""></p><p><img src="http://pbscl931v.bkt.clouddn.com/FtOks38CUKdJga4Q-rlSXjcdezPs" alt=""></p><p>修改DNS为：<br><strong>f1g1ns1.dnspod.net<br>f1g1ns2.dnspod.net</strong></p><p><strong>3 域名解析</strong><br>DNS修改好以后，到<strong>DNSPOD</strong>这个网站去解析你的域名。  </p><p>首先，微信登录并注册 <a href="https://www.dnspod.cn/" target="_blank" rel="noopener">https://www.dnspod.cn/</a>，点击域名解析，添加上你的域名。<br><img src="http://pbscl931v.bkt.clouddn.com/FuS2HLF9F9v9wvYiloqF8kpWMh8V" alt=""></p><p>接着，添加以下两条记录即可。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FojJP59gDAOuk41RMtCvEkuKijo2" alt=""></p><p>注意：<strong>makcyun.github.io.</strong>需换成你自己的名称，另外最后有一个<strong>“.”</strong></p><p><strong>4 新建CNAME文件</strong><br>在博客根目录文件夹下,例如我的<code>D:\blog\source</code>，新建名为<strong>CNAME</strong>的记事本文件，去掉后缀。<br>在里面输入你的域名，例如我的：<strong><a href="http://www.makcyun.top">www.makcyun.top</a></strong>即可，保存并关闭。</p><p><img src="http://pbscl931v.bkt.clouddn.com/FmmCR8YMx-heNEiN9RFb4MkEbFA0" alt=""></p><p><strong>注意： </strong><br>这里填不填写<strong>www</strong>前缀都是可以的，区别在于填写www，那么博客网址就会以www开头，例如：<strong><a href="http://www.makcyun.top">www.makcyun.top</a></strong>；如果不填写，博客网址是：<strong>makcyun.top</strong>，二者都可以，看你喜欢。</p><p>完成以上4步之后，根目录下再次运行：  </p><pre><code>hexo d -g  </code></pre><p>这时，输入你在记事本里的域名网址，即可打开你的博客。<br>至此，你的博客就换成了你想要的网址，别人也可以通过这个网址访问到你的博客。 </p><p><br></p><p>到这里，博客的初步搭建就算完成了，顺利的话不到1小时就能完成，如果中间出现差错，保持耐心多试几次应该就没问题。  </p><p>此时，还有一个比较重要的问题就是，你可能会觉得你的博客不够美观，不如我博客里前面提到的那几个博客，甚至也没有我的好看。  </p><p>如果你还愿意折腾的话，下一篇文章，我会以我的博客为例，讲一讲如何进行博客的美化。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;&lt;em&gt;4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。&lt;/em&gt;&lt;/b&gt;&lt;br&gt;之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客" scheme="https://www.makcyun.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://www.makcyun.top/tags/hexo/"/>
    
      <category term="个人博客" scheme="https://www.makcyun.top/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github" scheme="https://www.makcyun.top/tags/github/"/>
    
  </entry>
  
</feed>
